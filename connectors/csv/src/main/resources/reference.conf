# Note that a paragraph is written in one line, and paragraphs are separated by a blank line.
# This has the benefit of rendering well in markdown as well as plain-text help output (since
# the help text formatter wraps lines appropriately).
dsbulk {

  # CSV Connector configuration.
  connector.csv {

    # The URL or path of the resource(s) to read from or write to.
    #
    # Which URL protocols are available depend on which URL stream handlers have been installed, but at least the **file** protocol is guaranteed to be supported.
    #
    # The file protocol can be used with all supported file systems, local or not.
    # - When reading: the URL can point to a single file, or to an existing directory; in case of a directory, the *fileNamePattern* setting can be used to filter files to read, and the *recursive* setting can be used to control whether or not the connector should look for files in subdirectories as well.
    # - When writing: the URL will be treated as a directory; if it doesn't exist, the loader will attempt to create it; CSV files will be created inside this directory, and their names can be controlled with the *fileNameFormat* setting.
    #
    # Note that if the value specified here does not have a protocol, then it is assumed to be a file protocol. Relative URLs will be resolved against the current working directory. Also, for convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the current user's home directory.
    #
    # In addition the value `-` indicates `stdin` when loading and `stdout` when unloading. This is in line with Unix tools such as tar, which uses `-` to represent stdin/stdout when reading/writing an archive.
    #
    # Examples:
    #
    #     url = "/path/to/dir/or/file"           # without protocol
    #     url = "./path/to/dir/or/file"          # without protocol, relative to working directory
    #     url = "~/path/to/dir/or/file"          # without protocol, relative to the user's home directory
    #     url = "file:///path/to/dir/or/file"    # with protocol
    #     url = "-"                              # to read csv data from stdin (for load) or
    #     url = "-"                              # write csv data to stdout (for unload)
    #
    # For other URLs: the URL will be read or written directly; settings like *fileNamePattern*, *recursive*, and *fileNameFormat* will have no effect.
    #
    # The default value is `-` (read from `stdin` / write to `stdout`).
    url = "-"

    # The glob pattern to use when searching for files to read. The syntax to use is the glob syntax, as described in `java.nio.file.FileSystem.getPathMatcher()`. This setting is ignored when writing and for non-file URLs. Only applicable when the *url* setting points to a directory on a known filesystem, ignored otherwise.
    fileNamePattern = "**/*.csv"

    # The file name format to use when writing. This setting is ignored when reading and for non-file URLs. The file name must comply with the formatting rules of `String.format()`, and must contain a `%d` format specifier that will be used to increment file name counters.
    fileNameFormat = "output-%0,6d.csv"

    # Enable or disable scanning for files in the root's subdirectories. Only applicable when *url* is set to a directory on a known filesystem. Used for loading only.
    recursive = false

    # The maximum number of files that can be written simultaneously. This setting is ignored when reading and when the output URL is anything other than a directory on a filesystem. The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    maxConcurrentFiles = 0.25C

    # The file encoding to use for all read or written files.
    encoding = "UTF-8"

    # Enable or disable whether the files to read or write begin with a header line. If enabled for loading, the first non-empty line in every file will assign field names for each record column, in lieu of `schema.mapping`, `fieldA = col1, fieldB = col2, fieldC = col3`. If disabled for loading, records will not contain fields names, only field indexes, `0 = col1, 1 = col2, 2 = col3`. For unloading, if this setting is enabled, each file will begin with a header line, and if disabled, each file will not contain a header line.
    #
    # Note: This option will apply to all files loaded or unloaded.
    header = true

    # The character to use as field delimiter.
    delimiter = ","

    # The character used for quoting fields when the field delimiter is part of the field value. Only one character can be specified. Note that this setting applies to all files to be read or written.
    quote = "\""

    # The character used for escaping quotes inside an already quoted value. Only one character can be specified. Note that this setting applies to all files to be read or written.
    escape = "\\"

    # The character that represents a line comment when found in the beginning of a line of text. Only one character can be specified. Note that this setting applies to all files to be read or written. This feature is disabled by default (indicated by its `null` character value).
    comment = "\u0000"

    # The number of records to skip from each input file before the parser can begin to execute. Note that if the file contains a header line, that line is not counted as a valid record. This setting is ignored when writing.
    skipRecords = 0

    # The maximum number of records to read from or write to each file. When reading, all records past this number will be discarded. When writing, a file will contain at most this number of records; if more records remain to be written, a new file will be created using the *fileNameFormat* setting. Note that when writing to anything other than a directory, this setting is ignored. This setting takes into account the *header* setting: if a file begins with a header line, that line is not counted as a record. This feature is disabled by default (indicated by its `-1` value).
    maxRecords = -1

    # The maximum number of characters that a field can contain. This setting is used to size internal buffers and to avoid out-of-memory problems. If set to -1, internal buffers will be resized dynamically. While convenient, this can lead to memory problems. It could also hurt throughput, if some large fields require constant resizing; if this is the case, set this value to a fixed positive number that is big enough to contain all field values.
    maxCharsPerColumn = 4096

    # This group of settings is purely internal to the connector and are the interface for
    # DSBulk's infrastructure to customize how some settings are exposed to the user.
    #
    # In particular, how settings are documented and shortcut options that map to
    # settings that are commonly specified in the command line.
    metaSettings {
      # Specify how settings should be prioritized in generated docs and help.
      docHints {
        commonSettings = [url, delimiter, header, skipRecords, maxRecords]
        preferredSettings = [quote]
      }

      # Specify shortcuts for "long" options.
      # Format:
      # shortcut = unqualified long option.
      shortcuts {
        comment = comment
        delim = delimiter
        encoding = encoding
        escape = escape
        header = header
        skipRecords = skipRecords
        maxRecords = maxRecords
        maxConcurrentFiles = maxConcurrentFiles
        quote = quote
        url = url
      }
    }
  }

}
