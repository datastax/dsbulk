# Reference configuration for the DataStax Loader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
datastax-loader {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster.
    # This must be a list of strings with each contact point specified as "host:port". If the host is
    # a DNS name that resolves to multiple A-records, all the corresponding addresses will be used.
    # Do not use "localhost" as the host name (since it resolves to both IPv4 and IPv6 addresses on
    # some platforms).
    # Note that all nodes in a cluster must share the same port.
    # Defaults to ["127.0.0.1:9042"].
    contactPoints = ["127.0.0.1:9042"]

    # Native Protocol-specific settings.
    protocol {

      # The native protocol version to use.
      # Valid values are: `V3`, `V4`, `V5`, `DSE_V1`.
      # This option is not required. If it is absent, the driver will negotiate it.
      version = null

      # The compression algorithm to use.
      # Valid values are: NONE, LZ4, SNAPPY.
      # Defaults to LZ4.
      compression = LZ4

    }

    # Pooling-specific settings.
    # The driver maintains a connection pool to each node, according to the distance assigned to it
    # by the load balancing policy. If the distance is IGNORED, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver
      # sends a dummy message on it to make sure it's still alive. If not, the connection is
      # trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both reads and writes.
      # Allowed values are:
      # ANY, LOCAL_ONE, ONE, TWO, THREE, LOCAL_QUORUM, QUORUM, EACH_QUORUM, ALL.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes.
      # The allowed values are: SERIAL and LOCAL_SERIAL.
      # Only applicable if the data is inserted using lightweight transactions,
      # ignored otherwise.
      serialConsistency = LOCAL_SERIAL

      # The page size. This controls how many rows will be retrieved simultaneously in a single
      # network round trip (the goal being to avoid loading too many results in memory at the same
      # time).
      # Only applicable in read scenarios, ignored otherwise.
      fetchSize = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {

      # How long the driver waits for a request to complete. This is a global limit on the duration
      # of a session.execute() call, including any internal retries the driver might do.
      readTimeout = 12 seconds

    }

    # Authentication settings.
    auth {

      # The name of the AuthProvider to use.
      # This property is optional; if it is not present, no authentication will occur.
      # Valid values are:
      #  - PlainTextAuthProvider:
      #    Uses com.datastax.driver.core.PlainTextAuthProvider for authentication.
      #    Supports SASL authentication using the PLAIN mechanism (plain text authentication).
      #  - DsePlainTextAuthProvider:
      #    Uses com.datastax.driver.dse.auth.DsePlainTextAuthProvider for authentication.
      #    Supports SASL authentication to DSE clusters using the PLAIN mechanism (plain text authentication),
      #    and also supports optional proxy authentication; should be preferred to PlainTextAuthProvider
      #    when connecting to secured DSE clusters.
      #  - DseGSSAPIAuthProvider:
      #    Uses com.datastax.driver.dse.auth.DseGSSAPIAuthProvider for authentication.
      #    Supports SASL authentication to DSE clusters using the GSSAPI mechanism (Kerberos authentication),
      #    and also supports optional proxy authentication.
      provider = null

      # The username to use. Required.
      # Providers that accept this setting:
      #  - PlainTextAuthProvider
      #  - DsePlainTextAuthProvider
      username = null

      # The password to use. Required.
      # Providers that accept this setting:
      #  - PlainTextAuthProvider
      #  - DsePlainTextAuthProvider
      password = null

      # The authorization ID to use. Optional.
      # An authorization ID allows the currently authenticated user
      # to act as a different user (a.k.a. proxy authentication).
      # Providers that accept this setting:
      #  - DsePlainTextAuthProvider
      #  - DseGSSAPIAuthProvider
      authorizationId = null

      # The Kerberos principal to use. Required.
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      principal = null

      # The URL of the Kerberos keytab file to use for authentication, e.g.
      # "file:///path/to/my/keyTab".
      # Optional. If left unspecified, it is assumed that authentication will
      # be done with a ticket cache instead.
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      keyTab = null

      # The SASL protocol name to use. Required.
      # This value should match the username of the
      # Kerberos service principal used by the DSE server.
      # This information is specified in the dse.yaml file by the
      # service_principal option under the kerberos_options section,
      # and may vary from one DSE installation to another â€“ especially
      # if you installed DSE with an automated package installer.
      # Defaults to "dse".
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      saslProtocol = "dse"

    }

    # Encryption-specific settings.
    # For more information about how to configure this section,
    # see the Java Secure Socket Extension (JSSE) Reference Guide:
    # http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html
    # You can also check the DataStax Java driver documentation on SSL:
    # http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ssl {

      # The SSL provider to use.
      # This property is optional; if it is not present, SSL won't be activated.
      # Valid values are:
      # - JDK: uses JDK's SSLContext
      # - OpenSSL: uses Netty's native support for OpenSSL
      # Using OpenSSL provides better performance and generates less garbage.
      # A disadvantage of using the OpenSSL provider is that, unlike the JDK provider,
      # it requires a platform-specific dependency, named "netty-tcnative",
      # which must be added manually to the loader's classpath
      # (typically by dropping its jar in the lib subdirectory of the Loader archive).
      # Follow these instructions to find out how to add this dependency:
      # http://netty.io/wiki/forked-tomcat-native.html
      provider = null

      # The cipher suites to enable.
      # Example: cipherSuites = [ "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA" ]
      # This property is optional. If it is not present, the driver won't explicitly enable cipher
      # suites, which according to the JDK documentation results in "a minimum
      # quality of service".
      cipherSuites = []

      # The truststore to use to validate remote peer certificates.
      # This section is valid for both JDK and OpenSSL providers.
      truststore {

        # The URL of the truststore file, e.g.
        # "file:///path/to/my/truststore".
        # This setting is optional. If left unspecified,
        # server certificates will not be validated.
        url = null

        # The truststore password.
        password = null

        # The algorithm to use.
        # Valid values are: PKIX, SunX509.
        # Defauls to "SunX509".
        algorithm = SunX509

      }

      # The keystore to use for client authentication.
      # This section is only valid when using JDK provider;
      # it is ignored otherwise.
      keystore {

        # The URL of the keystore file, e.g.
        # "file:///path/to/my/keystore".
        # This setting is optional. If left unspecified,
        # no client authentication will be used.
        url = null

        # The keystore password.
        password = null

        # The algorithm to use.
        # Valid values are: SunX509, NewSunX509.
        # Defauls to "SunX509".
        algorithm = SunX509

      }

      # OpenSSL configuration for client authentication.
      # This section is only valid when using OpenSSL provider;
      # it is ignored otherwise.
      openssl {

        # The URL of the certificate chain file, e.g.
        # "file:///path/to/my/keyCertChain".
        # This setting is optional. If left unspecified,
        # no client authentication will be used.
        keyCertChain = null

        # The URL of the private key file, e.g.
        # "file:///path/to/my/privateKey".
        privateKey = null
      }
    }

    # The fully-qualified class name of the timestamp generator to use.
    # Built-in options are:
    # - AtomicTimestampGenerator: timestamps are guaranteed to be unique across all client threads.
    # - ThreadLocalTimestampGenerator: timestamps that are guaranteed to be unique within each
    #   thread only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    # Defaults to com.datastax.driver.core.AtomicMonotonicTimestampGenerator.
    timestampGenerator = AtomicMonotonicTimestampGenerator

    # The fully-qualified class name of the address translator to use.
    # This is only needed if the nodes are not directly reachable from the driver (for example, the
    # driver is in a different network region and needs to use a public IP, or it connects through
    # a proxy).
    # Defaults to com.datastax.driver.core.policies.IdentityTranslator.
    addressTranslator = IdentityTranslator

    # Settings for various driver policies.
    policy {

      # The fully-qualified class name of the RetryPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.DefaultRetryPolicy.
      retry = DefaultRetryPolicy

      # The fully-qualified class name of the LoadBalancingPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.RoundRobinPolicy.
      lbp = RoundRobinPolicy

      # The fully-qualified class name of the SpeculativeExecutionPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.NoSpeculativeExecutionPolicy.
      specexec = NoSpeculativeExecutionPolicy

    }

  }

  # Batch-specific settings.
  # These settings control how the workflow engine
  # groups together statements before writing them.
  # Only applicable for write operations, ignored otherwise.
  # See com.datastax.loader.executor.api.batch.StatementBatcher for more information.
  batch {

    # The grouping mode.
    # Valid values are:
    # - PARTITION_KEY : Groups together statements that share the same partition key. This is the default mode, and
    #                   the preferred one.
    # - REPLICA_SET   : Groups together statements that share the same replica set. This mode might yield better
    #                   results for small clusters and lower replication factors, but tends to perform equally well
    #                   or even worse than PARTITION_KEY for larger clusters or high replication factors.
    # Defaults to PARTITION_KEY.
    mode = PARTITION_KEY

    # Whether the upstream is already sorted by grouping key or not.
    # Defaults to false.
    # This should only be set to true if the upstream is guaranteed to be sorted,
    # otherwise the batching might not function properly.
    # If in doubt, leave this settings to false.
    sorted = false

    # The buffer size to use when batching.
    # Defaults to 1000.
    bufferSize = 1000

  }

  # Executor-specific settings.
  executor {

    # The maximum number of threads to allocate for the executor.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4C.
    maxThreads = 4C

    # The maximum number of "in-flight" requests. In other words, sets the maximum number of
    # concurrent uncompleted futures waiting for a response from the server. This acts as a safeguard
    # against workflows that generate more requests than they can handle.
    # Defaults to 1000.
    # Setting this option to any negative value will disable it.
    maxInflight = 1000

    # The maximum number of concurrent requests per second. This acts as a safeguard against
    # workflows that could overwhelm the cluster with more requests than it can handle.
    # Defaults to 100,000.
    # Setting this option to any negative value will disable it.
    maxPerSecond = 100000

    # Continuous-paging specific settings.
    # Only applicable for reads, and only if this feature is available in the remote cluster,
    # ignored otherwise.
    continuousPaging {

      # The unit to use for the page-size setting.
      # Possible values are: ROWS, BYTES.
      # Defaults to ROWS.
      pageUnit = ROWS

      # The size of the page. The unit to use
      # is determined by the pageUnit settings.
      # Defaults to 5000 (rows).
      #
      pageSize = 5000

      # The maximum number of pages to retrieve.
      # Defaults to 0, which means retrieve all pages available.
      #
      maxPages = 0

      # The maximum number of pages per second.
      # Defaults to 0, which indicates no limit.
      maxPagesPerSecond = 0

    }

  }

  # Log and error management settings.
  log {

    # The output directory where all log files will be stored.
    # Log files for a specific run will be located in a sub-directory
    # inside the directory specified here. Each run generates
    # a sub-directory identified by an "operation ID', which
    # is basically a timestamp in the format: yyyy_MM_dd_HH_mm_ss_nnnnnnnnn.
    # Defaults to "file:.", which means the current directory.
    outputDirectory = "file:."

    # The maximum number of threads to allocate to log files management.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4.
    maxThreads = 4

    # The maximum number of errors to tolerate before aborting
    # the entire operation.
    # Defaults to 100.
    # Setting this value to -1 disables this feature (not recommended).
    maxErrors = 100

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired verbosity. Possible values are:
      # - ABRIDGED : only prints basic information in summarized form.
      # - NORMAL   : prints basic information in summarized form, and the statement's query string,
      #              if available. For batch statements, this verbosity level also prints information
      #              about the batch's inner statements.
      # - EXTENDED : prints full information, including the statement's query string, if available,
      #              and the statement's bound values, if available. For batch statements, this verbosity
      #              level also prints all information available about the batch's inner statements.
      # Defaults to EXTENDED.
      verbosity = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      # Defaults to 500.
      # Setting this value to -1 disables this feature (not recommended).
      maxQueryStringLength = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit,
      # the exceeding values will not be printed.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      maxBoundValues = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      maxBoundValueLength = 50

      # The maximum number of inner statements to print for a batch statement.
      # Only applicable for batch statements, ignored otherwise.
      # If the batch statement has more children than
      # this value, the exceeding child statements will not be printed.
      # Defaults to 10.
      # Setting this value to -1 disables this feature (not recommended).
      maxInnerStatements = 10

    }
  }

  # Conversion-specific settings
  codec {

    # The locale to use for locale-sensitive conversions.
    # Defaults to en_US (US English).
    locale = en_US

    # The time zone to use for temporal conversions that do not convey any explicit time zone information.
    # Defaults to UTC.
    timeZone = UTC

    # All possible combinations for true:false (case insensitive); the first combination is considered the default
    # and used when formatting.
    boolean = ["1:0", "Y:N", "T:F", "YES:NO", "TRUE:FALSE"]

    # The DecimalFormat pattern to use for String-to-Number conversions.
    # See java.text.DecimalFormat for details about the pattern syntax to use.
    # Defaults to "#,###.##".
    number = "#,###.##"

    # The temporal pattern to use for String-to-CQL timestamp conversions.
    # This can be either:
    # - A date-time pattern;
    # - A pre-defined formatter such as ISO_DATE_TIME;
    # - Or the special value CQL_DATE_TIME, which is a special parser that accepts all valid CQL literal formats for
    #   the timestamp type.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to CQL_DATE_TIME.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for String-to-CQL date conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_DATE.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_DATE.
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for String-to-CQL time conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_TIME.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_TIME.
    time = "ISO_LOCAL_TIME"

  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter.
    # The console reporter will print useful metrics
    # about the ongoing operation at this rate.
    # Durations lesser than one second will be rounded up
    # to 1 second.
    # Defaults to 5 seconds.
    reportInterval = 5 seconds

    # The time unit to use when printing throughput rates.
    # Valid values: all TimeUnit enum constants.
    # Defaults to SECONDS.
    rateUnit = SECONDS

    # The time unit to use when printing latency durations.
    # Valid values: all TimeUnit enum constants.
    # Defaults to MILLISECONDS.
    durationUnit = MILLISECONDS

    # The expected total number of writes.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expectedWrites = -1

    # The expected total number of reads.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expectedReads = -1

    # Whether or not to enable JXM reporting.
    # Defaults to true.
    jmx = true

  }

  # Schema-specific settings.
  schema {

    # The keyspace to connect to.
    # If not specified, then the "statement" setting below must be specified.
    keyspace: null

    # The destination table.
    # If not specified, then the "statement" setting below must be specified.
    table: null

    # The INSERT or UPDATE statement to use to load data.
    # If not specified, the loader will infer the statement
    # based on the destination table's metadata using all availble columns.
    # Note that statements MUST use named bound variables;
    # positional bound variables will not work.
    # Their names usually match those of the columns in the destination table,
    # but this is not a strict requirement; it is however required that
    # their names match those specified in the mapping. See "mapping" setting below.
    statement: null

    # Values (case-sensitive) to map to null in the database when loading data.
    nullWords: []

    # Whether or not to map "null" input values to "unset" in the database, meaning don't
    # modify a potentially pre-existing value of this field for this row. "null"
    # input includes the values from the nullWords setting above.
    # Note that setting this to false leads to tombstones being created in the database
    # to represent null.
    # Defaults to true.
    nullToUnset: true

    # The field-to-column mapping to use.
    # If not specified, the loader will apply a strict one-to-one mapping
    # between the source fields and the target table.
    # If that is not what you want, then you must supply an explicit mapping.
    # Mappings should be specified as a HOCON map of the following form:
    # - Indexed data sources:
    #   { 0 = col1, 1 = col2, 2 = col3 }
    #   where 0, 1, 2, etc. are the zero-based indices of fields in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # - Mapped data sources:
    #   { fieldA = col1, fieldB = col2, fieldC = col3 }
    #   where fieldA, fieldB, fieldC, etc. are field names in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # The exact type of mapping to use depends on the connector being used.
    # Some connectors can only produce indexed records; others can only produce
    # mapped ones, while others are capable of producing both indexed and mapped
    # records at the same time. Refer to the connector's documentation
    # to know which kinds of mapping it supports.
    mapping {

      # unspecified by default

    }

  }

  # Connector-specific settings.
  # This section contains settings for the connector to use;
  # it also contains sub-sections, one for each available connector,
  # containing default values for every one of them.
  # What settings are available depend on which connector is being used,
  # and default values shown here will depend on which connectors are available;
  # please consult each connector's documentation for more information.
  connector {

    # The name of the connector to use.
    # Names can be specified either by a fully-qualified class name
    # of a class implementing com.datastax.loader.connectors.api.Connector;
    # or by a short name, in which case it is assumed that the name
    # should match that of a known connector implementation.
    # The matching is of type "starts-with" and is case insensitive;
    # e.g."csv" will match CSVConnector and "json" will match JsonConnector.
    # This setting has no default value and must be supplied by the user.
    name = null

  }

}
