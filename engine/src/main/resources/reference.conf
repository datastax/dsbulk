# Reference configuration for the DataStax Bulk Loader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
#
# Note that a paragraph is written in one line, and paragraphs are separated by a blank line.
# This has the benefit of rendering well in markdown as well as plain-text help output (since
# the help text formatter wraps lines appropriately).
dsbulk {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster. This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some platforms).
    hosts = ["127.0.0.1"]

    # The native transport port to connect to. This must match DSE's [native_transport_port](https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html#configCassandra_yaml_r__native_transport_port) configuration option.
    #
    # Note that all nodes in a cluster must accept connections on the same port number. Mixed-port clusters are not supported.
    port = 9042

    # Native Protocol-specific settings.
    protocol {

      # Specify the compression algorithm to use. Valid values are: `NONE`, `LZ4`, `SNAPPY`.
      compression = NONE

    }

    # Pooling-specific settings.
    #
    # The driver maintains a connection pool to each node, according to the distance assigned to it by the load balancing policy. If the distance is `IGNORED`, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool for nodes at "local" distance.
        connections = 8

        # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool for remote nodes.
        connections = 1

        # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver sends a dummy message on it to make sure it's still alive. If not, the connection is trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both loading and unloading. Valid values are: `ANY`, `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`, `EACH_QUORUM`, `ALL`.
      #
      # Note that stronger consistency levels usually result in reduced throughput; besides, any level higher than `ONE` will automatically disable continuous paging, which can dramatically reduce read throughput.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes. Only applicable if the data is inserted using lightweight transactions, ignored otherwise. Valid values are: `SERIAL` and `LOCAL_SERIAL`.
      serialConsistency = LOCAL_SERIAL

      # The page size, or how many rows will be retrieved simultaneously in a single network round trip. This setting will limit the number of results loaded into memory simultaneously during unloading. Not applicable for loading.
      fetchSize = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {
      # The time the driver waits for a request to complete. This is a global limit on the duration of a `session.execute()` call, including any internal retries the driver might do.
      readTimeout = 60 seconds

    }

    # Authentication settings.
    auth {

      # The name of the AuthProvider to use. Valid choices are:
      #
      #  - None: no authentication.
      #  - PlainTextAuthProvider: Uses `com.datastax.driver.core.PlainTextAuthProvider` for authentication. Supports SASL authentication using the `PLAIN` mechanism (plain text authentication).
      #  - DsePlainTextAuthProvider: Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `PLAIN` mechanism (plain text authentication), and also supports optional proxy authentication; should be preferred to `PlainTextAuthProvider` when connecting to secured DSE clusters.
      #  - DseGSSAPIAuthProvider: Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `GSSAPI` mechanism (Kerberos authentication), and also supports optional proxy authentication.
      #    - Note: When using this provider you may have to set the `java.security.krb5.conf` system property to point to your `krb5.conf` file (e.g. set the `DSBULK_JAVA_OPTS` environment variable to `-Djava.security.krb5.conf=/home/user/krb5.conf`). See the [Oracle Java Kerberos documentation](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html) for more details.
      provider = None

      # The username to use. Providers that accept this setting:
      #
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      username = ""

      # The password to use. Providers that accept this setting:
      #
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      password = ""

      # An authorization ID allows the currently authenticated user to act as a different user (proxy authentication). Providers that accept this setting:
      #
      #  - `DsePlainTextAuthProvider`
      #  - `DseGSSAPIAuthProvider`
      authorizationId = ""

      # The Kerberos principal to use. For example, `user@datastax.com`. If left unspecified, the principal is chosen from the first key in the ticket cache or keytab. Providers that accept this setting:
      #
      #  - `DseGSSAPIAuthProvider`
      principal = ""

      # The path of the Kerberos keytab file to use for authentication. If left unspecified, authentication uses a ticket cache. Providers that accept this setting:
      #
      #  - `DseGSSAPIAuthProvider`
      keyTab = ""

      # The SASL service name to use. This value should match the username of the Kerberos service principal used by the DSE server. This information is specified in the `dse.yaml` file by the *service_principal* option under the *kerberos_options* section, and may vary from one DSE installation to another - especially if you installed DSE with an automated package installer. Providers that accept this setting:
      #
      #  - `DseGSSAPIAuthProvider`
      saslService = "dse"

    }

    # Encryption-specific settings.
    #
    # For more information about how to configure this section, see the Java Secure Socket Extension (JSSE) Reference Guide: http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can also check the DataStax Java driver documentation on SSL: http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ssl {

      # The SSL provider to use. Valid values are:
      #
      # - **None**: no SSL.
      # - **JDK**: uses the JDK SSLContext
      # - **OpenSSL**: uses Netty's native support for OpenSSL. It provides better performance and generates less garbage. This is the recommended provider when using SSL.
      provider = None

      # The cipher suites to enable. For example:
      #
      # `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
      #
      # This property is optional. If it is not present, the driver won't explicitly enable cipher suites, which according to the JDK documentation results in "a minimum quality of service".
      cipherSuites = []

      # The truststore to use to validate remote peer certificates. This section is valid for both JDK and OpenSSL providers.
      truststore {

        # The path of the truststore file. This setting is optional. If left unspecified, server certificates will not be validated.
        path = ""

        # The truststore password.
        password = ""

        # The algorithm to use for the SSL truststore. Valid values are: `PKIX`, `SunX509`.
        algorithm = SunX509

      }

      # The keystore to use for client authentication.
      #
      # This section is only valid when using JDK provider; it is ignored otherwise.
      keystore {

        # The path of the keystore file. This setting is optional. If left unspecified, no client authentication will be used.
        path = ""

        # The keystore password.
        password = ""

        # The algorithm to use for the SSL keystore. Valid values are: `SunX509`, `NewSunX509`.
        algorithm = SunX509

      }

      # OpenSSL configuration for client authentication. This section is only valid when using OpenSSL provider; it is ignored otherwise.
      openssl {

        # The path of the certificate chain file. This setting is optional. If left unspecified, no client authentication will be used.
        keyCertChain = ""

        # The path of the private key file.
        privateKey = ""

      }

    }

    # The simple or fully-qualified class name of the timestamp generator to use. Built-in options are:
    #
    # - AtomicMonotonicTimestampGenerator: timestamps are guaranteed to be unique across all client threads.
    # - ThreadLocalTimestampGenerator: timestamps are guaranteed to be unique within each thread only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    timestampGenerator = AtomicMonotonicTimestampGenerator

    # The simple or fully-qualified class name of the address translator to use. This is only needed if the nodes are not directly reachable from the machine on which dsbulk is running (for example, the dsbulk machine is in a different network region and needs to use a public IP, or it connects through a proxy).
    addressTranslator = IdentityTranslator

    # Settings for various driver policies.
    policy {

      # Maximum number of retries for a timed-out request.
      maxRetries = 10

      # Load balancing policy settings
      lbp {

        # The name of the load balancing policy. Supported policies include: `dse`, `dcAwareRoundRobin`, `roundRobin`, `whiteList`, `tokenAware`. Available options for the policies are listed below as appropriate. For more information, refer to the driver documentation for the policy. If not specified, defaults to the driver's default load balancing policy, which is currently the `DseLoadBalancingPolicy` wrapping a `TokenAwarePolicy`, wrapping a `DcAwareRoundRobinPolicy`.
        #
        # Note: It is critical for a token-aware policy to be used in the chain in order to benefit from batching by partition key.
        name = ""

        # Settings for the DseLoadBalancingPolicy. See the driver documentation for this policy for more details.
        dse {

          # The child policy that the specified `dse` policy wraps.
          childPolicy = "roundRobin"

        }

        # Settings for the DCAwareRoundRobinPolicy. See the driver documentation for this policy for more details.
        dcAwareRoundRobin {

          # The datacenter name (commonly dc1, dc2, etc.) local to the machine on which dsbulk is running, so that requests are sent to nodes in the local datacenter whenever possible.
          localDc = ""

          # Enable or disable whether to allow remote datacenters to count for local consistency level in round robin awareness.
          allowRemoteDCsForLocalConsistencyLevel = false

          # The number of hosts per remote datacenter that the round robin policy should consider.
          usedHostsPerRemoteDc = 0

        }

        # Settings for the TokenAwarePolicy. See the driver documentation for this policy for more details.
        tokenAware {

          # The child policy that the specified `tokenAware` policy wraps.
          childPolicy = "roundRobin"

          # Specify how to order replicas.
          # 
          # Valid values are all `TokenAwarePolicy.ReplicaOrdering` enum constants:
          #
          # - RANDOM: Return replicas in a different, random order for each query plan. This is the default strategy;
          # for loading, it should be preferred has it can improve performance by distributing writes across replicas.
          # - TOPOLOGICAL: Order replicas by token ring topology, i.e. always return the "primary" replica first.
          # - NEUTRAL: Return the replicas in the exact same order in which they appear in the child policy's query plan.
          replicaOrdering = RANDOM

        }

        # Settings for the WhiteListPolicy. See the driver documentation for this policy for more details.
        whiteList {

          # The child policy that the specified `whiteList` policy wraps.
          childPolicy = "roundRobin"

          # List of hosts to white list. This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some platforms).
          hosts = []

        }

      }

    }

  }

  # Log and error management settings.
  log {

    # The writable directory where all log files will be stored; if the directory specified does not exist, it will be created. URLs are not acceptable (not even `file:/` URLs). Log files for a specific run, or execution, will be located in a sub-directory under the specified directory. Each execution generates a sub-directory identified by an "execution ID". See `engine.executionId` for more information about execution IDs. Relative paths will be resolved against the current working directory. Also, for convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the current user's home directory.
    directory = "./logs"

    # The maximum number of errors to tolerate before aborting the entire operation. Set to either a number or a string of the form `N%` where `N` is a decimal number between 0 and 100. Setting this value to `-1` disables this feature (not recommended).
    maxErrors = 100

    # Whether or not to use ANSI colors and other escape sequences in log messages printed to standard output and standard error.
    #
    # By default, DSBulk will use colored output when:
    #
    # - The terminal is compatible with ANSI escape sequences; all common terminals on *nix and BSD systems, including MacOS, are ANSI-compatible, as well as some popular terminals for Windows, such as Mintty (distributed with Cygwin) and MinGW.
    # - The terminal is the standard Windows DOS command prompt (in which case, ANSI sequences are translated on the fly).
    #
    # There should be no reason to disable ANSI escape sequences, but if, for some reason, colored messages are not desired or not printed correctly, this option allows disabling ANSI support altogether.
    ansiEnabled = true

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired log level. Valid values are:
      #
      # - ABRIDGED: Print only basic information in summarized form.
      # - NORMAL: Print basic information in summarized form, and the statement's query string, if available. For batch statements, this verbosity level also prints information about the batch's inner statements.
      # - EXTENDED: Print full information, including the statement's query string, if available, and the statement's bound values, if available. For batch statements, this verbosity level also prints all information available about the batch's inner statements.
      level = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxQueryStringLength = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit, the exceeding values will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValues = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValueLength = 50

      # The maximum number of inner statements to print for a batch statement. Only applicable for batch statements, ignored otherwise. If the batch statement has more children than this value, the exceeding child statements will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxInnerStatements = 10

    }

  }

  # Conversion-specific settings. These settings apply for both load and unload workflows.
  #
  # When writing, these settings determine how record fields emitted by connectors are parsed.
  #
  # When unloading, these settings determine how row cells emitted by DSE are formatted.
  codec {

    # The locale to use for locale-sensitive conversions.
    locale = en_US

    # The time zone to use for temporal conversions, when the input being parsed or formatted does not convey any explicit time zone information.
    timeZone = UTC

    # Comma-separated list of values that should be mapped to `null`. For loading, when a record field value exactly matches one of the specified strings, the value is replaced with `null` before writing to DSE. For unloading, this setting is only applicable for string-based connectors, such as the CSV connector: the first string specified will be used to change a row cell containing `null` to the specified string when written out. By default, empty strings are converted to `null` while loading, and `null` is converted to an empty string while unloading. This setting is applied before `schema.nullToUnset`, hence any `null` produced by a null-word can still be left unset if required.
    nullStrings: [""]

    # Specify how true and false representations can be used by dsbulk. Each representation is of the form `true-value:false-value`, case-insensitive. For loading, all representations are honored: when a record field value exactly matches one of the specified strings, the value is replaced with `true` of `false` before writing to DSE. For unloading, this setting is only applicable for string-based connectors, such as the CSV connector: the first representation will be used to format booleans before they are written out, and all others are ignored.
    booleanStrings = ["1:0", "Y:N", "T:F", "YES:NO", "TRUE:FALSE"]

    # Set how true and false representations of numbers are interpreted. The representation is of the form `true_value,false_value`. The mapping is reciprocal, so that numbers are mapping to Boolean and vice versa. All numbers unspecified in this setting are rejected.
    booleanNumbers = [1, 0]

    # The `DecimalFormat` pattern to use for conversions between `String` and CQL numeric types.
    #
    # See [java.text.DecimalFormat](https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html) for details about the pattern syntax to use.
    #
    # The default pattern is `#,###.##`. This format recognizes almost every input, with an optional, localized thousands separator, and a localized decimal separator. It also recognizes an optional exponent. When used with locale `en_US`, the following inputs are all valid when parsing: `1234`, `1,234`, `1234.5678`, `1,234.5678`, `1,234.5678E2`.
    #
    # Beware that, when unloading / formatting, rounding may be necessary and could incur in precision loss. You can specify the rounding strategy to apply, see `roundingStrategy` below.
    #
    # In addition to the pattern specified here, DSBulk will always recognize Java's numeric notation as valid input, both decimal and hexadecimal. Thus, regardless of the pattern being used, the following examples are always correctly parsed:
    #
    # - Decimal notation: `1.234e+56`
    # - Hexadecimal notation: `0x1.fffP+1023`
    number = "#,###.##"

    # Whether or not to use the `number` pattern to format numeric output.
    #
    # Only applicable when unloading, and only if the connector in use requires stringification (i.e., if it does not handle raw numeric data, e.g. the CSV connector); ignored otherwise.
    #
    # When `true`, the numeric pattern defined under `number` will be used to format all numbers. This allows for nicely-formatted output, but may result in rounding (see `roundingStrategy`), or alteration of the original decimal's scale. When `false`, numbers will be simply stringified using their `toString()` method. This is the safest choice as it never results in rounding nor in scale alteration.
    formatNumbers = false

    # The rounding strategy to use for conversions from CQL numeric types to `String`.
    #
    # Only applicable when unloading, if `formatNumbers` is true and if the connector in use requires stringification (i.e., if it does not handle raw numeric data, e.g. the CSV connector); ignored otherwise.
    #
    # Valid choices: any `java.math.RoundingMode` enum constant name, that is: `CEILING`, `FLOOR`, `UP`, `DOWN`, `HALF_UP`, `HALF_EVEN`, `HALF_DOWN`, and `UNNECESSARY`.
    #
    # The precision used when rounding is inferred from the numeric pattern declared under `number`. For example, `#,###.##` allows up to 2 fraction digits, so the rounding precision will be 2; 123.456 would be rounded to 123.46 with this pattern and strategy `UP`.
    #
    # The default is `UNNECESSARY` because this is the only strategy that allows to export numeric data without precision loss. Note however that this strategy will result in infinite precision and thus will print decimal numbers with as many fraction digits as required, regardless of the number of fraction digits declared in the numeric pattern in use.
    roundingStrategy = UNNECESSARY

    # The overflow strategy to use when converting from `String` to CQL numeric types.
    #
    # "Overflow" should be understood here in 3 possible ways:
    #
    # - The value is out of the target CQL type's range. For example, trying to convert 128 to a CQL `tinyint` results in overflow, as the maximum value for such type is 127.
    # - The value is decimal, but the target CQL type is integral. For example, trying to convert 123.45 to a CQL `int` results in overflow.
    # - The value's precision is too large for the target CQL type. For example, trying to insert 0.1234567890123456789 into a CQL `double` results in overflow as there are too many significant digits to fit in a 64-bit double.
    #
    # Valid choices:
    #
    # - `REJECT`: overflows are considered errors and the data is rejected. This is the default value.
    # - `TRUNCATE`: the data is truncated to fit in the target CQL type. The truncation algorithm is similar to the narrowing primitive conversion defined in section 5.1.3 of The Java Language Specification, with the following exceptions:
    #     - If the value is too big or too small, it is rounded up or down to the maximum or minimum value allowed, rather than truncated at bit level. For example, 128 would be rounded down to 127 to fit in a byte, whereas Java would have truncated the exceeding bits and converted to -127 instead.
    #     - If the value is decimal, but the target CQL type is integral, it is first rounded to an integral using the defined rounding strategy, then narrowed to fit into the target type.
    #
    # Beware that `TRUNCATE` may result in precision loss and should be used with caution.
    #
    # Note: this setting only applies for loading, when parsing numeric inputs; it does not apply for unloading, since formatting never results in any of the overflow situations outlined above (but it may involve rounding â€“ see `roundingStrategy`).
    overflowStrategy = REJECT

    # The temporal pattern to use for `String` to CQL timestamp conversions. Valid choices:
    #
    # - A date-time pattern such as `yyyy-MM-dd HH:mm:ss`.
    # - A pre-defined formatter such as `ISO_ZONED_DATE_TIME` or `ISO_INSTANT`. Any public static field in `java.time.format.DateTimeFormatter` can be used.
    # - The special value `CQL_DATE_TIME`, which is a special parser that accepts most CQL date, time and timestamp literals (see below).
    #
    # For more information on patterns and pre-defined formatters, see [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    #
    # For more information about CQL date, time and timestamp literals, see [Date, time, and timestamp format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    #
    # The default value is the special `CQL_DATE_TIME` value. When parsing, this format recognizes all CQL temporal literals; if the input is a local date, the timestamp is resolved using the time zone specified under `timeZone`, at midnight. When formatting, this format produces formatted output of the following form: '2011-12-03 10:15:30.567 CEST'.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for `String` to CQL date conversions. Valid choices:
    #
    # - A date-time pattern such as `yyyy-MM-dd`.
    # - A pre-defined formatter such as `ISO_LOCAL_DATE`. Any public static field in `java.time.format.DateTimeFormatter` can be used.
    #
    # For more information on patterns and pre-defined formatters, see [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    #
    # For more information about CQL date, time and timestamp literals, see [Date, time, and timestamp format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for `String` to CQL time conversions. Valid choices:
    #
    # - A date-time pattern such as `HH:mm:ss`.
    # - A pre-defined formatter such as `ISO_LOCAL_TIME`. Any public static field in `java.time.format.DateTimeFormatter` can be used.
    #
    # For more information on patterns and pre-defined formatters, see [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    #
    # For more information about CQL date, time and timestamp literals, see [Date, time, and timestamp format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    time = "ISO_LOCAL_TIME"

    # The time unit to use when converting numeric input to CQL temporal types.
    #
    # All `TimeUnit` enum constants are valid choices.
    #
    # Numeric inputs are converted using this unit and the instant specified under `epoch`. For example, if the unit is `DAYS` and the epoch is `2000-01-01T00:00:00Z`, then the number 31 is converted to `2000-01-01` + 31 days, that is, `2000-02-01T00:00:00Z`.
    #
    #  The default values for `unit` and `epoch` result in numeric input being interpreted as milliseconds since Unix Epoch (1970-01-01).
    unit = MILLISECONDS

    # The instant, or "epoch", to use when converting numeric input to CQL temporal types, and when converting local times to full timestamps.
    #
    # The value must be expressed in [`ISO_ZONED_DATE_TIME`](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_ZONED_DATE_TIME) format.
    #
    # Numeric inputs are converted using this instant and the unit specified under `unit`. For example, if the unit is `DAYS` and the epoch is `2000-01-01T00:00:00Z`, then the number 31 is converted to `2000-01-01` + 31 days, that is, `2000-02-01T00:00:00Z`.
    #
    # When local times need to be converted to full timestamps, they are converted using the local date extracted from this value. For example, if the epoch is `2000-01-01T00:00:00Z`, then the local time `12:34:56` is converted to `2000-01-01T12:34:56Z`.
    #
    #  The default values for `unit` and `epoch` result in numeric input being interpreted as milliseconds since Unix Epoch (1970-01-01).
    epoch = "1970-01-01T00:00:00Z"

    # Strategy to use when generating time-based (version 1) UUIDs from timestamps. Clock sequence and node ID parts of generated UUIDs are determined on a best-effort basis and are not fully compliant with RFC 4122. Valid values are:
    #
    # - RANDOM: Generates UUIDs using a random number in lieu of the local clock sequence and node ID. This strategy will ensure that the generated UUIDs are unique, even if the original timestamps are not guaranteed to be unique.
    # - FIXED: Preferred strategy if original timestamps are guaranteed unique, since it is faster. Generates UUIDs using a fixed local clock sequence and node ID.
    # - MIN: Generates the smallest possible type 1 UUID for a given timestamp. Warning: this strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    # - MAX: Generates the biggest possible type 1 UUID for a given timestamp. Warning: this strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    uuidStrategy = RANDOM
  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter. The console reporter will print useful metrics about the ongoing operation at this rate. Durations lesser than one second will be rounded up to 1 second.
    reportRate = 5 seconds

    # The time unit used when printing throughput rates. Valid values: all `TimeUnit` enum constants.
    rateUnit = SECONDS

    # The time unit used when printing latency durations. Valid values: all `TimeUnit` enum constants.
    durationUnit = MILLISECONDS

    # The expected total number of writes. Optional, but if set, the console reporter will also print the overall achievement percentage. Setting this value to `-1` disables this feature.
    expectedWrites = -1

    # The expected total number of reads. Optional, but if set, the console reporter will also print the overall achievement percentage. Setting this value to `-1` disables this feature.
    expectedReads = -1

    # Enable or disable JMX reporting. Note that to enable remote JMX reporting, several properties must also be set in the JVM during launch. This is accomplished via the `DSBULK_JAVA_OPTS` environment variable.
    jmx = true

    # Enable or disable CSV reporting. If enabled, CSV files containing metrics will be generated in the designated log directory.
    csv = false

  }

  # Schema-specific settings.
  schema {

    # Keyspace used for loading or unloading data. Required option if `schema.query` is not specified; otherwise, optional.
    #
    # Keyspace names should not be quoted and will be treated in a case-sensitive manner, i.e., `MyKeyspace` will match
    # a keyspace named "MyKeyspace" but will not match a keyspace named "mykeyspace".
    keyspace: ""

    # Table used for loading or unloading data. Required option if `schema.query` is not specified; otherwise, optional.
    #
    # Table names should not be quoted and will be treated in a case-sensitive manner, i.e., `MyTable` will match
    # a table named "MyTable" but will not match a table named "mytable".
    table: ""

    # The query to use. If not specified, then *schema.keyspace* and *schema.table* must be specified, and dsbulk will infer the appropriate statement based on the table's metadata, using all available columns. If `schema.keyspace` is provided, the query need not include the keyspace to qualify the table reference.
    #
    # For loading, the statement can be any `INSERT` or `UPDATE` statement, but must use named bound variables exclusively; positional bound variables will not work. Bound variable names usually match those of the columns in the destination table, but this is not a strict requirement; it is, however, required that their names match those specified in the mapping.
    #
    # For unloading, the statement can be any regular `SELECT` statement; it can optionally contain a token range restriction clause of the form: `token(...) > :start and token(...) <= :end`. If such a clause is present, the engine will generate as many statements as there are token ranges in the cluster, thus allowing parallelization of reads while at the same time targeting coordinators that are also replicas. The column names in the SELECT clause will be used to match column names specified in the mapping.
    #
    # Note: The dsbulk query is parsed to discover which bound variables are present, to map the variable correctly to fields.
    #
    # See *schema.mapping* setting for more information.
    query: ""

    # Specify whether to map `null` input values to "unset" in the database, i.e., don't modify a potentially pre-existing value of this field for this row. Valid for load scenarios, otherwise ignore. Note that setting to false creates tombstones to represent `null`.
    #
    # Note that this setting is applied after the *codec.nullStrings* setting, and may intercept `null`s produced by that setting.
    nullToUnset: true

    # The field-to-column mapping to use, that applies to both loading and unloading. If not specified, the loader will apply a strict one-to-one mapping between the source fields and the database table. If that is not what you want, then you must supply an explicit mapping. Mappings should be specified as a map of the following form:
    #
    # - Indexed data sources: `0 = col1, 1 = col2, 2 = col3`, where `0`, `1`, `2`, are the zero-based indices of fields in the source data; and `col1`, `col2`, `col3` are bound variable names in the insert statement.
    #     - A shortcut to map the first `n` fields is to simply specify the destination columns: `col1, col2, col3`.
    # - Mapped data sources: `fieldA = col1, fieldB = col2, fieldC = col3`, where `fieldA`, `fieldB`, `fieldC`, are field names in the source data; and `col1`, `col2`, `col3` are bound variable names in the insert statement.
    #
    # To specify that a field should be used as the timestamp (a.k.a. write-time) or ttl (a.k.a. time-to-live) of the inserted row, use the specially named fake columns `__ttl` and `__timestamp`: `fieldA = __timestamp, fieldB = __ttl`. Note that Timestamp fields are parsed as regular CQL timestamp columns and must comply with either `codec.timestamp`, or alternatively, with `codec.unit` + `codec.epoch`. TTL fields are parsed as integers representing durations in seconds, and must comply with `codec.number`.
    #
    # To specify that a column should be populated with the result of a function call, specify the function call as the input field (e.g. `now() = c4`). Note, this is only relevant for load operations. In addition, for mapped data sources, it is also possible to specify that the mapping be partly auto-generated and partly explicitly specified. For example, if a source row has fields `c1`, `c2`, `c3`, and `c5`, and the table has columns `c1`, `c2`, `c3`, `c4`, one can map all like-named columns and specify that `c5` in the source maps to `c4` in the table as follows: `* = *, c5 = c4`.
    #
    # One can specify that all like-named fields be mapped, except for `c2`: `* = -c2`. To skip `c2` and `c3`: `* = [-c2, -c3]`.
    #
    # The exact type of mapping to use depends on the connector being used. Some connectors can only produce indexed records; others can only produce mapped ones, while others are capable of producing both indexed and mapped records at the same time. Refer to the connector's documentation to know which kinds of mapping it supports.
    mapping: ""

    # Whether or not to accept records that contain extra fields that are not declared in the mapping. Only applicable for loads, ignored otherwise.
    #
    # For example, if a record contains 3 fields A, B, C, but the mapping only declares fields A and B, then if this option is `true`, C will be silently ignored and the record will be considered valid; if it is `false`, the record will be rejected.
    allowExtraFields = true

    # Whether or not to accept records that are missing fields declared in the mapping. Only applicable for loads, ignored otherwise.
    #
    # For example, if the mapping declares 3 fields A, B, C, but a record contains only A and B, then if this option is `true`, C will be silently assigned `null` and the record will be considered valid; if it is `false`, the record will be rejected.
    #
    # Note: if the missing field is mapped to a primary key column, the record will be rejected regardless of this option's value, since such a record would be rejected by the database anyway.
    allowMissingFields = false

    # The Time-To-Live (TTL) of inserted/updated cells during load (seconds); a value of -1 means there is no TTL. Not applicable to unloading. For more information, see the [CQL Reference](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__ime-value), [Setting the time-to-live (TTL) for value](http://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useTTL.html) and [Expiring data with time-to-live](http://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html).
    queryTtl = -1

    # The timestamp of inserted/updated cells during load; otherwise, the current time of the system running the tool is used. Not applicable to unloading.
    #
    # The value must be expressed in [`ISO_ZONED_DATE_TIME`](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_ZONED_DATE_TIME) format.
    #
    # Query timestamps for DSE have microsecond resolution; any sub-microsecond information specified is lost. For more information, see the [CQL Reference](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__timestamp-value).
    queryTimestamp = ""
  }

  # Connector-specific settings. This section contains settings for the connector to use; it also contains sub-sections, one for each available connector.
  connector {

    # The name of the connector to use.
    name = "csv"

  }


  # Batch-specific settings.
  #
  # These settings control how the workflow engine groups together statements before writing them.
  #
  # Only applicable for loading.
  #
  # See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
  batch {

    # Enable or disable statement batching.
    enabled = true

    # The grouping mode. Valid values are:
    # - PARTITION_KEY: Groups together statements that share the same partition key. This is the default mode, and the preferred one.
    # - REPLICA_SET: Groups together statements that share the same replica set. This mode might yield better results for small clusters and lower replication factors, but tends to perform equally well or worse than `PARTITION_KEY` for larger clusters or high replication factors.
    mode = PARTITION_KEY

    # The maximum batch size that depends on the size of the data inserted and the batch mode in use. Larger data requires a smaller value. For batch mode, `PARTITION_KEY` requires larger batch sizes, whereas `REPLICA_SET` requires smaller batch sizes, such as below 10.
    maxBatchSize = 32

    # The buffer size to use for flushing batching statements. Do not set higher than `maxBatchSize` unless the loaded data is unsorted, when a higher value could improve performance. When set to a negative value the buffer size is implicitly set to `maxBatchSize`.
    bufferSize = -1

  }

  # Executor-specific settings.
  executor {

    # The maximum number of "in-flight" requests, or maximum number of concurrent requests waiting for a response from the server. This acts as a safeguard to prevent more requests than the cluster can handle. Batch statements count as one request. Reduce this value when the throughput for reads and writes cannot match the throughput of mappers; this is usually a sign that the workflow engine is not well calibrated and will eventually run out of memory. Setting this option to any negative value will disable it.
    maxInFlight = 1024

    # The maximum number of concurrent operations per second. This acts as a safeguard to prevent more requests than the cluster can handle. Batch statements are counted by the number of statements included. Reduce this setting when the latencies get too high and a remote cluster cannot keep up with throughput, as `dsbulk` requests will eventually time out. Setting this option to any negative value will disable it.
    maxPerSecond = -1

    # Continuous-paging specific settings.
    #
    # Only applicable for unloads, and only if this feature is available in the remote cluster, ignored otherwise.
    continuousPaging {

      # Enable or disable continuous paging.
      #
      # Note that if the target cluster does not support continuous paging, or if the consistency level is not `ONE` or `LOCAL_ONE` (see `driver.query.consistency`), traditional paging will be used regardless of this setting.
      enabled = true

      # The unit to use for the `pageSize` setting. Possible values are: `ROWS`, `BYTES`.
      pageUnit = ROWS

      # The size of the page. The unit to use is determined by the `pageUnit` setting.
      pageSize = 5000

      # The maximum number of pages to retrieve. Setting this value to zero retrieves all pages available.
      maxPages = 0

      # The maximum number of pages per second. Setting this value to zero indicates no limit.
      maxPagesPerSecond = 0

    }

  }

  # Workflow Engine-specific settings.
  engine {

    # Enable or disable dry-run mode, a test mode that runs the command but does not load data. Not applicable for unloading.
    dryRun = false

    # A unique identifier to attribute to each execution. When unspecified or empty, the engine will automatically generate identifiers of the following form: *workflow*_*timestamp*, where :
    #
    # - *workflow* stands for the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - *timestamp* is the current timestamp formatted as `uuuuMMdd-HHmmss-SSSSSS` (see [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns)) in UTC, with microsecond precision if available, and millisecond precision otherwise.
    #
    # When this identifier is user-supplied, it is important to guarantee its uniqueness; failing to do so may result in execution failures. It is also possible to provide templates here. Any format compliant with the formatting rules of [`String.format()`](https://docs.oracle.com/javase/8/docs/api/java/util/Formatter.html#syntax) is accepted, and can contain the following parameters:
    #
    # - `%1$s` : the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - `%2$t` : the current time (with microsecond precision if available, and millisecond precision otherwise);
    # - `%3$s` : the JVM process PID (this parameter might not be available on some operating systems; if its value cannot be determined, a random integer will be inserted instead).
    executionId = ""

  }

  # This group of settings is purely internal to the connector and are the interface for
  # DSBulk's infrastructure to customize how some settings are exposed to the user.
  #
  # In particular, how settings are documented and shortcut options that map to
  # settings that are commonly specified in the command line.
  metaSettings {
    # Specify how settings should be prioritized in generated docs and help.
    docHints {
      commonSettings = [
        schema.keyspace,
        schema.table,
        schema.mapping,
        engine.dryRun,
        driver.hosts,
        driver.port,
        driver.auth.username,
        driver.auth.password,
        driver.query.consistency,
        executor.maxPerSecond,
        log.maxErrors,
        log.directory,
        monitoring.reportRate
      ]
      preferredSettings = [
        driver.auth.provider,
        driver.policy.lbp.name
      ]
    }

    # Specify shortcuts for "long" options.
    shortcuts {
      locale = codec.locale
      timeZone = codec.timeZone
      c = connector.name
      p = driver.auth.password
      u = driver.auth.username
      h = driver.hosts
      lbp = driver.policy.lbp.name
      maxRetries = driver.policy.maxRetries
      port = driver.port
      cl = driver.query.consistency
      dryRun = engine.dryRun
      maxErrors = log.maxErrors
      logDir = log.directory
      jmx = monitoring.jmx
      reportRate = monitoring.reportRate
      k = schema.keyspace
      m = schema.mapping
      nullStrings = codec.nullStrings
      query = schema.query
      t = schema.table
    }
  }
}
