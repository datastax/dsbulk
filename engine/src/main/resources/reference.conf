# Reference configuration for the DataStax Loader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
datastax-loader {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster.
    # This must be a list of strings with each contact point's host-name or ip address. If the host is
    # a DNS name that resolves to multiple A-records, all the corresponding addresses will be used.
    # Do not use "localhost" as the host name (since it resolves to both IPv4 and IPv6 addresses on
    # some platforms).
    # Defaults to ["127.0.0.1"].
    contactPoints = ["127.0.0.1"]

    # The port to connect to on contact points.
    # Note that all nodes in a cluster must share the same port.
    # Defaults to 9042.
    port = 9042

    # Native Protocol-specific settings.
    protocol {

      # The native protocol version to use.
      # Valid values are: `V3`, `V4`, `V5`, `DSE_V1`.
      # This option is not required. If it is absent, the driver will negotiate it.
      version = null

      # The compression algorithm to use.
      # Valid values are: NONE, LZ4, SNAPPY.
      # Defaults to LZ4.
      compression = LZ4

    }

    # Pooling-specific settings.
    # The driver maintains a connection pool to each node, according to the distance assigned to it
    # by the load balancing policy. If the distance is IGNORED, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver
      # sends a dummy message on it to make sure it's still alive. If not, the connection is
      # trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both reads and writes.
      # Allowed values are:
      # ANY, LOCAL_ONE, ONE, TWO, THREE, LOCAL_QUORUM, QUORUM, EACH_QUORUM, ALL.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes.
      # The allowed values are: SERIAL and LOCAL_SERIAL.
      # Only applicable if the data is inserted using lightweight transactions,
      # ignored otherwise.
      serialConsistency = LOCAL_SERIAL

      # The page size. This controls how many rows will be retrieved simultaneously in a single
      # network round trip (the goal being to avoid loading too many results in memory at the same
      # time).
      # Only applicable in read scenarios, ignored otherwise.
      fetchSize = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {

      # How long the driver waits for a request to complete. This is a global limit on the duration
      # of a session.execute() call, including any internal retries the driver might do.
      readTimeout = 12 seconds

    }

    # Authentication settings.
    auth {

      # The name of the AuthProvider to use.
      # This property is optional; if it is not present, no authentication will occur.
      # Valid values are:
      #  - PlainTextAuthProvider:
      #    Uses com.datastax.driver.core.PlainTextAuthProvider for authentication.
      #    Supports SASL authentication using the PLAIN mechanism (plain text authentication).
      #  - DsePlainTextAuthProvider:
      #    Uses com.datastax.driver.dse.auth.DsePlainTextAuthProvider for authentication.
      #    Supports SASL authentication to DSE clusters using the PLAIN mechanism (plain text authentication),
      #    and also supports optional proxy authentication; should be preferred to PlainTextAuthProvider
      #    when connecting to secured DSE clusters.
      #  - DseGSSAPIAuthProvider:
      #    Uses com.datastax.driver.dse.auth.DseGSSAPIAuthProvider for authentication.
      #    Supports SASL authentication to DSE clusters using the GSSAPI mechanism (Kerberos authentication),
      #    and also supports optional proxy authentication.
      provider = null

      # The username to use. Required.
      # Providers that accept this setting:
      #  - PlainTextAuthProvider
      #  - DsePlainTextAuthProvider
      username = null

      # The password to use. Required.
      # Providers that accept this setting:
      #  - PlainTextAuthProvider
      #  - DsePlainTextAuthProvider
      password = null

      # The authorization ID to use. Optional.
      # An authorization ID allows the currently authenticated user
      # to act as a different user (a.k.a. proxy authentication).
      # Providers that accept this setting:
      #  - DsePlainTextAuthProvider
      #  - DseGSSAPIAuthProvider
      authorizationId = null

      # The Kerberos principal to use. Required.
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      principal = null

      # The URL of the Kerberos keytab file to use for authentication, e.g.
      # "file:///path/to/my/keyTab".
      # Optional. If left unspecified, it is assumed that authentication will
      # be done with a ticket cache instead.
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      keyTab = null

      # The SASL protocol name to use. Required.
      # This value should match the username of the
      # Kerberos service principal used by the DSE server.
      # This information is specified in the dse.yaml file by the
      # service_principal option under the kerberos_options section,
      # and may vary from one DSE installation to another â€“ especially
      # if you installed DSE with an automated package installer.
      # Defaults to "dse".
      # Providers that accept this setting:
      #  - DseGSSAPIAuthProvider
      saslProtocol = "dse"

    }

    # Encryption-specific settings.
    # For more information about how to configure this section,
    # see the Java Secure Socket Extension (JSSE) Reference Guide:
    # http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html
    # You can also check the DataStax Java driver documentation on SSL:
    # http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ssl {

      # The SSL provider to use.
      # This property is optional; if it is not present, SSL won't be activated.
      # Valid values are:
      # - JDK: uses JDK's SSLContext
      # - OpenSSL: uses Netty's native support for OpenSSL
      # Using OpenSSL provides better performance and generates less garbage.
      # A disadvantage of using the OpenSSL provider is that, unlike the JDK provider,
      # it requires a platform-specific dependency, named "netty-tcnative",
      # which must be added manually to the loader's classpath
      # (typically by dropping its jar in the lib subdirectory of the Loader archive).
      # Follow these instructions to find out how to add this dependency:
      # http://netty.io/wiki/forked-tomcat-native.html
      provider = null

      # The cipher suites to enable.
      # Example: cipherSuites = [ "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA" ]
      # This property is optional. If it is not present, the driver won't explicitly enable cipher
      # suites, which according to the JDK documentation results in "a minimum
      # quality of service".
      cipherSuites = []

      # The truststore to use to validate remote peer certificates.
      # This section is valid for both JDK and OpenSSL providers.
      truststore {

        # The URL of the truststore file, e.g.
        # "file:///path/to/my/truststore".
        # This setting is optional. If left unspecified,
        # server certificates will not be validated.
        url = null

        # The truststore password.
        password = null

        # The algorithm to use.
        # Valid values are: PKIX, SunX509.
        # Defauls to "SunX509".
        algorithm = SunX509

      }

      # The keystore to use for client authentication.
      # This section is only valid when using JDK provider;
      # it is ignored otherwise.
      keystore {

        # The URL of the keystore file, e.g.
        # "file:///path/to/my/keystore".
        # This setting is optional. If left unspecified,
        # no client authentication will be used.
        url = null

        # The keystore password.
        password = null

        # The algorithm to use.
        # Valid values are: SunX509, NewSunX509.
        # Defauls to "SunX509".
        algorithm = SunX509

      }

      # OpenSSL configuration for client authentication.
      # This section is only valid when using OpenSSL provider;
      # it is ignored otherwise.
      openssl {

        # The URL of the certificate chain file, e.g.
        # "file:///path/to/my/keyCertChain".
        # This setting is optional. If left unspecified,
        # no client authentication will be used.
        keyCertChain = null

        # The URL of the private key file, e.g.
        # "file:///path/to/my/privateKey".
        privateKey = null
      }
    }

    # The fully-qualified class name of the timestamp generator to use.
    # Built-in options are:
    # - AtomicTimestampGenerator: timestamps are guaranteed to be unique across all client threads.
    # - ThreadLocalTimestampGenerator: timestamps that are guaranteed to be unique within each
    #   thread only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    # Defaults to com.datastax.driver.core.AtomicMonotonicTimestampGenerator.
    timestampGenerator = AtomicMonotonicTimestampGenerator

    # The fully-qualified class name of the address translator to use.
    # This is only needed if the nodes are not directly reachable from the driver (for example, the
    # driver is in a different network region and needs to use a public IP, or it connects through
    # a proxy).
    # Defaults to com.datastax.driver.core.policies.IdentityTranslator.
    addressTranslator = IdentityTranslator

    # Settings for various driver policies.
    policy {

      # The fully-qualified class name of the RetryPolicy implementation to use.
      # If not specified, defaults to the driver's default retry policy
      # (com.datastax.driver.core.policies.DefaultRetryPolicy).
      retry = null

      # The fully-qualified class name of the LoadBalancingPolicy implementation to use.
      # If not specified, defaults to the driver's default load balancing policy
      # (com.datastax.driver.dse.DseLoadBalancingPolicy).
      lbp = null

      # The fully-qualified class name of the SpeculativeExecutionPolicy implementation to use.
      # If not specified, defaults to the driver's default speculative execution policy
      # (to com.datastax.driver.core.policies.NoSpeculativeExecutionPolicy).
      specexec = null

    }

  }

  # Batch-specific settings.
  # These settings control how the workflow engine
  # groups together statements before writing them.
  # Only applicable for write workflows, ignored otherwise.
  # See com.datastax.loader.executor.api.batch.StatementBatcher for more information.
  batch {

    # The grouping mode.
    # Valid values are:
    # - PARTITION_KEY : Groups together statements that share the same partition key. This is the default mode, and
    #                   the preferred one.
    # - REPLICA_SET   : Groups together statements that share the same replica set. This mode might yield better
    #                   results for small clusters and lower replication factors, but tends to perform equally well
    #                   or even worse than PARTITION_KEY for larger clusters or high replication factors.
    # Defaults to PARTITION_KEY.
    mode = PARTITION_KEY

    # Whether the upstream is already sorted by grouping key or not.
    # Defaults to false.
    # This should only be set to true if the upstream is guaranteed to be sorted,
    # otherwise the batching might not function properly.
    # If in doubt, leave this setting to false.
    sorted = false

    # The buffer size to use when batching.
    # Defaults to 1000.
    bufferSize = 1000

  }

  # Executor-specific settings.
  executor {

    # The maximum number of threads to allocate for the executor.
    # These threads are used to submit requests and process responses.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4C.
    maxThreads = 4C

    # The maximum number of "in-flight" requests. In other words, sets the maximum number of
    # concurrent uncompleted futures waiting for a response from the server. This acts as a safeguard
    # against workflows that generate more requests than they can handle.
    # Batch statements count for as many requests as their number of inner statements.
    # Defaults to 1000.
    # Setting this option to any negative value will disable it.
    maxInflight = 1000

    # The maximum number of concurrent requests per second. This acts as a safeguard against
    # workflows that could overwhelm the cluster with more requests than it can handle.
    # Batch statements count for as many requests as their number of inner statements.
    # Defaults to 100,000.
    # Setting this option to any negative value will disable it.
    maxPerSecond = 100000

    # Continuous-paging specific settings.
    # Only applicable for reads, and only if this feature is available in the remote cluster,
    # ignored otherwise.
    continuousPaging {

      # The unit to use for the page-size setting.
      # Possible values are: ROWS, BYTES.
      # Defaults to ROWS.
      pageUnit = ROWS

      # The size of the page. The unit to use
      # is determined by the pageUnit settings.
      # Defaults to 5000 (rows).
      #
      pageSize = 5000

      # The maximum number of pages to retrieve.
      # Defaults to 0, which means retrieve all pages available.
      maxPages = 0

      # The maximum number of pages per second.
      # Defaults to 0, which indicates no limit.
      maxPagesPerSecond = 0

    }

  }

  # Log and error management settings.
  log {

    # The output directory where all log files will be stored.
    # Note that this must be a path pointing to a writable directory.
    # Log files for a specific run will be located in a sub-directory
    # inside the directory specified here. Each run generates
    # a sub-directory identified by an "operation ID', which
    # is basically a timestamp in the format: yyyy_MM_dd_HH_mm_ss_nnnnnnnnn.
    # Defaults to ".", which denotes the current working directory.
    outputDirectory = "."

    # The maximum number of threads to allocate to log files management.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4.
    maxThreads = 4

    # The maximum number of errors to tolerate before aborting
    # the entire operation.
    # Defaults to 100.
    # Setting this value to -1 disables this feature (not recommended).
    maxErrors = 100

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired verbosity. Possible values are:
      # - ABRIDGED : only prints basic information in summarized form.
      # - NORMAL   : prints basic information in summarized form, and the statement's query string,
      #              if available. For batch statements, this verbosity level also prints information
      #              about the batch's inner statements.
      # - EXTENDED : prints full information, including the statement's query string, if available,
      #              and the statement's bound values, if available. For batch statements, this verbosity
      #              level also prints all information available about the batch's inner statements.
      # Defaults to EXTENDED.
      verbosity = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      # Defaults to 500.
      # Setting this value to -1 disables this feature (not recommended).
      maxQueryStringLength = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit,
      # the exceeding values will not be printed.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      maxBoundValues = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      maxBoundValueLength = 50

      # The maximum number of inner statements to print for a batch statement.
      # Only applicable for batch statements, ignored otherwise.
      # If the batch statement has more children than
      # this value, the exceeding child statements will not be printed.
      # Defaults to 10.
      # Setting this value to -1 disables this feature (not recommended).
      maxInnerStatements = 10

    }
  }

  # Conversion-specific settings.
  # These settings apply for both write and read workflows;
  # when writing, these settings determine how record fields
  # emitted by connectors are parsed;
  # when reading, these settings determine how row cells
  # emitted by DSE are formatted.
  codec {

    # The locale to use for locale-sensitive conversions.
    # Defaults to en_US (US English).
    locale = en_US

    # The time zone to use for temporal conversions that do not convey any explicit time zone information.
    # Defaults to UTC.
    timeZone = UTC

    # All possible combinations for true:false (case insensitive);
    # in write workflows, all combinations are taken into account;
    # in read workflows, the first combination specified here will be
    # used to format booleans; all other combinations will be ignored.
    booleanWords = ["1:0", "Y:N", "T:F", "YES:NO", "TRUE:FALSE"]

    # The DecimalFormat pattern to use for String-to-Number conversions.
    # See java.text.DecimalFormat for details about the pattern syntax to use.
    # Defaults to "#,###.##".
    number = "#,###.##"

    # The temporal pattern to use for String-to-CQL timestamp conversions.
    # This can be either:
    # - A date-time pattern;
    # - A pre-defined formatter such as ISO_DATE_TIME;
    # - Or the special value CQL_DATE_TIME, which is a special parser that accepts all valid CQL literal formats for
    #   the timestamp type.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to CQL_DATE_TIME.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for String-to-CQL date conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_DATE.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_DATE.
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for String-to-CQL time conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_TIME.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_TIME.
    time = "ISO_LOCAL_TIME"

    # The delimiter for tokenized fields.
    # This setting will be used when mapping a String field to
    # CQL columns that require special tokenization (collections, tuples and UDTs),
    # to isolate elements in the string.
    # Note that this is not the same as CSV parsing.
    # This setting will be applied to record fields produced by any connector,
    # as long as the the target CQL type requires tokenization.
    # Defaults to "," (comma).
    delimiter = ","

    # The key-value separator to use to split keys and values in a tokenized field.
    # This setting will be used when mapping a String field to
    # CQL columns of type map or UDT, to isolate keys/fields from values, after key-value pairs
    # have been tokenized using the "delimiter" setting.
    # This setting will be applied to record fields produced by any connector,
    # as long as the the target CQL type requires key-value tokenization.
    # Defaults to ":" (colon).
    keyValueSeparator = ":"
  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter.
    # The console reporter will print useful metrics
    # about the ongoing operation at this rate.
    # Durations lesser than one second will be rounded up
    # to 1 second.
    # Defaults to 5 seconds.
    reportInterval = 5 seconds

    # The time unit to use when printing throughput rates.
    # Valid values: all TimeUnit enum constants.
    # Defaults to SECONDS.
    rateUnit = SECONDS

    # The time unit to use when printing latency durations.
    # Valid values: all TimeUnit enum constants.
    # Defaults to MILLISECONDS.
    durationUnit = MILLISECONDS

    # The expected total number of writes.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expectedWrites = -1

    # The expected total number of reads.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expectedReads = -1

    # Whether or not to enable JXM reporting.
    # Defaults to true.
    jmx = true

  }

  # Schema-specific settings.
  schema {

    # The keyspace to connect to. Optional.
    # If not specified, then the "statement" setting must be specified.
    keyspace: null

    # The destination table. Optional.
    # If not specified, then the "statement" setting must be specified.
    table: null

    # The  statement to use. Optional.
    # If not specified, then "keyspace" and "setting" must
    # be specified, and the loader will infer the appropriate statement
    # based on the table's metadata, using all availble columns.
    # In write worflows, the statement can be any INSERT or UPDATE
    # statement, but MUST use named bound variables exclusively;
    # positional bound variables will not work.
    # Bound variable names usually match those of the columns in the destination table,
    # but this is not a strict requirement; it is however required that
    # their names match those specified in the mapping.
    # See "mapping" setting for more information.
    # In read worflows, the statement can be any regular SELECT
    # statement; it can optionally contain a token range restriction clause of
    # the form: "token(...) > :start and token(...) <= :end."
    # If such a clause is present, the engine will generate as many statements
    # as there are token ranges in the cluster, thus allowing to parallelize
    # reads while at the same time targeting coordinators that are also replicas.
    # The column names in the SELECT clause will be used to match column names specified
    # in the mapping. See "mapping" setting for more information.
    statement: null

    # Case-sensitive strings that should be mapped to null.
    # In write workflows, when a record field value matches one of these words,
    # then that value is replaced with a null and forwarded to DSE as such.
    # This setting only applies for fields of type String.
    # Note that this setting is applied before the
    # "nullToUnset" setting, hence any nulls produced by a null word
    # can still be left unset if required.
    # In read workflows, only the first string specified here will be used:
    # when a row cell contains a null value, then it will be replaced with that
    # word and forwarded as such to the connector.
    # Defaults to [""] â€“ which means that in write workflows,
    # empty strings are converted to nulls, and in read workflows,
    # nulls are converted to empty strings.
    nullWords: [""]

    # Whether or not to map "null" input values to "unset" in the database, meaning don't
    # modify a potentially pre-existing value of this field for this row.
    # This is only valid for write scenarios; it is ignored otherwise.
    # Note that this setting is applied after the "nullWords" setting,
    # and may intercept nulls produced by such words.
    # Note that setting this to false leads to tombstones being created in the database
    # to represent null.
    # Defaults to true.
    nullToUnset: true

    # The field-to-column mapping to use.
    # Applies to both write and read workflows.
    # If not specified, the loader will apply a strict one-to-one mapping
    # between the source fields and the database table.
    # If that is not what you want, then you must supply an explicit mapping.
    # Mappings should be specified as a HOCON map of the following form:
    # - Indexed data sources:
    #   { 0 = col1, 1 = col2, 2 = col3 }
    #   where 0, 1, 2, etc. are the zero-based indices of fields in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # - Mapped data sources:
    #   { fieldA = col1, fieldB = col2, fieldC = col3 }
    #   where fieldA, fieldB, fieldC, etc. are field names in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # The exact type of mapping to use depends on the connector being used.
    # Some connectors can only produce indexed records; others can only produce
    # mapped ones, while others are capable of producing both indexed and mapped
    # records at the same time. Refer to the connector's documentation
    # to know which kinds of mapping it supports.
    mapping {

      # unspecified by default

    }

    # Record metadata.
    # Applies within both write and read workflows to
    # records being respectively read from or written to the connector.
    # This information is optional, and rarely needed.
    # If not specified:
    # - If the connector is capable of reporting the record metadata accurately
    #   (for example, some database connectors might be able to inspect
    #   the target table's metadata), then this section is only required
    #   if you want to override some field types as reported by the connector.
    # - If the connector is not capable of reporting the record metadata accurately
    #   (for example, file connectors usually cannot report such information),
    #   then all fields are assumed to be of type String. If this is not correct,
    #   then you need to provide the correct type information here.
    # Field metadata should be specified as a HOCON map of the following form:
    # - Indexed data sources:
    #   { 0 = java.lang.String, 1 = java.lang.Double }
    #   where 0, 1, 2, etc. are the zero-based indices of fields in the source data;
    #   and the values are the expected types for each field.
    # - Mapped data sources:
    #   { fieldA = java.lang.String, fieldB = java.lang.Double }
    #   where fieldA, fieldB, fieldC, etc. are field names in the source data;
    #   and the values are the expected types for each field.
    recordMetadata {

      # unspecified by default

    }

  }

  # Connector-specific settings.
  # This section contains settings for the connector to use;
  # it also contains sub-sections, one for each available connector,
  # containing default values for every one of them.
  # What settings are available depend on which connector is being used,
  # and default values shown here will depend on which connectors are available;
  # please consult each connector's documentation for more information.
  connector {

    # The name of the connector to use.
    # Names can be specified either by a fully-qualified class name
    # of a class implementing com.datastax.loader.connectors.api.Connector;
    # or by a short name, in which case it is assumed that the name
    # should match that of a known connector implementation.
    # The matching is of type "starts-with" and is case insensitive;
    # e.g."csv" will match CSVConnector and "json" will match JsonConnector.
    # This setting has no default value and must be supplied by the user.
    name = null

  }

  # Workflow Engine-specific settings.
  engine {

    # The maximum number of threads to allocate for serializing and deserializing,
    # as well as for mapping records or results.
    # Applies to both read and write workflows.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 1C.
    maxMappingThreads = 1C

    # The maximum number of range reads that can be issued
    # concurrently for the same table.
    # Applies only to read workflows; ignored otherwise.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4.
    maxConcurrentReads = 4

  }

}
