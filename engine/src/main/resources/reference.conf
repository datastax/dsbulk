# Reference configuration for the DataStax Bulk Loader/Unloader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
#
# Note that a paragraph is written in one line, and paragraphs are separated by a blank line.
# This has the benefit of rendering well in markdown as well as plain-text help output (since
# the help text formatter wraps lines appropriately).
dsbulk {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster.
    #
    # This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some platforms).
    #
    # Note that each host entry may optionally be followed by `:port` to specify the port to connect to. When not specified, this value falls back to the *port* setting.
    hosts = "127.0.0.1"

    # The port to connect to at initial contact points.
    #
    # Note that all nodes in a cluster must accept connections on the same port number.
    port = 9042

    # Native Protocol-specific settings.
    protocol {

      # The compression algorithm to use.
      # Valid values are: `NONE`, `LZ4`, `SNAPPY`.
      compression = LZ4

    }

    # Pooling-specific settings.
    #
    # The driver maintains a connection pool to each node, according to the distance assigned to it by the load balancing policy. If the distance is `IGNORED`, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool for nodes at "local" distance.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        #
        # This must be between 1 and 32768.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool for nodes at "remote" distance.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        #
        # This must be between 1 and 32768.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver sends a dummy message on it to make sure it's still alive. If not, the connection is trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both loads and unloads.
      #
      # Valid values are: `ANY`, `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`, `EACH_QUORUM`, `ALL`.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes.
      #
      # Valid values are: `SERIAL` and `LOCAL_SERIAL`.
      #
      # Only applicable if the data is inserted using lightweight transactions, ignored otherwise.
      serialConsistency = LOCAL_SERIAL

      # The page size. This controls how many rows will be retrieved simultaneously in a single network round trip (the goal being to avoid loading too many results in memory at the same time).
      #
      # Only applicable in unload scenarios, ignored otherwise.
      fetchSize = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {

      # How long the driver waits for a request to complete. This is a global limit on the duration of a `session.execute()` call, including any internal retries the driver might do.
      readTimeout = 12 seconds

    }

    # Authentication settings.
    auth {

      # The name of the AuthProvider to use.
      #  - **None**: no authentication.
      #  - **PlainTextAuthProvider**:
      #    Uses `com.datastax.driver.core.PlainTextAuthProvider` for authentication. Supports SASL authentication using the `PLAIN` mechanism (plain text authentication).
      #  - **DsePlainTextAuthProvider**:
      #    Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `PLAIN` mechanism (plain text authentication), and also supports optional proxy authentication; should be preferred to `PlainTextAuthProvider` when connecting to secured DSE clusters.
      #  - **DseGSSAPIAuthProvider**:
      #    Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `GSSAPI` mechanism (Kerberos authentication), and also supports optional proxy authentication.
      provider = None

      # The username to use. Required.
      #
      # Providers that accept this setting:
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      username = cassandra

      # The password to use. Required.
      #
      # Providers that accept this setting:
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      password = cassandra

      # The authorization ID to use. Optional.
      #
      # An authorization ID allows the currently authenticated user to act as a different user (a.k.a. proxy authentication).
      #
      # Providers that accept this setting:
      #  - `DsePlainTextAuthProvider`
      #  - `DseGSSAPIAuthProvider`
      authorizationId = ""

      # The Kerberos principal to use. Required.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      principal = "user@DATASTAX.COM"

      # The path of the Kerberos keytab file to use for authentication. Optional.
      #
      # If left unspecified, it is assumed that authentication will be done with a ticket cache instead.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      keyTab = ""

      # The SASL protocol name to use. Required.
      #
      # This value should match the username of the Kerberos service principal used by the DSE server. This information is specified in the `dse.yaml` file by the *service_principal* option under the *kerberos_options* section, and may vary from one DSE installation to another â€“ especially if you installed DSE with an automated package installer.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      saslProtocol = "dse"

    }

    # Encryption-specific settings.
    #
    # For more information about how to configure this section, see the Java Secure Socket Extension (JSSE) Reference Guide: http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can also check the DataStax Java driver documentation on SSL: http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ssl {

      # The SSL provider to use.
      #
      # Valid values are:
      #
      # - **None**: no SSL.
      # - **JDK**: uses JDK's SSLContext
      # - **OpenSSL**: uses Netty's native support for OpenSSL
      #
      # Using OpenSSL provides better performance and generates less garbage. A disadvantage of using the OpenSSL provider is that, unlike the JDK provider, it requires a platform-specific dependency, named `netty-tcnative`, which must be added manually to the loader's classpath (typically by dropping its jar in the lib subdirectory of the DSBulk archive).
      #
      # Follow these instructions to find out how to add this dependency: http://netty.io/wiki/forked-tomcat-native.html
      provider = None

      # The cipher suites to enable.
      #
      # Example: `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
      #
      # This property is optional. If it is not present, the driver won't explicitly enable cipher suites, which according to the JDK documentation results in "a minimum quality of service".
      cipherSuites = []

      # The truststore to use to validate remote peer certificates. This section is valid for both JDK and OpenSSL providers.
      truststore {

        # The path of the truststore file.
        #
        # This setting is optional. If left unspecified, server certificates will not be validated.
        path = ""

        # The truststore password.
        password = ""

        # The algorithm to use.
        #
        # Valid values are: `PKIX`, `SunX509`.
        algorithm = SunX509

      }

      # The keystore to use for client authentication.
      #
      # This section is only valid when using JDK provider; it is ignored otherwise.
      keystore {

        # The path of the keystore file.
        #
        # This setting is optional. If left unspecified, no client authentication will be used.
        path = ""

        # The keystore password.
        password = ""

        # The algorithm to use.
        #
        # Valid values are: `SunX509`, `NewSunX509`.
        algorithm = SunX509

      }

      # OpenSSL configuration for client authentication. This section is only valid when using OpenSSL provider; it is ignored otherwise.
      openssl {

        # The path of the certificate chain file.
        #
        # This setting is optional. If left unspecified, no client authentication will be used.
        keyCertChain = ""

        # The path of the private key file.
        privateKey = ""
      }
    }

    # The simple or fully-qualified class name of the timestamp generator to use. Built-in options are:
    #
    # - **AtomicTimestampGenerator**: timestamps are guaranteed to be unique across all client threads.
    # - **ThreadLocalTimestampGenerator**: timestamps are guaranteed to be unique within each thread only.
    # - **ServerSideTimestampGenerator**: do not generate timestamps, let the server assign them.
    timestampGenerator = AtomicMonotonicTimestampGenerator

    # The simple or fully-qualified class name of the address translator to use.
    #
    # This is only needed if the nodes are not directly reachable from the driver (for example, the driver is in a different network region and needs to use a public IP, or it connects through a proxy).
    addressTranslator = IdentityTranslator

    # Settings for various driver policies.
    policy {

      # The simple or fully-qualified class name of the `RetryPolicy` implementation to use.
      #
      # If not specified, defaults to the driver's default retry policy (`com.datastax.driver.core.policies.DefaultRetryPolicy`).
      retry = ""

      # The simple or fully-qualified class name of the `LoadBalancingPolicy` implementation to use.
      #
      # If not specified, defaults to the driver's default load balancing policy (`com.datastax.driver.dse.DseLoadBalancingPolicy`).
      lbp = ""

      # The simple or fully-qualified class name of the `SpeculativeExecutionPolicy` implementation to use.
      #
      # If not specified, defaults to the driver's default speculative execution policy (`com.datastax.driver.core.policies.NoSpeculativeExecutionPolicy`).
      specexec = ""

    }

  }

  # Batch-specific settings.
  #
  # These settings control how the workflow engine groups together statements before writing them.
  #
  # Only applicable for load workflows, ignored otherwise.
  #
  # See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
  batch {

    # The grouping mode. Valid values are:
    # - **PARTITION_KEY**: Groups together statements that share the same partition key. This is the default mode, and the preferred one.
    # - **REPLICA_SET**: Groups together statements that share the same replica set. This mode might yield better results for small clusters and lower replication factors, but tends to perform equally well or worse than `PARTITION_KEY` for larger clusters or high replication factors.
    mode = PARTITION_KEY

    # The buffer size to use when batching.
    bufferSize = 10000

    # The maximum batch size.
    maxBatchSize = 100

  }

  # Executor-specific settings.
  executor {

    # The maximum number of threads to allocate for the executor. These threads are used to submit requests and process responses.
    #
    # The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    maxThreads = 4C

    # The maximum number of "in-flight" requests. In other words, sets the maximum number of concurrent uncompleted futures waiting for a response from the server. This acts as a safeguard against workflows that generate more requests than they can handle. Batch statements count for as many requests as their number of inner statements.
    #
    # Setting this option to any negative value will disable it.
    maxInFlight = 1000

    # The maximum number of concurrent requests per second. This acts as a safeguard against workflows that could overwhelm the cluster with more requests than it can handle. Batch statements count for as many requests as their number of inner statements.
    #
    # Setting this option to any negative value will disable it.
    maxPerSecond = 100000

    # Continuous-paging specific settings.
    #
    # Only applicable for unloads, and only if this feature is available in the remote cluster, ignored otherwise.
    continuousPaging {

      # The unit to use for the *executor.continuousPaging.pageSize* setting.
      #
      # Possible values are: `ROWS`, `BYTES`.
      pageUnit = ROWS

      # The size of the page. The unit to use is determined by the *executor.continuousPaging.pageUnit* setting.
      #
      pageSize = 5000

      # The maximum number of pages to retrieve.
      #
      # Setting this value to zero retrieves all pages available.
      maxPages = 0

      # The maximum number of pages per second.
      #
      # Setting this value to zero indicates no limit.
      maxPagesPerSecond = 0

    }

  }

  # Log and error management settings.
  log {

    # The directory where all log files will be stored.
    #
    # Note that this must be a path pointing to a writable directory.
    #
    # Log files for a specific run will be located in a sub-directory inside the directory specified here. Each run generates a sub-directory identified by an "operation ID', which is basically a timestamp in the format: `yyyy_MM_dd_HH_mm_ss_nnnnnnnnn`.
    #
    # Setting this value to `.` denotes the current working directory.
    directory = "."

    # The maximum number of threads to allocate to log files management.
    #
    # The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    maxThreads = "4"

    # The maximum number of errors to tolerate before aborting the entire operation.
    #
    # Setting this value to `-1` disables this feature (not recommended).
    maxErrors = 100

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired log level.
      #
      # Possible values are:
      # - **ABRIDGED**: only prints basic information in summarized form.
      # - **NORMAL**: prints basic information in summarized form, and the statement's query string, if available. For batch statements, this verbosity level also prints information about the batch's inner statements.
      # - **EXTENDED**: prints full information, including the statement's query string, if available, and the statement's bound values, if available. For batch statements, this verbosity level also prints all information available about the batch's inner statements.
      level = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxQueryStringLength = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit, the exceeding values will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValues = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValueLength = 50

      # The maximum number of inner statements to print for a batch statement.
      #
      # Only applicable for batch statements, ignored otherwise.
      #
      # If the batch statement has more children than this value, the exceeding child statements will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxInnerStatements = 10

    }
  }

  # Conversion-specific settings. These settings apply for both load and unload workflows.
  #
  # When writing, these settings determine how record fields emitted by connectors are parsed.
  #
  # When unloading, these settings determine how row cells emitted by DSE are formatted.
  codec {

    # The locale to use for locale-sensitive conversions.
    locale = en_US

    # The time zone to use for temporal conversions that do not convey any explicit time zone information.
    timeZone = UTC

    # All representations of true and false supported by dsbulk. Each representation is of the form `true-value:false-value`, case-insensitive.
    #
    # In load workflows, all representations are taken into account.
    #
    # In unload workflows, the first true-false pair will be used to format booleans; all other pairs will be ignored.
    booleanWords = ["1:0", "Y:N", "T:F", "YES:NO", "TRUE:FALSE"]

    # The `DecimalFormat` pattern to use for `String` to `Number` conversions. See `java.text.DecimalFormat` for details about the pattern syntax to use.
    number = "#,###.##"

    # The temporal pattern to use for `String` to CQL timestamp conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_DATE_TIME`
    # - The special value `CQL_DATE_TIME`, which is a special parser that accepts all valid CQL literal formats for the `timestamp` type
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for `String` to CQL date conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_LOCAL_DATE`
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for `String` to CQL time conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_LOCAL_TIME`
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    time = "ISO_LOCAL_TIME"

    # The delimiter for tokenized fields. This setting will be used when mapping a `String` field to CQL columns that require special tokenization (collections, tuples, maps, and UDTs), to isolate elements in the string.
    #
    # For maps and udt's, this setting delimits entries, while *keyValueSeparator* separates the key from the value within an entry.
    #
    # Note that this is not the same as CSV parsing. This setting will be applied to record fields produced by any connector, as long as the target CQL type requires tokenization.
    itemDelimiter = ","

    # The key-value separator to use to split keys and values in a tokenized field.
    #
    # This setting will be used when mapping a `String` field to CQL columns of type map or UDT, to isolate keys/fields from values, after key-value pairs have been tokenized using the *codec.itemDelimiter* setting.
    #
    # This setting will be applied to record fields produced by any connector, as long as the the target CQL type requires key-value tokenization.
    keyValueSeparator = ":"
  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter.
    #
    # The console reporter will print useful metrics about the ongoing operation at this rate.
    #
    # Durations lesser than one second will be rounded up to 1 second.
    reportRate = 5 seconds

    # The time unit to use when printing throughput rates.
    #
    # Valid values: all `TimeUnit` enum constants.
    rateUnit = SECONDS

    # The time unit to use when printing latency durations.
    #
    # Valid values: all `TimeUnit` enum constants.
    durationUnit = MILLISECONDS

    # The expected total number of writes.
    #
    # This information is optional; if present, the console reporter will also print the the overall achievement percentage.
    #
    # Setting this value to `-1` disables this feature.
    expectedWrites = -1

    # The expected total number of reads.
    #
    # This information is optional; if present, the console reporter will also print the the overall achievement percentage.
    #
    # Setting this value to `-1` disables this feature.
    expectedReads = -1

    # Whether or not to enable JMX reporting.
    #
    # Note that to enable *remote* JMX reporting, several properties must also be set in the JVM during launch. This is accomplished via the `DSBULK_JAVA_OPTS` environment variable.
    jmx = true

  }

  # Schema-specific settings.
  schema {

    # The keyspace to connect to. Optional.
    #
    # If not specified, then the *schema.query* setting must be specified.
    keyspace: ""

    # The destination table. Optional.
    #
    # If not specified, then the *schema.query* setting must be specified.
    table: ""

    # The query to use. Optional.
    #
    # If not specified, then *schema.keyspace* and *schema.table* must be specified, and dsbulk will infer the appropriate statement based on the table's metadata, using all available columns.
    #
    # In load worflows, the statement can be any `INSERT` or `UPDATE` statement, but **must** use named bound variables exclusively; positional bound variables will not work.
    #
    # Bound variable names usually match those of the columns in the destination table, but this is not a strict requirement; it is, however, required that their names match those specified in the mapping.
    #
    # See *schema.mapping* setting for more information.
    #
    # In unload worflows, the statement can be any regular `SELECT` statement; it can optionally contain a token range restriction clause of the form: `token(...) > :start and token(...) <= :end.`
    #
    # If such a clause is present, the engine will generate as many statements as there are token ranges in the cluster, thus allowing parallelization of reads while at the same time targeting coordinators that are also replicas.
    #
    # The column names in the SELECT clause will be used to match column names specified in the mapping. See "mapping" setting for more information.
    query: ""

    # Case-sensitive strings that should be mapped to `null`.
    #
    # In load workflows, when a record field value matches one of these words, then that value is replaced with a `null` and forwarded to DSE as such.
    #
    # This setting only applies for fields of type String.
    #
    # Note that this setting is applied before the *schema.nullToUnset* setting, hence any `null`s produced by a null-string can still be left unset if required.
    #
    # In unload workflows, only the first string specified here will be used: when a row cell contains a `null` value, then it will be replaced with that word and forwarded as such to the connector.
    #
    # The default value â€“ `[""]` â€“ means that in load workflows, empty strings are converted to `null`s, and in unload workflows, `null`s are converted to empty strings.
    nullStrings: [""]

    # Whether or not to map `null` input values to "unset" in the database, meaning don't modify a potentially pre-existing value of this field for this row.
    #
    # This is only valid for load scenarios; it is ignored otherwise.
    #
    # Note that this setting is applied after the *schema.nullStrings* setting, and may intercept `null`s produced by that setting.
    #
    # Note that setting this to false leads to tombstones being created in the database to represent `null`.
    nullToUnset: true

    # The field-to-column mapping to use.
    #
    # Applies to both load and unload workflows.
    #
    # If not specified, the loader will apply a strict one-to-one mapping between the source fields and the database table. If that is not what you want, then you must supply an explicit mapping.
    #
    # Mappings should be specified as a HOCON map of the following form:
    #
    # - Indexed data sources: `{ 0 = col1, 1 = col2, 2 = col3 }`, where 0, 1, 2, etc. are the zero-based indices of fields in the source data; and col1, col2, col3 are bound variable names in the insert statement.
    # - Mapped data sources: `{ fieldA = col1, fieldB = col2, fieldC = col3 }`, where fieldA, fieldB, fieldC, etc. are field names in the source data; and col1, col2, col3 are bound variable names in the insert statement.
    #
    # The exact type of mapping to use depends on the connector being used. Some connectors can only produce indexed records; others can only produce mapped ones, while others are capable of producing both indexed and mapped records at the same time. Refer to the connector's documentation to know which kinds of mapping it supports.
    mapping {

      # unspecified by default

    }

    # Record metadata.
    #
    # Applies within both load and unload workflows to records being respectively read from or written to the connector.
    #
    # This information is optional, and rarely needed.
    #
    # If not specified:
    #
    # - If the connector is capable of reporting the record metadata accurately (for example, some database connectors might be able to inspect the target table's metadata), then this section is only required if you want to override some field types as reported by the connector.
    # - If the connector is not capable of reporting the record metadata accurately (for example, file connectors usually cannot report such information), then all fields are assumed to be of type String. If this is not correct, then you need to provide the correct type information here.
    #
    # Field metadata should be specified as a HOCON map of the following form:
    #
    # - Indexed data sources: `{ 0 = java.lang.String, 1 = java.lang.Double }`, where 0, 1, 2, etc. are the zero-based indices of fields in the source data; and the values are the expected types for each field.
    # - Mapped data sources: `{ fieldA = java.lang.String, fieldB = java.lang.Double }`, where fieldA, fieldB, fieldC, etc. are field names in the source data; and the values are the expected types for each field.
    recordMetadata {

      # unspecified by default

    }

  }

  # Connector-specific settings. This section contains settings for the connector to use; it also contains sub-sections, one for each available connector.
  connector {

    # The name of the connector to use.
    #
    # It is used in two places:
    #
    # 1. The path to the group of settings for the connector are located under `connector.<name>`.
    # 2. The connector class name must start with `name`, case-insensitively. It is permitted for `name` to be the fully-qualified class name of the connector. That simply implies that the settings root will be at that fully-qualified location.
    #
    # Example: `csv` for class `CSVConnector`, with settings located under `connector.csv`.
    name = "csv"

  }

  # Workflow Engine-specific settings.
  engine {

    # The maximum number of threads to allocate for serializing and deserializing, as well as for mapping records or results.
    #
    # Applies to both read and write workflows.
    #
    # The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    maxMappingThreads = 1C

    # The maximum number of range reads that can be issued concurrently for the same table.
    #
    # Applies only to read workflows; ignored otherwise.
    #
    # The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    maxConcurrentReads = 4

  }

}
