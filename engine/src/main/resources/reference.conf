# Reference configuration for the DataStax Bulk Loader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
#
# Note that a paragraph is written in one line, and paragraphs are separated by a blank line.
# This has the benefit of rendering well in markdown as well as plain-text help output (since
# the help text formatter wraps lines appropriately).
dsbulk {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster.
    #
    # This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some platforms).
    #
    # Note that each host entry may optionally be followed by `:port` to specify the port to connect to. When not specified, this value falls back to the *port* setting.
    hosts = "127.0.0.1"

    # The port to connect to at initial contact points.
    #
    # Note that all nodes in a cluster must accept connections on the same port number.
    port = 9042

    # Native Protocol-specific settings.
    protocol {

      # The compression algorithm to use.
      # Valid values are: `NONE`, `LZ4`, `SNAPPY`.
      compression = NONE

    }

    # Pooling-specific settings.
    #
    # The driver maintains a connection pool to each node, according to the distance assigned to it by the load balancing policy. If the distance is `IGNORED`, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool for nodes at "local" distance.
        connections = 4

        # The maximum number of requests that can be executed concurrently on a connection.
        #
        # This must be between 1 and 32768.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool for nodes at "remote" distance.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        #
        # This must be between 1 and 32768.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver sends a dummy message on it to make sure it's still alive. If not, the connection is trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both loads and unloads.
      #
      # Valid values are: `ANY`, `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`, `EACH_QUORUM`, `ALL`.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes.
      #
      # Valid values are: `SERIAL` and `LOCAL_SERIAL`.
      #
      # Only applicable if the data is inserted using lightweight transactions, ignored otherwise.
      serialConsistency = LOCAL_SERIAL

      # The page size. This controls how many rows will be retrieved simultaneously in a single network round trip (the goal being to avoid loading too many results in memory at the same time).
      #
      # Only applicable in unload scenarios, ignored otherwise.
      fetchSize = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {

      # How long the driver waits for a request to complete. This is a global limit on the duration of a `session.execute()` call, including any internal retries the driver might do.
      readTimeout = 60 seconds

    }

    # Authentication settings.
    auth {

      # The name of the AuthProvider to use.
      #  - **None**: no authentication.
      #  - **PlainTextAuthProvider**:
      #    Uses `com.datastax.driver.core.PlainTextAuthProvider` for authentication. Supports SASL authentication using the `PLAIN` mechanism (plain text authentication).
      #  - **DsePlainTextAuthProvider**:
      #    Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `PLAIN` mechanism (plain text authentication), and also supports optional proxy authentication; should be preferred to `PlainTextAuthProvider` when connecting to secured DSE clusters.
      #  - **DseGSSAPIAuthProvider**:
      #    Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for authentication. Supports SASL authentication to DSE clusters using the `GSSAPI` mechanism (Kerberos authentication), and also supports optional proxy authentication.
      provider = None

      # The username to use. Required.
      #
      # Providers that accept this setting:
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      username = cassandra

      # The password to use. Required.
      #
      # Providers that accept this setting:
      #  - `PlainTextAuthProvider`
      #  - `DsePlainTextAuthProvider`
      password = cassandra

      # The authorization ID to use. Optional.
      #
      # An authorization ID allows the currently authenticated user to act as a different user (a.k.a. proxy authentication).
      #
      # Providers that accept this setting:
      #  - `DsePlainTextAuthProvider`
      #  - `DseGSSAPIAuthProvider`
      authorizationId = ""

      # The Kerberos principal to use. Required.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      principal = "user@DATASTAX.COM"

      # The path of the Kerberos keytab file to use for authentication. Optional.
      #
      # If left unspecified, it is assumed that authentication will be done with a ticket cache instead.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      keyTab = ""

      # The SASL protocol name to use. Required.
      #
      # This value should match the username of the Kerberos service principal used by the DSE server. This information is specified in the `dse.yaml` file by the *service_principal* option under the *kerberos_options* section, and may vary from one DSE installation to another â€“ especially if you installed DSE with an automated package installer.
      #
      # Providers that accept this setting:
      #  - `DseGSSAPIAuthProvider`
      saslProtocol = "dse"

    }

    # Encryption-specific settings.
    #
    # For more information about how to configure this section, see the Java Secure Socket Extension (JSSE) Reference Guide: http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can also check the DataStax Java driver documentation on SSL: http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ssl {

      # The SSL provider to use.
      #
      # Valid values are:
      #
      # - **None**: no SSL.
      # - **JDK**: uses JDK's SSLContext
      # - **OpenSSL**: uses Netty's native support for OpenSSL
      #
      # Using OpenSSL provides better performance and generates less garbage. A disadvantage of using the OpenSSL provider is that, unlike the JDK provider, it requires a platform-specific dependency, named `netty-tcnative`, which must be added manually to the loader's classpath (typically by dropping its jar in the lib subdirectory of the DSBulk archive).
      #
      # Follow these instructions to find out how to add this dependency: http://netty.io/wiki/forked-tomcat-native.html
      provider = None

      # The cipher suites to enable.
      #
      # Example: `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
      #
      # This property is optional. If it is not present, the driver won't explicitly enable cipher suites, which according to the JDK documentation results in "a minimum quality of service".
      cipherSuites = []

      # The truststore to use to validate remote peer certificates. This section is valid for both JDK and OpenSSL providers.
      truststore {

        # The path of the truststore file.
        #
        # This setting is optional. If left unspecified, server certificates will not be validated.
        path = ""

        # The truststore password.
        password = ""

        # The algorithm to use.
        #
        # Valid values are: `PKIX`, `SunX509`.
        algorithm = SunX509

      }

      # The keystore to use for client authentication.
      #
      # This section is only valid when using JDK provider; it is ignored otherwise.
      keystore {

        # The path of the keystore file.
        #
        # This setting is optional. If left unspecified, no client authentication will be used.
        path = ""

        # The keystore password.
        password = ""

        # The algorithm to use.
        #
        # Valid values are: `SunX509`, `NewSunX509`.
        algorithm = SunX509

      }

      # OpenSSL configuration for client authentication. This section is only valid when using OpenSSL provider; it is ignored otherwise.
      openssl {

        # The path of the certificate chain file.
        #
        # This setting is optional. If left unspecified, no client authentication will be used.
        keyCertChain = ""

        # The path of the private key file.
        privateKey = ""

      }

    }

    # The simple or fully-qualified class name of the timestamp generator to use. Built-in options are:
    #
    # - **AtomicTimestampGenerator**: timestamps are guaranteed to be unique across all client threads.
    # - **ThreadLocalTimestampGenerator**: timestamps are guaranteed to be unique within each thread only.
    # - **ServerSideTimestampGenerator**: do not generate timestamps, let the server assign them.
    timestampGenerator = AtomicMonotonicTimestampGenerator

    # The simple or fully-qualified class name of the address translator to use.
    #
    # This is only needed if the nodes are not directly reachable from the driver (for example, the driver is in a different network region and needs to use a public IP, or it connects through a proxy).
    addressTranslator = IdentityTranslator

    # Settings for various driver policies.
    policy {

      # Maximum number of retries for a timed-out request.
      maxRetries = 10

      # Load balancing policy settings
      lbp {
        # The name of the load balancing policy.
        #
        # Supported policies include: `dse`, `dcAwareRoundRobin`, `roundRobin`, `whiteList`, `tokenAware`. Available options for the policies are listed below as appropriate. For more information, refer to the driver documentation for the policy.
        #
        # If not specified, defaults to the driver's default load balancing policy, which is currently the `DseLoadBalancingPolicy` wrapping a `TokenAwarePolicy`, wrapping a `DcAwareRoundRobinPolicy`.
        #
        # NOTE: It is critical for a token-aware policy to be used in the chain in order to benefit from batching by partition key.
        name = ""

        # Settings for the DseLoadBalancingPolicy. See the driver documentation for this policy for more details.
        dse {

          # The child policy being wrapped.
          #
          # It is required to be one of the policies mentioned above.
          childPolicy = "roundRobin"

        }

        # Settings for the DCAwareRoundRobinPolicy. See the driver documentation for this policy for more details.
        dcAwareRoundRobin {
          localDc = ""

          allowRemoteDCsForLocalConsistencyLevel = false

          usedHostsPerRemoteDc = 0

        }

        # Settings for the TokenAwarePolicy. See the driver documentation for this policy for more details.
        tokenAware {

          # The child policy being wrapped.
          #
          # It is required to be one of the policies mentioned above.
          childPolicy = "roundRobin"

          shuffleReplicas = true

        }

        # Settings for the WhiteListPolicy. See the driver documentation for this policy for more details.
        whiteList {
          # The child policy being wrapped.
          #
          # It is required to be one of the policies mentioned above.
          childPolicy = "roundRobin"

          # List of hosts to white list.
          #
          # This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some platforms).
          hosts = ""

        }

      }

    }

  }

  # Log and error management settings.
  log {

    # The directory where all log files will be stored.
    #
    # Note that this must be a path pointing to a writable directory.
    #
    # Log files for a specific run will be located in a sub-directory inside the directory specified here. Each run generates a sub-directory identified by an "operation ID', which is basically a timestamp in the format: `yyyy_MM_dd_HH_mm_ss_nnnnnnnnn`.
    #
    # Setting this value to `.` denotes the current working directory.
    directory = "./logs"

    # The maximum number of errors to tolerate before aborting the entire operation.
    #
    # Setting this value to `-1` disables this feature (not recommended).
    maxErrors = 100

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired log level.
      #
      # Possible values are:
      # - **ABRIDGED**: only prints basic information in summarized form.
      # - **NORMAL**: prints basic information in summarized form, and the statement's query string, if available. For batch statements, this verbosity level also prints information about the batch's inner statements.
      # - **EXTENDED**: prints full information, including the statement's query string, if available, and the statement's bound values, if available. For batch statements, this verbosity level also prints all information available about the batch's inner statements.
      level = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxQueryStringLength = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit, the exceeding values will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValues = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxBoundValueLength = 50

      # The maximum number of inner statements to print for a batch statement.
      #
      # Only applicable for batch statements, ignored otherwise.
      #
      # If the batch statement has more children than this value, the exceeding child statements will not be printed.
      #
      # Setting this value to `-1` disables this feature (not recommended).
      maxInnerStatements = 10

    }

  }

  # Conversion-specific settings. These settings apply for both load and unload workflows.
  #
  # When writing, these settings determine how record fields emitted by connectors are parsed.
  #
  # When unloading, these settings determine how row cells emitted by DSE are formatted.
  codec {

    # The locale to use for locale-sensitive conversions.
    locale = en_US

    # The time zone to use for temporal conversions that do not convey any explicit time zone information.
    timeZone = UTC

    # All representations of true and false supported by dsbulk. Each representation is of the form `true-value:false-value`, case-insensitive.
    #
    # In load workflows, all representations are taken into account.
    #
    # In unload workflows, the first true-false pair will be used to format booleans; all other pairs will be ignored.
    booleanWords = ["1:0", "Y:N", "T:F", "YES:NO", "TRUE:FALSE"]

    # The `DecimalFormat` pattern to use for `String` to `Number` conversions. See `java.text.DecimalFormat` for details about the pattern syntax to use.
    number = "#,###.##"

    # The temporal pattern to use for `String` to CQL timestamp conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_DATE_TIME`
    # - The special value `CQL_DATE_TIME`, which is a special parser that accepts all valid CQL literal formats for the `timestamp` type
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for `String` to CQL date conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_LOCAL_DATE`
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for `String` to CQL time conversions. This can be either:
    #
    # - A date-time pattern
    # - A pre-defined formatter such as `ISO_LOCAL_TIME`
    #
    # For more information on patterns and pre-defined formatters, see https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    time = "ISO_LOCAL_TIME"

  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter.
    #
    # The console reporter will print useful metrics about the ongoing operation at this rate.
    #
    # Durations lesser than one second will be rounded up to 1 second.
    reportRate = 5 seconds

    # The time unit to use when printing throughput rates.
    #
    # Valid values: all `TimeUnit` enum constants.
    rateUnit = SECONDS

    # The time unit to use when printing latency durations.
    #
    # Valid values: all `TimeUnit` enum constants.
    durationUnit = MILLISECONDS

    # The expected total number of writes.
    #
    # This information is optional; if present, the console reporter will also print the the overall achievement percentage.
    #
    # Setting this value to `-1` disables this feature.
    expectedWrites = -1

    # The expected total number of reads.
    #
    # This information is optional; if present, the console reporter will also print the the overall achievement percentage.
    #
    # Setting this value to `-1` disables this feature.
    expectedReads = -1

    # Whether or not to enable JMX reporting.
    #
    # Note that to enable *remote* JMX reporting, several properties must also be set in the JVM during launch. This is accomplished via the `DSBULK_JAVA_OPTS` environment variable.
    jmx = true

  }

  # Schema-specific settings.
  schema {

    # The keyspace to connect to. Optional.
    #
    # If not specified, then the *schema.query* setting must be specified.
    keyspace: ""

    # The destination table. Optional.
    #
    # If not specified, then the *schema.query* setting must be specified.
    table: ""

    # The query to use. Optional.
    #
    # If not specified, then *schema.keyspace* and *schema.table* must be specified, and dsbulk will infer the appropriate statement based on the table's metadata, using all available columns.
    #
    # In load worflows, the statement can be any `INSERT` or `UPDATE` statement, but **must** use named bound variables exclusively; positional bound variables will not work.
    #
    # Bound variable names usually match those of the columns in the destination table, but this is not a strict requirement; it is, however, required that their names match those specified in the mapping.
    #
    # See *schema.mapping* setting for more information.
    #
    # In unload worflows, the statement can be any regular `SELECT` statement; it can optionally contain a token range restriction clause of the form: `token(...) > :start and token(...) <= :end.`
    #
    # If such a clause is present, the engine will generate as many statements as there are token ranges in the cluster, thus allowing parallelization of reads while at the same time targeting coordinators that are also replicas.
    #
    # The column names in the SELECT clause will be used to match column names specified in the mapping. See "mapping" setting for more information.
    query: ""

    # Case-sensitive strings (in the form of a comma-delimited list) that should be mapped to `null`.
    #
    # In load workflows, when a record field value matches one of these words, then that value is replaced with a `null` and forwarded to DSE as such.
    #
    # This setting only applies for fields of type String.
    #
    # Note that this setting is applied before the *schema.nullToUnset* setting, hence any `null`s produced by a null-string can still be left unset if required.
    #
    # In unload workflows, only the first string specified here will be used: when a row cell contains a `null` value, then it will be replaced with that word and forwarded as such to the connector.
    #
    # By default, empty strings are converted to `null`s in load workflows, and conversely `null`s are converted to empty strings in unload workflows.
    nullStrings: ""

    # Whether or not to map `null` input values to "unset" in the database, meaning don't modify a potentially pre-existing value of this field for this row.
    #
    # This is only valid for load scenarios; it is ignored otherwise.
    #
    # Note that this setting is applied after the *schema.nullStrings* setting, and may intercept `null`s produced by that setting.
    #
    # Note that setting this to false leads to tombstones being created in the database to represent `null`.
    nullToUnset: true

    # The field-to-column mapping to use.
    #
    # Applies to both load and unload workflows.
    #
    # If not specified, the loader will apply a strict one-to-one mapping between the source fields and the database table. If that is not what you want, then you must supply an explicit mapping.
    #
    # Mappings should be specified as a map of the following form:
    #
    # - Indexed data sources: `0 = col1, 1 = col2, 2 = col3`, where `0`, `1`, `2`, etc. are the zero-based indices of fields in the source data; and `col1`, `col2`, `col3` are bound variable names in the insert statement.
    #     - A shortcut to map the first `n` fields is to simply specify the destination columns: `col1, col2, col3`.
    # - Mapped data sources: `fieldA = col1, fieldB = col2, fieldC = col3`, where `fieldA`, `fieldB`, `fieldC`, etc. are field names in the source data; and `col1`, `col2`, `col3` are bound variable names in the insert statement.
    #
    # To specify that a field should be used for the query timestamp or ttl, use the specially named fake columns `__query_ttl` and `__query_timestamp`: `fieldA = __query_ttl`. Note that unlike `schema.query_timestamp`, this mapping only supports the numeric format of timestamp.
    #
    # In addition, for mapped data sources, it is also possible to specify that the mapping be partly auto-generated and partly explicitly specified. For example, if a source row has fields `c1`, `c2`, `c3`, and `c5`, and the table has columns `c1`, `c2`, `c3`, `c4`, one can map all like-named columns and specify that `c5` in the source maps to `c4` in the table as follows: `* = *, c5 = c4`
    #
    # One can specify that all like-named fields be mapped, except for `c2`: `* = -c2`
    #
    # To skip `c2` and `c3`: `* = [-c2, -c3]`
    #
    # The exact type of mapping to use depends on the connector being used. Some connectors can only produce indexed records; others can only produce mapped ones, while others are capable of producing both indexed and mapped records at the same time. Refer to the connector's documentation to know which kinds of mapping it supports.
    mapping: ""

    # Record metadata.
    #
    # Applies within both load and unload workflows to records being respectively read from or written to the connector.
    #
    # This information is optional, and rarely needed.
    #
    # If not specified:
    #
    # - If the connector is capable of reporting the record metadata accurately (for example, some database connectors might be able to inspect the target table's metadata), then this section is only required if you want to override some field types as reported by the connector.
    # - If the connector is not capable of reporting the record metadata accurately (for example, file connectors usually cannot report such information), then all fields are assumed to be of type `String`. If this is not correct, then you need to provide the correct type information here.
    #
    # Field metadata should be specified as a HOCON map (https://github.com/typesafehub/config/blob/master/HOCON.md) of the following form:
    #
    # - Indexed data sources: `0 = java.lang.String, 1 = java.lang.Double`, where `0`, `1`, etc. are the zero-based indices of fields in the source data; and the values are the expected types for each field.
    # - Mapped data sources: `fieldA = java.lang.String, fieldB = java.lang.Double`, where `fieldA`, `fieldB`, etc. are field names in the source data; and the values are the expected types for each field.
    recordMetadata: ""

    # Time-to-live of inserted/updated cells during load.
    #
    # Only applicable for load; ignored for unload.
    #
    # A value of -1 means there is no ttl.
    queryTtl = -1

    # Timestamp of inserted/updated cells during load.
    #
    # Only applicable for load; ignored for unload.
    #
    # The following formats are supported:
    #
    # * An integer indicating number of microseconds since epoch.
    # * ISO UTC date-time, e.g. `2017-01-02T14:56:78`
    #
    # Note that the second format has seconds-precision, not microseconds.
    #
    # If not specified, inserts/updates use current time of the system running the tool.
    queryTimestamp = ""
  }

  # Connector-specific settings. This section contains settings for the connector to use; it also contains sub-sections, one for each available connector.
  connector {

    # The name of the connector to use.
    #
    # It is used in two places:
    #
    # 1. The path to the group of settings for the connector are located under `connector.<name>`.
    # 2. The connector class name must start with `name`, case-insensitively. It is permitted for `name` to be the fully-qualified class name of the connector. That simply implies that the settings root will be at that fully-qualified location.
    #
    # Example: `csv` for class `CSVConnector`, with settings located under `connector.csv`.
    name = "csv"

  }


  # Batch-specific settings.
  #
  # These settings control how the workflow engine groups together statements before writing them.
  #
  # Only applicable for load workflows, ignored otherwise.
  #
  # See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
  batch {

    # Whether to enable batching of statements or not.
    enabled = true

    # The grouping mode. Valid values are:
    # - **PARTITION_KEY**: Groups together statements that share the same partition key. This is the default mode, and the preferred one.
    # - **REPLICA_SET**: Groups together statements that share the same replica set. This mode might yield better results for small clusters and lower replication factors, but tends to perform equally well or worse than `PARTITION_KEY` for larger clusters or high replication factors.
    mode = PARTITION_KEY

    # The maximum batch size.
    #
    # The ideal batch size depends on how large is the data to be inserted: the larger the data, the smaller this value should be.
    #
    # The ideal batch size also depends on the batch mode in use. When using **PARTITION_KEY**, it is usually better to use larger batch sizes. When using **REPLICA_SET** however, batches sizes should remain small (below 10).
    maxBatchSize = 32

    # The buffer size to use for batching statements.
    #
    # The buffer will be flushed when this size is reached, or when *bufferTimeout* is elapsed, whichever happens first.
    #
    # It is usually not necessary to set this value higher than `maxBatchSize`, unless the dataset to load is unsorted, in which case a higher value might improve the average batch size.
    bufferSize = ${dsbulk.batch.maxBatchSize}

    # The maximum amount of time to wait for incoming items to batch before flushing.
    #
    # The buffer will be flushed when this duration is elapsed or when *bufferSize* is reached, whichever happens first.
    bufferTimeout = 1 seconds

  }

  # Executor-specific settings.
  executor {

    # The maximum number of "in-flight" requests. In other words, sets the maximum number of concurrent uncompleted futures waiting for a response from the server. This acts as a safeguard against workflows that generate more requests than they can handle. Batch statements count for 1 request.
    #
    # You should reduce this value when the throughput for reads and writes cannot match the throughput of mappers; this is usually a sign that the workflow engine is not well calibrated and will eventually run out of memory.
    #
    # Setting this option to any negative value will disable it.
    maxInFlight = 1024

    # The maximum number of concurrent operations per second. This acts as a safeguard against workflows that could overwhelm the cluster with more requests than it can handle. Batch statements count for as many operations as their number of inner statements.
    #
    # You should reduce this value when your latencies get too high. This is usually a sign that the remote cluster cannot keep up with the throughput, and requests will eventually time out.
    #
    # Setting this option to any negative value will disable it.
    maxPerSecond = -1

    # Continuous-paging specific settings.
    #
    # Only applicable for unloads, and only if this feature is available in the remote cluster, ignored otherwise.
    continuousPaging {

      # Whether or not continuous paging is enabled.
      #
      # If the target cluster does not support continuous paging, traditional paging will be used regardless
      # of this setting.
      enabled = true

      # The unit to use for the *executor.continuousPaging.pageSize* setting.
      #
      # Possible values are: `ROWS`, `BYTES`.
      pageUnit = ROWS

      # The size of the page. The unit to use is determined by the *executor.continuousPaging.pageUnit* setting.
      #
      pageSize = 5000

      # The maximum number of pages to retrieve.
      #
      # Setting this value to zero retrieves all pages available.
      maxPages = 0

      # The maximum number of pages per second.
      #
      # Setting this value to zero indicates no limit.
      maxPagesPerSecond = 0

    }

  }

  # Workflow Engine-specific settings.
  engine {

    # The maximum number of concurrent record and result mappings.
    #
    # Applies to both load and unload workflows.
    #
    # The special syntax `NC` can be used to specify a number of threads that is a multiple of the number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    maxConcurrentMappings = 0.25C

    # The buffer size used internally by the workflow engine.
    #
    # Usually, the higher this number the better is the throughput; if you encounter OutOfMemoryErrors however, you should probably lower this number.
    bufferSize = 4096

    # Whether or not to run in dry-run mode.
    #
    # Only applicable for loads, the tool proceeds as normal except it does not actually load any data.
    dryRun = false

    # The minimum number of parallel tasks above which the egine will apply internal optimizations attempting to "pin" one thread to one CPU core.
    #
    # When the number of tasks to execute is below this threshold, the engine will operate in a mode where more thread context switches will happen, but thus allowing to optimize CPU usage when there aren't enough parallalel tasks available to occupy each CPU core.
    #
    # When the number of tasks to execute is equal to or grater than this threshold, the engine will operate in a mode where a thread will be "pinned" to a CPU core; this usually results in less thread context switches and better performance, provided that there are enough parallel tasks available to occupy all or most CPU cores.
    #
    # The exact meaning of "parallel tasks" depends on the workflow:
    #
    # - For load workflows, it is the number of distinct resources to read, as reported by the connector in use.
    # - For unload workflows, it is the number of statements to execute.
    threadPerCoreThreshold = 4

  }

}
