# Reference configuration for the DataStax Loader.
#
# All the values declared here will be used as defaults if you don't override them through
# command line arguments.
#
# This file is in HOCON format, see https://github.com/typesafehub/config/blob/master/HOCON.md.
datastax-loader {

  # Driver-specific configuration.
  driver {

    # The contact points to use for the initial connection to the cluster.
    # This must be a list of strings with each contact point specified as "host:port". If the host is
    # a DNS name that resolves to multiple A-records, all the corresponding addresses will be used.
    # Do not use "localhost" as the host name (since it resolves to both IPv4 and IPv6 addresses on
    # some platforms).
    # Note that all nodes in a cluster must share the same port.
    # Defaults to ["127.0.0.1:9042"].
    contact-points = ["127.0.0.1:9042"]

    # Native Protocol-specific settings.
    protocol {

      # The native protocol version to use.
      # Valid values are: `V3`, `V4`, `V5`, `DSE_V1`.
      # This option is not required. If it is absent, the driver will negotiate it.
      version = null

      # The compression algorithm to use.
      # Valid values are: NONE, LZ4, SNAPPY.
      # Defaults to LZ4.
      compression = LZ4

    }

    # Pooling-specific settings.
    # The driver maintains a connection pool to each node, according to the distance assigned to it
    # by the load balancing policy. If the distance is IGNORED, no connections are maintained.
    pooling {

      # Pooling settings for nodes at LOCAL distance.
      local {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 32768

      }

      # Pooling settings for nodes at REMOTE distance.
      remote {

        # The number of connections in the pool.
        connections = 1

        # The maximum number of requests that can be executed concurrently on a connection.
        # This must be between 1 and 32768.
        requests = 1024

      }

      # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver
      # sends a dummy message on it to make sure it's still alive. If not, the connection is
      # trashed and replaced.
      heartbeat = 30 seconds

    }

    # Query-related settings.
    query {

      # The consistency level to use for both reads and writes.
      # Allowed values are:
      # ANY, LOCAL_ONE, ONE, TWO, THREE, LOCAL_QUORUM, QUORUM, EACH_QUORUM, ALL.
      consistency = LOCAL_ONE

      # The serial consistency level to use for writes.
      # The allowed values are: SERIAL and LOCAL_SERIAL.
      # Only applicable if the data is inserted using lightweight transactions,
      # ignored otherwise.
      serial-consistency = LOCAL_SERIAL

      # The page size. This controls how many rows will be retrieved simultaneously in a single
      # network round trip (the goal being to avoid loading too many results in memory at the same
      # time).
      # Only applicable in read scenarios, ignored otherwise.
      fetch-size = 5000

      # The default idempotence of statements generated by the loader.
      idempotence = true

    }

    # Socket-related settings.
    socket {

      # How long the driver waits for a request to complete. This is a global limit on the duration
      # of a session.execute() call, including any internal retries the driver might do.
      read-timeout = 12 seconds

    }

    # Authentication settings.
    # TODO DAT-24: externalize authentication-specific settings to separate reference.conf files,
    # one per concrete auth provider
    auth {

      # The AuthProvider implementation class name to use.
      # This property is optional; if it is not present, no authentication will occur.
      provider = null

      # Sample configuration for the plain-text provider:
      // username = cassandra
      // password = cassandra

    }

    # Encryption-specific settings.
    # TODO DAT-24
    ssl {

      # This property is optional; if it is not present, SSL won't be activated.
      engine-factory = null

      # Sample configuration for the default SSL factory:
      # The cipher suites to enable when creating an SSLEngine for a connection.
      # This property is optional. If it is not present, the driver won't explicitly enable cipher
      # suites on the engine, which according to the JDK documentations results in "a minimum
      # quality of service".
      // cipher-suites = [ "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA" ]
      # cipher-suites = []

    }

    # The fully-qualified class name of the timestamp generator to use.
    # Built-in options are:
    # - AtomicTimestampGenerator: timestamps are guaranteed to be unique across all client threads.
    # - ThreadLocalTimestampGenerator: timestamps that are guaranteed to be unique within each
    #   thread only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    # Defaults to com.datastax.driver.core.AtomicMonotonicTimestampGenerator.
    timestamp-generator = com.datastax.driver.core.AtomicMonotonicTimestampGenerator

    # The fully-qualified class name of the address translator to use.
    # This is only needed if the nodes are not directly reachable from the driver (for example, the
    # driver is in a different network region and needs to use a public IP, or it connects through
    # a proxy).
    # Defaults to com.datastax.driver.core.policies.IdentityTranslator.
    address-translator = com.datastax.driver.core.policies.IdentityTranslator

    # Settings for various driver policies.
    policy {

      # The fully-qualified class name of the RetryPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.DefaultRetryPolicy.
      retry = com.datastax.driver.core.policies.DefaultRetryPolicy

      # The fully-qualified class name of the LoadBalancingPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.RoundRobinPolicy.
      lbp = com.datastax.driver.core.policies.RoundRobinPolicy

      # The fully-qualified class name of the SpeculativeExecutionPolicy implementation to use.
      # Defaults to com.datastax.driver.core.policies.NoSpeculativeExecutionPolicy.
      specexec = com.datastax.driver.core.policies.NoSpeculativeExecutionPolicy

    }

  }

  # Batch-specific settings.
  # These settings control how the workflow engine
  # groups together statements before writing them.
  # Only applicable for write operations, ignored otherwise.
  # See com.datastax.loader.executor.api.batch.StatementBatcher for more information.
  batch {

    # The grouping mode.
    # Valid values are:
    # - PARTITION_KEY : Groups together statements that share the same partition key. This is the default mode, and
    #                   the preferred one.
    # - REPLICA_SET   : Groups together statements that share the same replica set. This mode might yield better
    #                   results for small clusters and lower replication factors, but tends to perform equally well
    #                   or even worse than PARTITION_KEY for larger clusters or high replication factors.
    # Defaults to PARTITION_KEY.
    mode = PARTITION_KEY

    # Whether the upstream is already sorted by grouping key or not.
    # Defaults to false.
    # This should only be set to true if the upstream is guaranteed to be sorted,
    # otherwise the batching might not function properly.
    # If in doubt, leave this settings to false.
    sorted = false

    # The buffer size to use when batching.
    # Defaults to 1000.
    buffer-size = 1000

  }

  # Executor-specific settings.
  executor {

    # The maximum number of threads to allocate for the executor.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4C.
    max-threads = 4C

    # The maximum number of "in-flight" requests. In other words, sets the maximum number of
    # concurrent uncompleted futures waiting for a response from the server. This acts as a safeguard
    # against workflows that generate more requests than they can handle.
    # Defaults to 1000.
    # Setting this option to any negative value will disable it.
    max-inflight = 1000

    # The maximum number of concurrent requests per second. This acts as a safeguard against
    # workflows that could overwhelm the cluster with more requests than it can handle.
    # Defaults to 100,000.
    # Setting this option to any negative value will disable it.
    max-per-second = 100000

    # Continuous-paging specific settings.
    # Only applicable for reads, and only if this feature is available in the remote cluster,
    # ignored otherwise.
    continuous-paging {

      # The unit to use for the page-size setting.
      # Possible values are: ROWS, BYTES.
      # Defaults to ROWS.
      page-unit = ROWS

      # The size of the page. The unit to use
      # is determined by the page-unit settings.
      # Defaults to 5000 (rows).
      #
      page-size = 5000

      # The maximum number of pages to retrieve.
      # Defaults to 0, which means retrieve all pages available.
      #
      max-pages = 0

      # The maximum number of pages per second.
      # Defaults to 0, which indicates no limit.
      max-pages-per-second = 0

    }

  }

  # Log and error management settings.
  log {

    # The output directory where all log files will be stored.
    # Log files for a specific run will be located in a sub-directory
    # inside the directory specified here. Each run generates
    # a sub-directory identified by an "operation ID', which
    # is basically a timestamp in the format: yyyy_MM_dd_HH_mm_ss_nnnnnnnnn.
    # Defaults to "file:.", which means the current directory.
    output-directory = "file:."

    # The maximum number of threads to allocate to log files management.
    # The special syntax "NC" can be used to specify a number of threads
    # that is a multiple of the number of available cores, e.g.
    # if the number of cores is 8, then 4C = 4 * 8 = 32 threads.
    # Defaults to 4.
    max-threads = 4

    # The maximum number of errors to tolerate before aborting
    # the entire operation.
    # Defaults to 100.
    # Setting this value to -1 disables this feature (not recommended).
    max-errors = 100

    # Settings controlling how statements are printed to log files.
    stmt {

      # The desired verbosity. Possible values are:
      # - ABRIDGED : only prints basic information in summarized form.
      # - NORMAL   : prints basic information in summarized form, and the statement's query string,
      #              if available. For batch statements, this verbosity level also prints information
      #              about the batch's inner statements.
      # - EXTENDED : prints full information, including the statement's query string, if available,
      #              and the statement's bound values, if available. For batch statements, this verbosity
      #              level also prints all information available about the batch's inner statements.
      # Defaults to EXTENDED.
      verbosity = EXTENDED

      # The maximum length for a query string. Query strings longer than this value will be truncated.
      # Defaults to 500.
      # Setting this value to -1 disables this feature (not recommended).
      max-query-string-length = 500

      # The maximum number of bound values to print. If the statement has more bound values than this limit,
      # the exceeding values will not be printed.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      max-bound-values = 50

      # The maximum length for a bound value. Bound values longer than this value will be truncated.
      # Defaults to 50.
      # Setting this value to -1 disables this feature (not recommended).
      max-bound-value-length = 50

      # The maximum number of inner statements to print for a batch statement.
      # Only applicable for batch statements, ignored otherwise.
      # If the batch statement has more children than
      # this value, the exceeding child statements will not be printed.
      # Defaults to 10.
      # Setting this value to -1 disables this feature (not recommended).
      max-inner-statements = 10

    }
  }

  # Conversion-specific settings
  codec {

    # The locale to use for locale-sensitive conversions.
    # Defaults to en_US (US English).
    locale = en_US

    # The time zone to use for temporal conversions that do not convey any explicit time zone information.
    # Defaults to UTC.
    time-zone = UTC

    # All possible combinations for true:false; the first combination is considered the default
    # and used when formatting.
    boolean = ["1:0", "Y:N", "y:n", "T:F", "t:f", "YES:NO", "yes:no", "Yes:No", "TRUE:FALSE", "true:false", "True:False"]

    # The DecimalFormat pattern to use for String-to-Number conversions.
    # See java.text.DecimalFormat for details about the pattern syntax to use.
    # Defaults to "#,###.##".
    number = "#,###.##"

    # The temporal pattern to use for String-to-CQL timestamp conversions.
    # This can be either:
    # - A date-time pattern;
    # - A pre-defined formatter such as ISO_DATE_TIME;
    # - Or the special value CQL_DATE_TIME, which is a special parser that accepts all valid CQL literal formats for
    #   the timestamp type.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to CQL_DATE_TIME.
    timestamp = "CQL_DATE_TIME"

    # The temporal pattern to use for String-to-CQL date conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_DATE.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_DATE.
    date = "ISO_LOCAL_DATE"

    # The temporal pattern to use for String-to-CQL time conversions.
    # This can be either:
    # - A date-time pattern;
    # - Or a pre-defined formatter such as ISO_LOCAL_TIME.
    # For more information on patterns and pre-defined formatters, see
    # https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
    # Defaults to ISO_LOCAL_TIME.
    time = "ISO_LOCAL_TIME"

  }

  # Monitoring-specific settings.
  monitoring {

    # The report interval for the console reporter.
    # The console reporter will print useful metrics
    # about the ongoing operation at this rate.
    # Durations lesser than one second will be rounded up
    # to 1 second.
    # Defaults to 5 seconds.
    report-interval = 5 seconds

    # The time unit to use when printing throughput rates.
    # Valid values: all TimeUnit enum constants.
    # Defaults to SECONDS.
    rate-unit = SECONDS

    # The time unit to use when printing latency durations.
    # Valid values: all TimeUnit enum constants.
    # Defaults to MILLISECONDS.
    duration-unit = MILLISECONDS

    # The expected total number of writes.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expected-writes = -1

    # The expected total number of reads.
    # This information is optional; if present,
    # the console reporter will also print the
    # the overall achievement percentage.
    # Defaults to -1, which disables this feature.
    expected-reads = -1

    # Whether or not to enable JXM reporting.
    # Defaults to true.
    jmx = true

  }

  # Schema-specific settings.
  schema {

    # The keyspace to connect to.
    # If not specified, then the "statement" setting below must be specified.
    keyspace: null

    # The destination table.
    # If not specified, then the "statement" setting below must be specified.
    table: null

    # The INSERT or UPDATE statement to use to load data.
    # If not specified, the loader will infer the statement
    # based on the destination table's metadata using all availble columns.
    # Note that statements MUST use named bound variables;
    # positional bound variables will not work.
    # Their names usually match those of the columns in the destination table,
    # but this is not a strict requirement; it is however required that
    # their names match those specified in the mapping. See "mapping" setting below.
    statement: null

    # Values (case-sensitive) to map to null in the database when loading data.
    null-words: []

    # Whether or not to map "null" input values to "unset" in the database, meaning don't
    # modify a potentially pre-existing value of this field for this row. "null"
    # input includes the values from the null-words setting above.
    # Note that setting this to false leads to tombstones being created in the database
    # to represent null.
    # Defaults to true.
    null-to-unset: true

    # The field-to-column mapping to use.
    # If not specified, the loader will apply a strict one-to-one mapping
    # between the source fields and the target table.
    # If that is not what you want, then you must supply an explicit mapping.
    # Mappings should be specified as a HOCON map of the following form:
    # - Indexed data sources:
    #   { 0 = col1, 1 = col2, 2 = col3 }
    #   where 0, 1, 2, etc. are the zero-based indices of fields in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # - Mapped data sources:
    #   { fieldA = col1, fieldB = col2, fieldC = col3 }
    #   where fieldA, fieldB, fieldC, etc. are field names in the source data;
    #   and col1, col2, col3 are bound variable names in the insert statement.
    # The exact type of mapping to use depends on the connector being used.
    # Some connectors can only produce indexed records; others can only produce
    # mapped ones, while others are capable of producing both indexed and mapped
    # records at the same time. Refer to the connector's documentation
    # to know which kinds of mapping it supports.
    mapping {

      # unspecified by default

    }

  }

  # Connector-specific settings.
  # This section contains settings for the connector to use;
  # it also contains sub-sections, one for each available connector,
  # containing default values for every one of them.
  # What settings are available depend on which connector is being used,
  # and default values shown here will depend on which connectors are available;
  # please consult each connector's documentation for more information.
  connector {

    # The name of the connector to use.
    # Names can be specified either by a fully-qualified class name
    # of a class implementing com.datastax.loader.connectors.api.Connector;
    # or by a short name, in which case it is assumed that the name
    # should match that of a known connector implementation.
    # The matching is of type "starts-with" and is case insensitive;
    # e.g."csv" will match CSVConnector and "json" will match JsonConnector.
    # This setting has no default value and must be supplied by the user.
    name = null

  }

}
