server_type: dse
server_install_type: tarball
server_version: 6.7.7
cql_version: 3.4.5
reuse: false
dsbulk_branch: 1.x
connectors: "csv,json"
workloads: "load,unload,count"
verbosity: 1

---

ensemble:
  server:
    node.count: 5
    provisioner:
      name: ctool
      properties:
        cloud.provider: openstack
        cloud.tenant: performance
        cloud.instance.type: ms1.small
        name: dsbulk-perf-server
        mark_for_reuse: {{reuse}}
    configuration_manager:
      - name: ctool
        properties:
          product.type: {{server_type}}
          product.install.type: {{server_install_type}}
          product.version: {{server_version}}
          enable.graph: false
          cassandra.yaml.set:
            hinted_handoff_enabled: false
            auto_snapshot: false
            batch_size_warn_threshold_in_kb: 64
            unlogged_batch_across_partitions_warn_threshold: 10
      - name: ctool_monitoring
        properties:
          components: os,jvm,cassandra
  observer:
    node.count: 1
    provisioner:
      name: ctool
      properties:
        cloud.provider: nebula
        cloud.tenant: drivers-automation
        cloud.instance.type: m3.large
        mark_for_reuse: {{reuse}}
    configuration_manager:
      - name: ctool
      - name: ctool_monitoring
        properties:
          graphite.create_server: true
          save_graphs: true
          export.enabled: true
          export.prefix: performance_regressions.oss.test.dsbulk
          export.metrics:
            - AVERAGE_CONTEXT_SWITCH(<PHASE>)
            - AVERAGE_LOAD(<PHASE>)
            - AVERAGE_DISK_IO(<PHASE>)
            - AVERAGE_MEMORY(<PHASE>)
            - AVERAGE_CPU(<PHASE>)
            - AVERAGE_GC(<PHASE>)
            - AVERAGE_NETWORK_IO(<PHASE>)
  clients:
    - name: dsbulk-client
      node.count: 1
      provisioner:
        name: ctool
        properties:
          cloud.provider: nebula
          cloud.tenant: drivers-automation
          cloud.instance.type: c4.8xlarge
          mark_for_reuse: {{reuse}}
      configuration_manager:
        - name: ctool
          properties:
            install.maven: true
            java.version: openjdk8
        - name: ctool_monitoring
          properties:
            components: os

workload:
  phases:

    - prepare-data:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 2 hours
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            sudo apt update --assume-yes
            sudo apt-get install lftp --assume-yes

            #setup data-set (random Partition Key)
            mkdir -p ${FALLOUT_SCRATCH_DIR}/mnt/data/DSEBulkLoadTest
            chmod 777 ${FALLOUT_SCRATCH_DIR}/mnt/data
            cd ${FALLOUT_SCRATCH_DIR}/mnt/data/DSEBulkLoadTest

            # download in and out form remote ftp
            echo "Transferring data from FTP server..."
            lftp -e 'mirror /dsbulk/in /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/ && exit' -u 'dse_ftp','dse$#@RULES' ftp://fileserver001.datastax.lan/
            lftp -e 'mirror /dsbulk/json /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/ && exit' -u 'dse_ftp','dse$#@RULES' ftp://fileserver001.datastax.lan/
            echo "Transfer done"

            du -sh /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/in
            du -sh /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/json

            #install maven and java
            sudo apt-get -q install maven unzip --assume-yes

            # install cqlsh
            sudo pip install --upgrade pip
            sudo pip install cqlsh

    - setup-schema:
        module: cqlsh
        properties:
          num.nodes: 1
          command: >
            CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'};
            CREATE TABLE IF NOT EXISTS test.test100b(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.test1kb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.test10kb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.test1mb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.test100(pkey BIGINT, ccol BIGINT, col0 BIGINT, col1 BIGINT, col2 BIGINT, col3 BIGINT, col4 BIGINT, col5 BIGINT, col6 BIGINT, col7 BIGINT, col8 BIGINT, col9 BIGINT, col10 BIGINT, col11 BIGINT, col12 BIGINT, col13 BIGINT, col14 BIGINT, col15 BIGINT, col16 BIGINT, col17 BIGINT, col18 BIGINT, col19 BIGINT, col20 BIGINT, col21 BIGINT, col22 BIGINT, col23 BIGINT, col24 BIGINT, col25 BIGINT, col26 BIGINT, col27 BIGINT, col28 BIGINT, col29 BIGINT, col30 BIGINT, col31 BIGINT, col32 BIGINT, col33 BIGINT, col34 BIGINT, col35 BIGINT, col36 BIGINT, col37 BIGINT, col38 BIGINT, col39 BIGINT, col40 BIGINT, col41 BIGINT, col42 BIGINT, col43 BIGINT, col44 BIGINT, col45 BIGINT, col46 BIGINT, col47 BIGINT, col48 BIGINT, col49 BIGINT, col50 BIGINT, col51 BIGINT, col52 BIGINT, col53 BIGINT, col54 BIGINT, col55 BIGINT, col56 BIGINT, col57 BIGINT, col58 BIGINT, col59 BIGINT, col60 BIGINT, col61 BIGINT, col62 BIGINT, col63 BIGINT, col64 BIGINT, col65 BIGINT, col66 BIGINT, col67 BIGINT, col68 BIGINT, col69 BIGINT, col70 BIGINT, col71 BIGINT, col72 BIGINT, col73 BIGINT, col74 BIGINT, col75 BIGINT, col76 BIGINT, col77 BIGINT, col78 BIGINT, col79 BIGINT, col80 BIGINT, col81 BIGINT, col82 BIGINT, col83 BIGINT, col84 BIGINT, col85 BIGINT, col86 BIGINT, col87 BIGINT, col88 BIGINT, col89 BIGINT, col90 BIGINT, col91 BIGINT, col92 BIGINT, col93 BIGINT, col94 BIGINT, col95 BIGINT, col96 BIGINT, col97 BIGINT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.test10(pkey BIGINT, ccol BIGINT, col0 BIGINT, col1 BIGINT, col2 BIGINT, col3 BIGINT, col4 BIGINT, col5 BIGINT, col6 BIGINT, col7 BIGINT, PRIMARY KEY ((pkey), ccol));
            CREATE TABLE IF NOT EXISTS test.transactions(user_id TEXT, date timestamp, item TEXT, price float, quantity int, total decimal, currency TEXT, payment TEXT, contact list<text>, PRIMARY KEY ((user_id), date));

    - clone-and-build-dsbulk:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            echo "Cloning DSBulk repository..."
            git clone -b {{dsbulk_branch}} git@github.com:riptano/dsbulk.git
            cd dsbulk
            echo "Building DSBulk..."
            mvn clean package -DskipTests -Prelease --quiet
            echo "Build finished successfully"
            version=$(mvn help:evaluate -Dexpression=project.version -q -DforceStdout)
            sha=$(git rev-parse --short HEAD)
            echo "DSBulk version: $version branch: {{dsbulk_branch}} SHA: $sha"
            if [[ -d dist ]]
            then
              cp dist/target/*.zip ${FALLOUT_SCRATCH_DIR}/mnt/data/
            else
              cp distribution/target/*.zip ${FALLOUT_SCRATCH_DIR}/mnt/data/
            fi
            cd ${FALLOUT_SCRATCH_DIR}/mnt/data/
            unzip -q *.zip
            rm *.zip
            mv dsbulk-* dsbulk
            mkdir -p ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs

    - disable-auto-compaction:
        module: bash
        properties:
          target.group: server
          target.ordinals: all
          export_output: false
          script: |
            nodetool -h localhost disableautocompaction test
            sleep 2m

    - load-csv-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 8 hours
          script: |
            contains() { for e in  $(echo "$1" | tr "," "\n"); do [[ "$e" == "$2" ]] && return 0; done; return 1; }
            if contains {{connectors}} csv && contains {{workloads}} load; then
              cd ${FALLOUT_SCRATCH_DIR}

              echo "CSV - Loading 100B (multiple files)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test100b;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test100b -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data100B/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --engine.executionId LOAD_CSV_100B_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 100B (multiple files, batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test100b;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test100b -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data100B/ \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_100B_TPC_BATCH_DISABLED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 100B (single file)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test100b;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test100b -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data100B_one_file/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --engine.executionId LOAD_CSV_100B_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 100B (single file, batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test100b;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test100b -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data100B_one_file/ \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_100B_PARALLEL_BATCH_DISABLED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1KB"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1KB/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --engine.executionId LOAD_CSV_1KB \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1KB (compressed)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1KB/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --driver.advanced.protocol.compression LZ4 \
                --engine.executionId LOAD_CSV_1KB_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1KB (batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1KB/ \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_1KB_BATCH_DISABLED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1KB (batch disabled, compressed)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1KB/ \
                --batch.mode DISABLED \
                --driver.advanced.protocol.compression LZ4 \
                --engine.executionId LOAD_CSV_1KB_BATCH_DISABLED_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 10KB (batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test10kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test10kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data10KB/ \
                --connector.csv.maxCharsPerColumn 11000 \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_10KB \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 10KB (batch disabled, compressed)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test10kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test10kb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data10KB/ \
                --connector.csv.maxCharsPerColumn 11000 \
                --batch.mode DISABLED \
                --driver.advanced.protocol.compression LZ4 \
                --engine.executionId LOAD_CSV_10KB_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1MB (batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1mb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1mb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1MB/ \
                --connector.csv.maxCharsPerColumn 1100000 \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_1MB \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 1MB (batch disabled, compressed)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1mb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1mb -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data1MB/ \
                --connector.csv.maxCharsPerColumn 1100000 \
                --batch.mode DISABLED \
                --driver.advanced.protocol.compression LZ4 \
                --engine.executionId LOAD_CSV_1MB_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading 10 cols"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test10;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test10 -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/data10/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --engine.executionId LOAD_CSV_10_COLS \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading transactions (multiple files)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.transactions;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t transactions -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/generated \
                -delim '|' \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact' \
                --codec.timestamp ISO_ZONED_DATE_TIME \
                --batch.mode PARTITION_KEY \
                --engine.executionId LOAD_CSV_TRANSACTIONS_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading transactions (single file)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.transactions;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t transactions -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/generated_one_file \
                -delim '|' \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact' \
                --codec.timestamp ISO_ZONED_DATE_TIME \
                --batch.mode PARTITION_KEY \
                --engine.executionId LOAD_CSV_TRANSACTIONS_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading transactions (multiple files, batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.transactions;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t transactions -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/generated \
                -delim '|' \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact' \
                --codec.timestamp ISO_ZONED_DATE_TIME \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_TRANSACTIONS_TPC_BATCH_DISABLED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Loading transactions (single file, batch disabled)"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.transactions;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t transactions -header false \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/generated_one_file \
                -delim '|' \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact' \
                --codec.timestamp ISO_ZONED_DATE_TIME \
                --batch.mode DISABLED \
                --engine.executionId LOAD_CSV_TRANSACTIONS_PARALLEL_BATCH_DISABLED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              rm -Rf mnt/data/DSEBulkLoadTest/in

            else
              echo "ignoring load-csv-performance-test step"
            fi

    - load-json-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 8 hours
          script: |
            contains() { for e in  $(echo "$1" | tr "," "\n"); do [[ "$e" == "$2" ]] && return 0; done; return 1; }
            if contains {{connectors}} json && contains {{workloads}} load; then
              cd ${FALLOUT_SCRATCH_DIR}

              echo "JSON - Loading 100B"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test100b;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test100b -c json \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/json/data100B/ \
                --batch.mode REPLICA_SET \
                --batch.maxBatchStatements 10 \
                --engine.executionId LOAD_JSON_100B \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "JSON - Loading 1KB"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1kb -c json \
              -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
              -url mnt/data/DSEBulkLoadTest/json/data1KB/  \
              --batch.mode REPLICA_SET \
              --batch.maxBatchStatements 10 \
              --engine.executionId LOAD_JSON_1KB  \
              -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "JSON - Loading 10KB"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test10kb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test10kb -c json  \
              -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
              -url mnt/data/DSEBulkLoadTest/json/data10KB/  \
              --batch.mode DISABLED  \
              --engine.executionId LOAD_JSON_10KB  \
              -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "JSON - Loading 1MB"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test1mb;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test1mb -c json  \
              -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
              -url mnt/data/DSEBulkLoadTest/json/data1MB/  \
              --batch.mode DISABLED \
              --engine.executionId LOAD_JSON_1MB  \
              -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "JSON - Loading 10 cols"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.test10;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t test10 -c json  \
              -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
              -url mnt/data/DSEBulkLoadTest/json/data10/  \
              --batch.mode REPLICA_SET  \
              --batch.maxBatchStatements 10  \
              --engine.executionId LOAD_JSON_10_COLS  \
              -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "JSON - Loading transactions"
              cqlsh ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} --cqlversion={{cql_version}} -e 'TRUNCATE test.transactions;'
              mnt/data/dsbulk/bin/dsbulk load -verbosity {{verbosity}} -k test -t transactions -c json  \
              -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
              -url mnt/data/DSEBulkLoadTest/json/transactions/generated  \
              --codec.timestamp ISO_ZONED_DATE_TIME  \
              --batch.mode PARTITION_KEY  \
              --engine.executionId LOAD_JSON_TRANSACTIONS  \
              -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              rm -Rf mnt/data/DSEBulkLoadTest/json

              else
              echo "ignoring load-json-performance-test step"
              fi

    - repair:
        module: repair
        properties:
          keyspace: test
          full: true
          max.server.tasks: 4

    - compact:
        module: bash
        properties:
          target.group: server
          target.ordinals: all
          export_output: false
          timeout: 8 hours
          script: |
            nodetool -h localhost enableautocompaction test
            nodetool -h localhost compact test

    - wait-for-compactions:
        module: wait_for_compactions

    - unload-csv-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 8 hours
          script: |
            contains() { for e in  $(echo "$1" | tr "," "\n"); do [[ "$e" == "$2" ]] && return 0; done; return 1; }
            if contains {{connectors}} csv && contains {{workloads}} unload; then
              cd ${FALLOUT_SCRATCH_DIR}

              echo "CSV - Unloading 100B (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test100b -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                --engine.executionId UNLOAD_CSV_100B_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 100B (single token range)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -query 'SELECT * FROM test.test100b WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId UNLOAD_CSV_100B_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 100B (multiple token ranges, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test100b -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/ \
                -maxConcurrentFiles 1 \
                --engine.executionId UNLOAD_CSV_100B_TPC_SINGLE_FILE \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/

              echo "CSV - Unloading 100B (single token range, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -maxConcurrentFiles 1 \
                -query 'SELECT * FROM test.test100b WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId UNLOAD_CSV_100B_PARALLEL_SINGLE_FILE \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 1KB (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test1kb -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data1KB/  \
                --engine.executionId UNLOAD_CSV_1KB_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/

              echo "CSV - Unloading 1KB (single token range)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -query 'SELECT * FROM test.test1kb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId UNLOAD_CSV_1KB_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 10KB (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test10kb -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data10KB/  \
                --engine.executionId UNLOAD_CSV_10KB_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/

              echo "CSV - Unloading 10KB (single token range)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -query 'SELECT * FROM test.test10kb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId UNLOAD_CSV_10KB_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 1MB (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test1mb -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data1MB/ \
                --driver.advanced.continuous-paging.page-size 100 \
                --engine.executionId UNLOAD_CSV_1MB_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/

              echo "CSV - Unloading 1MB (multiple token ranges, compressed)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test1mb -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data1MB/ \
                --driver.advanced.protocol.compression LZ4 \
                --driver.advanced.continuous-paging.page-size 100 \
                --engine.executionId UNLOAD_CSV_1MB_TPC_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/

              echo "CSV - Unloading 1MB (single token range)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -query 'SELECT * FROM test.test1mb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --driver.advanced.continuous-paging.page-size 100 \
                --engine.executionId UNLOAD_CSV_1MB_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 1MB (single token range, compressed)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -query 'SELECT * FROM test.test1mb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --driver.advanced.protocol.compression LZ4 \
                --driver.advanced.continuous-paging.page-size 100 \
                --engine.executionId UNLOAD_CSV_1MB_PARALLEL_COMPRESSED \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "CSV - Unloading 10 cols"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test10 -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data10/  \
                --engine.executionId UNLOAD_CSV_10_COLS \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/

              echo "CSV - Unloading transactions (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t transactions -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/out  \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact'  \
                --engine.executionId UNLOAD_CSV_TRANSACTIONS_TPC \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out

              echo "CSV - Unloading transactions (single token range)"
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/out  \
                -query 'SELECT * FROM test.transactions WHERE token(user_id) > -9223372036854775808 and token (user_id) <= -9223372036854775808'  \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact'  \
                --engine.executionId UNLOAD_CSV_TRANSACTIONS_PARALLEL \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out

              echo "CSV - Unloading transactions (multiple token ranges, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t transactions -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/out  \
                -maxConcurrentFiles 1 \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact'  \
                --engine.executionId UNLOAD_CSV_TRANSACTIONS_TPC_SINGLE_FILE  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out

              echo "CSV - Unloading transactions (single token range, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -header false  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/in/transactions/out  \
                -query 'SELECT * FROM test.transactions WHERE token(user_id) > -9223372036854775808 and token (user_id) <= -9223372036854775808'  \
                -maxConcurrentFiles 1 \
                -m 'user_id,date,item,price,quantity,total,currency,payment,contact'  \
                --engine.executionId UNLOAD_CSV_TRANSACTIONS_PARALLEL_SINGLE_FILE \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/out

            else
              echo "ignoring unload-csv-performance-test step"
            fi

    - unload-json-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 8 hours
          script: |
            contains() { for e in  $(echo "$1" | tr "," "\n"); do [[ "$e" == "$2" ]] && return 0; done; return 1; }
            if contains {{connectors}} json && contains {{workloads}} unload; then
              cd ${FALLOUT_SCRATCH_DIR}

              echo "JSON - Unloading 100B (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test100b -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_100B
              echo "Exit status" $?

              echo "JSON - Unloading 100B (multiple token ranges, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test100b -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data100B/  \
                -maxConcurrentFiles 1 \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_100B_SINGLE_FILE
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/

              echo "JSON - Unloading 1KB"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test1kb -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data1KB/  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_1KB
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/

              echo "JSON - Unloading 10KB"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test10kb -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data10KB/  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_10KB
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/

              echo "JSON - Unloading 1MB"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test1mb -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data1MB/ \
                --driver.advanced.continuous-paging.page-size 100 \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_1MB
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/

              echo "JSON - Unloading 10 cols"
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t test10 -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/out/data10/  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_10_COLS
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/

              echo "JSON - Unloading transactions (multiple token ranges)"
              rm -Rf mnt/data/DSEBulkLoadTest/json/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t transactions -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/json/transactions/out  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_TRANSACTIONS
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/json/transactions/out

              echo "JSON - Unloading transactions (multiple token ranges, single file)"
              rm -Rf mnt/data/DSEBulkLoadTest/json/transactions/out
              mnt/data/dsbulk/bin/dsbulk unload -verbosity {{verbosity}} -k test -t transactions -c json  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -url mnt/data/DSEBulkLoadTest/json/transactions/out  \
                -maxConcurrentFiles 1 \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null  \
                --engine.executionId UNLOAD_JSON_TRANSACTIONS_SINGLE_FILE
              echo "Exit status" $?
              rm -Rf mnt/data/DSEBulkLoadTest/json/transactions/out

            else
              echo "ignoring unload-json-performance-test step"
            fi

    - count-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: true
          timeout: 4 hours
          script: |
            contains() { for e in  $(echo "$1" | tr "," "\n"); do [[ "$e" == "$2" ]] && return 0; done; return 1; }
            if contains {{workloads}} count; then

              cd ${FALLOUT_SCRATCH_DIR}

              echo "Counting 100B"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t test100b  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_100B  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 100B (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.test100b WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId COUNT_100B_CUSTOM_QUERY  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 1KB"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t test1kb  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_1KB  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 1KB (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}}  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.test1kb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                -query "SELECT pkey, ccol, data FROM test.test1kb" \
                --engine.executionId COUNT_1KB_CUSTOM_QUERY  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 10KB"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t test10kb  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_10KB  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 10KB (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.test10kb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId COUNT_10KB_CUSTOM_QUERY  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 1MB"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t test1mb  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_1MB  \
                --driver.advanced.continuous-paging.page-size 100 \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 1MB (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.test1mb WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId COUNT_1MB_CUSTOM_QUERY  \
                --driver.advanced.continuous-paging.page-size 100 \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 10 columns"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t test10  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_10  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting 10 columns (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.test10 WHERE token(pkey) > -9223372036854775808 and token (pkey) <= -9223372036854775808'  \
                --engine.executionId COUNT_10_CUSTOM_QUERY  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting transactions"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} -k test -t transactions  \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                --engine.executionId COUNT_TRANSACTIONS  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

              echo "Counting transactions (single token range)"
              mnt/data/dsbulk/bin/dsbulk count -verbosity {{verbosity}} \
                -h ${FALLOUT_SERVER_PRODUCT_CONTACT_POINT} \
                -query 'SELECT * FROM test.transactions WHERE token(user_id) > -9223372036854775808 and token (user_id) <= -9223372036854775808'  \
                --engine.executionId COUNT_TRANSACTIONS_CUSTOM_QUERY  \
                -logDir ${FALLOUT_ARTIFACT_DIR}/dsbulk-logs 2>/dev/null
              echo "Exit status" $?

            else
              echo "ignoring count-performance-test step"
            fi
  checkers:
    verify_success:
      checker: nofail