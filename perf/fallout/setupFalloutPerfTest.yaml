# branch and version of the dsbulk to benchmark
branch: "1.x"
version: "1.5.1-SNAPSHOT"
# type of connector that should be performance tested, could be json or csv.
type_of_test: "csv"

---

ensemble:
  server:
    node.count: 5
    provisioner:
      name: ctool
      properties:
        cloud.provider: openstack
        cloud.tenant: performance
        cloud.instance.type: ms1.small
        name: dseserver-dsbulk-perf
        mark_for_reuse: true
    configuration_manager:
      - name: ctool
        properties:
          java.version: "1.8_151"
          product.type: dse
          product.install.type: tarball
          product.version: 6.7-dev
          enable.graph: false
          json.topology: |
            {
               "cluster":
                {
                  "snitch":"GossipingPropertyFileSnitch",
                  "nodes":
                  {
                    "0":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra", "seed":"True"},
                    "1":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra", "seed":"True"},
                    "2":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"},
                    "3":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"},
                    "4":{"datacenter":"kc-dc", "rack":"rack1", "node_type":"Cassandra"}
                  }
                }
            }
      - name: ctool_monitoring
        properties:
          components: os,jvm, dse-db, cassandra-all
  observer:
    node.count: 1
    provisioner:
      name: ctool
      properties:
        cloud.tenant: performance
        cloud.instance.type: m1.xlarge
        mark_for_reuse: true
    configuration_manager:
      - name: CTool
        properties:
          java.version: "1.8_144"
      - name: ctool_monitoring
        properties:
          graphite.create_server: true
      # ops-center will be available on {observer_ip}:8888/opscenter/index.html
      - name: ctool_opscenter
        properties:
          opscenter.repair.restart_period: 60
          opscenter.use_graphite_if_available: true
  clients:
    - name: dsbulk-client
      node.count: 1
      provisioner:
        name: ctool
        properties:
          cloud.provider: openstack
          cloud.tenant: performance
          cloud.instance.type: c3.8xlarge
          mark_for_reuse: true
      configuration_manager:
        - name: ctool
          properties:
            install.maven: true
            java.version: openjdk8
        - name: ctool_monitoring
          properties:
            components: os

workload:
  phases:
    - prepare-data:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 2 hours
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            sudo apt-get install lftp --assume-yes
            #setup data-set (random Partition Key)
            mkdir -p ${FALLOUT_SCRATCH_DIR}/mnt/data/DSEBulkLoadTest
            chmod 777 ${FALLOUT_SCRATCH_DIR}/mnt/data
            cd ${FALLOUT_SCRATCH_DIR}/mnt/data/DSEBulkLoadTest
            # download in and out form remote ftp
            lftp -e 'mirror /dsbulk/in /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/ && exit' -u 'dse_ftp','dse$#@RULES' ftp://fileserver001.datastax.lan/
            lftp -e 'mirror /dsbulk/out /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/ && exit' -u 'dse_ftp','dse$#@RULES' ftp://fileserver001.datastax.lan/
            lftp -e 'mirror /dsbulk/json /home/automaton/fallout_scratch/mnt/data/DSEBulkLoadTest/ && exit' -u 'dse_ftp','dse$#@RULES' ftp://fileserver001.datastax.lan/


            #install maven and java
            sudo apt update --assume-yes
            sudo apt install maven --assume-yes
            sudo apt-get install unzip --assume-yes

            # install cqlsh
            sudo pip install --upgrade pip
            sudo pip install cqlsh
    - setup-dse-schema:
        module: cqlsh
        properties:
          num.nodes: 1
          command: >
                    CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'};
                    CREATE TABLE IF NOT EXISTS test.test100b(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.test1kb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.test10kb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.test1mb(pkey TEXT, ccol BIGINT, data TEXT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.test100(pkey BIGINT, ccol BIGINT, col0 BIGINT, col1 BIGINT, col2 BIGINT, col3 BIGINT, col4 BIGINT, col5 BIGINT, col6 BIGINT, col7 BIGINT, col8 BIGINT, col9 BIGINT, col10 BIGINT, col11 BIGINT, col12 BIGINT, col13 BIGINT, col14 BIGINT, col15 BIGINT, col16 BIGINT, col17 BIGINT, col18 BIGINT, col19 BIGINT, col20 BIGINT, col21 BIGINT, col22 BIGINT, col23 BIGINT, col24 BIGINT, col25 BIGINT, col26 BIGINT, col27 BIGINT, col28 BIGINT, col29 BIGINT, col30 BIGINT, col31 BIGINT, col32 BIGINT, col33 BIGINT, col34 BIGINT, col35 BIGINT, col36 BIGINT, col37 BIGINT, col38 BIGINT, col39 BIGINT, col40 BIGINT, col41 BIGINT, col42 BIGINT, col43 BIGINT, col44 BIGINT, col45 BIGINT, col46 BIGINT, col47 BIGINT, col48 BIGINT, col49 BIGINT, col50 BIGINT, col51 BIGINT, col52 BIGINT, col53 BIGINT, col54 BIGINT, col55 BIGINT, col56 BIGINT, col57 BIGINT, col58 BIGINT, col59 BIGINT, col60 BIGINT, col61 BIGINT, col62 BIGINT, col63 BIGINT, col64 BIGINT, col65 BIGINT, col66 BIGINT, col67 BIGINT, col68 BIGINT, col69 BIGINT, col70 BIGINT, col71 BIGINT, col72 BIGINT, col73 BIGINT, col74 BIGINT, col75 BIGINT, col76 BIGINT, col77 BIGINT, col78 BIGINT, col79 BIGINT, col80 BIGINT, col81 BIGINT, col82 BIGINT, col83 BIGINT, col84 BIGINT, col85 BIGINT, col86 BIGINT, col87 BIGINT, col88 BIGINT, col89 BIGINT, col90 BIGINT, col91 BIGINT, col92 BIGINT, col93 BIGINT, col94 BIGINT, col95 BIGINT, col96 BIGINT, col97 BIGINT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.test10(pkey BIGINT, ccol BIGINT, col0 BIGINT, col1 BIGINT, col2 BIGINT, col3 BIGINT, col4 BIGINT, col5 BIGINT, col6 BIGINT, col7 BIGINT, PRIMARY KEY ((pkey), ccol));
                    CREATE TABLE IF NOT EXISTS test.transactions(user_id TEXT, date timestamp, item TEXT, price float, quantity int, total decimal, currency TEXT, payment TEXT, contact list<text>, PRIMARY KEY ((user_id), date));
    - clone-and-build-dsbulk:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            git clone -b {{branch}} git@github.com:riptano/dsbulk.git
            cd dsbulk
            mvn clean package -DskipTests -P release
            cp dist/target/*.zip ${FALLOUT_SCRATCH_DIR}/mnt/data/
            cd ${FALLOUT_SCRATCH_DIR}/mnt/data/
            unzip *.zip
            rm *.zip
            mv dsbulk-* dsbulk
    - disable-auto-compation-server-0:
        module: bash
        properties:
          target.group: server
          target.ordinals: 0
          export_output: false
          script: |
            nodetool -h localhost disableautocompaction test
            sleep 2m
    - load-csv-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 8 hours
          script: |
            if [ "{{type_of_test}}" = "csv" ]
            then
              cd ${FALLOUT_SCRATCH_DIR}
              dse_ip=$(echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1)

              # 100b TPC
              mnt/data/dsbulk/bin/dsbulk load -k test -t test100b -header false --batch.mode DISABLED --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/in/data100B/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_100b_tpc
              cqlsh ${FALLOUT_SERVER_NODE0_DSE_JMXADDRESS} --cqlversion=3.4.5 -e 'TRUNCATE test.test100b;'
              # 100b parallell
              mnt/data/dsbulk/bin/dsbulk load -k test -t test100b -header false --batch.mode DISABLED --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/in/data100B_one_file/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_100b_parallel
              # 1 KB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test1kb -header false --batch.mode REPLICA_SET -url mnt/data/DSEBulkLoadTest/in/data1KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_1KB
              # 10 KB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test10kb -header false --batch.mode DISABLED --driver.basic.request.timeout '5 minutes' --connector.csv.maxCharsPerColumn 11000 -url mnt/data/DSEBulkLoadTest/in/data10KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_10KB
              # 1 MB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test1mb -header false --batch.mode DISABLED --driver.basic.request.timeout '5 minutes' --connector.csv.maxCharsPerColumn 1100000 --executor.maxInFlight 64 -url mnt/data/DSEBulkLoadTest/in/data1MB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_1MB
              # 10 columns
              mnt/data/dsbulk/bin/dsbulk load -k test -t test10 -header false --batch.mode REPLICA_SET -url mnt/data/DSEBulkLoadTest/in/data10/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_10columns

              # transactions tpc
              mnt/data/dsbulk/bin/dsbulk load -k test -t transactions -header false --batch.mode PARTITION_KEY -url mnt/data/DSEBulkLoadTest/in/transactions/generated -h ${dse_ip} -delim '|' -m '0=user_id,1=date,2=item,3=price,4=quantity,5=total,6=currency,7=payment,8=contact' --codec.timestamp ISO_ZONED_DATE_TIME &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_transactions_tpc
              cqlsh 10.200.176.200 --cqlversion=3.4.5 -e 'TRUNCATE test.transactions;'
              # transactions parallel
              mnt/data/dsbulk/bin/dsbulk load -k test -t transactions -header false --batch.mode PARTITION_KEY -url mnt/data/DSEBulkLoadTest/in/transactions/generated_one_file -h ${dse_ip} -delim '|' -m '0=user_id,1=date,2=item,3=price,4=quantity,5=total,6=currency,7=payment,8=contact' --codec.timestamp ISO_ZONED_DATE_TIME &> ${FALLOUT_ARTIFACT_DIR}/LOAD_CSV_transactions_parallel
            else
              echo "ignoring load-csv-performance-test step"
            fi
    - load-json-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 8 hours
          script: |
            if [ "{{type_of_test}}" = "json" ]
            then
              cd ${FALLOUT_SCRATCH_DIR}
              dse_ip=$(echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1)

              # 100b
              mnt/data/dsbulk/bin/dsbulk load -k test -t test100b -c json --batch.mode REPLICA_SET -url mnt/data/DSEBulkLoadTest/json/data100B/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_100b

              # 1KB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test1kb -c json --batch.mode REPLICA_SET --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/json/data1KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_1KB

              # 10KB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test10kb -c json --batch.mode DISABLED --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/json/data10KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_10KB

              # 1MB
              mnt/data/dsbulk/bin/dsbulk load -k test -t test1mb -c json --batch.mode DISABLED --executor.maxInFlight 64 --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/json/data1MB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_1MB

              # 10 columns
              mnt/data/dsbulk/bin/dsbulk load -k test -t test10 -c json --batch.mode REPLICA_SET --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/json/data10/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_10columns

              # transactions
              mnt/data/dsbulk/bin/dsbulk load -k test -t transactions -c json --batch.mode PARTITION_KEY --driver.basic.request.timeout '5 minutes' -url mnt/data/DSEBulkLoadTest/json/transactions/generated -h ${dse_ip} --codec.timestamp ISO_ZONED_DATE_TIME &> ${FALLOUT_ARTIFACT_DIR}/LOAD_JSON_transactions
            else
              echo "ignoring load-json-performance-test step"
            fi
    - repair-and-compact:
        module: bash
        properties:
          target.group: server
          target.ordinals: 0
          export_output: false
          timeout: 4 hours
          script: |
            # run repair to make COUNT and LOAD yield proper results - repair is blocking
            nodetool -h localhost repair
            nodetool -h localhost enableautocompaction test
            nodetool -h localhost compact test
            nodetool -h localhost compactionstats
    - unload-csv-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 8 hours
          script: |
            if [ "{{type_of_test}}" = "csv" ]
            then
              cd ${FALLOUT_SCRATCH_DIR}
              dse_ip=$(echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1)

              #100B TPC
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test100b -header false -url mnt/data/DSEBulkLoadTest/out/data100B/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_100b_tpc

              #100B parallel
              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -header false -url mnt/data/DSEBulkLoadTest/out/data100B/ -h ${dse_ip} -query 'SELECT * FROM test.test100b WHERE token(pkey) > -9223372036854775807 and token (pkey) <= 3074457345618258602' &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_100b_parallel

              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test1kb -header false -url mnt/data/DSEBulkLoadTest/out/data1KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_1KB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test10kb -header false -url mnt/data/DSEBulkLoadTest/out/data10KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_10KB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test1mb -header false -url mnt/data/DSEBulkLoadTest/out/data1MB/ -h ${dse_ip} --executor.continuousPaging.pageSize 500000 --executor.continuousPaging.pageUnit BYTES &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_1MB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test10 -header false -url mnt/data/DSEBulkLoadTest/out/data10/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_10columns

              #run dsbulk step (sorted data-set) - UNLOAD JSON
              #TPC
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/generated
              mnt/data/dsbulk/bin/dsbulk unload -k test -t transactions -header false -url mnt/data/DSEBulkLoadTest/in/transactions/generated -h ${dse_ip} -m '0=user_id,1=date,2=item,3=price,4=quantity,5=total,6=currency,7=payment,8=contact' &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_transactions_tpc

              #parallel
              rm -Rf mnt/data/DSEBulkLoadTest/in/transactions/generated
              mnt/data/dsbulk/bin/dsbulk unload -header false -url mnt/data/DSEBulkLoadTest/in/transactions/generated -h ${dse_ip} -m '0=user_id,1=date,2=item,3=price,4=quantity,5=total,6=currency,7=payment,8=contact'  -query 'SELECT * FROM test.transactions WHERE token(user_id) > -9223372036854775807 and token (user_id) <= 3074457345618258602' &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_CSV_transactions_parallel
            else
              echo "ignoring unload-csv-performance-test step"
            fi
    - unload-json-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 8 hours
          script: |
            if [ "{{type_of_test}}" = "json" ]
            then
              cd ${FALLOUT_SCRATCH_DIR}
              dse_ip=$(echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1)

              rm -Rf mnt/data/DSEBulkLoadTest/out/data100B/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test100b -c json -url mnt/data/DSEBulkLoadTest/out/data100B/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_100b

              rm -Rf mnt/data/DSEBulkLoadTest/out/data1KB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test1kb -c json -url mnt/data/DSEBulkLoadTest/out/data1KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_1KB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data10KB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test10kb -c json -url mnt/data/DSEBulkLoadTest/out/data10KB/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_10KB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data1MB/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test1mb -c json -url mnt/data/DSEBulkLoadTest/out/data1MB/ -h ${dse_ip} --executor.continuousPaging.pageSize 500000 --executor.continuousPaging.pageUnit BYTES &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_1MB

              rm -Rf mnt/data/DSEBulkLoadTest/out/data10/
              mnt/data/dsbulk/bin/dsbulk unload -k test -t test10 -c json -url mnt/data/DSEBulkLoadTest/out/data10/ -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_10columns

              #run dsbulk step (sorted data-set) - UNLOAD
              rm -Rf mnt/data/DSEBulkLoadTest/json/transactions/generated
              mnt/data/dsbulk/bin/dsbulk unload -k test -t transactions -c json -url mnt/data/DSEBulkLoadTest/json/transactions/generated -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/UNLOAD_JSON_transactions
            else
              echo "ignoring unload-json-performance-test step"
            fi
    - count-performance-test:
        module: bash
        properties:
          target.group: dsbulk-client
          export_output: false
          timeout: 1 hours
          script: |
            cd ${FALLOUT_SCRATCH_DIR}
            dse_ip=$(echo ${FALLOUT_SERVER_PRODUCT_CONTACT_POINTS} | cut -d "," -f 1)
            mnt/data/dsbulk/bin/dsbulk count -k test -t test100b -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_100b
            mnt/data/dsbulk/bin/dsbulk count -k test -t test1kb -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_1kb
            mnt/data/dsbulk/bin/dsbulk count -k test -t test10kb -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_10kb
            mnt/data/dsbulk/bin/dsbulk count -k test -t test1mb -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_1mb
            mnt/data/dsbulk/bin/dsbulk count -k test -t test10 -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_10

            #run dsbulk step (ordered data-set) - COUNT
            mnt/data/dsbulk/bin/dsbulk count -k test -t transactions -h ${dse_ip} &> ${FALLOUT_ARTIFACT_DIR}/COUNT_transactions
  checkers:
    verify_success:
      checker: nofail