####################################################################################################
# This is a template configuration file for the DataStax Bulk Loader (DSBulk).
#
# This file is written in HOCON format; see
# https://github.com/typesafehub/config/blob/master/HOCON.md
# for more information on its syntax.
#
# Uncomment settings as needed to configure DSBulk. When this file is named application.conf and
# placed in the /conf directory, it will be automatically picked up and used by default. To use
# other file names see the -f command-line option.
####################################################################################################

dsbulk {

    ################################################################################################
    # Connector-specific settings. This section contains settings for the connector to use; it also
    # contains sub-sections, one for each available connector.
    ################################################################################################

    # The name of the connector to use.
    # Type: string
    # Default value: "csv"
    #connector.name = "csv"

    ################################################################################################
    # CSV Connector configuration.
    ################################################################################################

    # The URL or path of the resource(s) to read from or write to.
    # 
    # Which URL protocols are available depend on which URL stream handlers have been installed, but
    # at least the **file** protocol is guaranteed to be supported for reads and writes, and the
    # **http** and **https** protocols are guaranteed to be supported for reads.
    # 
    # The file protocol can be used with all supported file systems, local or not.
    # - When reading: the URL can point to a single file, or to an existing directory; in case of a
    # directory, the *fileNamePattern* setting can be used to filter files to read, and the
    # *recursive* setting can be used to control whether or not the connector should look for files
    # in subdirectories as well.
    # - When writing: the URL will be treated as a directory; if it doesn't exist, the loader will
    # attempt to create it; CSV files will be created inside this directory, and their names can be
    # controlled with the *fileNameFormat* setting.
    # 
    # Note that if the value specified here does not have a protocol, then it is assumed to be a
    # file protocol. Relative URLs will be resolved against the current working directory. Also, for
    # convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the
    # current user's home directory.
    # 
    # In addition the value `-` indicates `stdin` when loading and `stdout` when unloading. This is
    # in line with Unix tools such as tar, which uses `-` to represent stdin/stdout when
    # reading/writing an archive.
    # 
    # Examples:
    # 
    # url = "/path/to/dir/or/file"           # without protocol
    # url = "./path/to/dir/or/file"          # without protocol, relative to working directory
    # url = "~/path/to/dir/or/file"          # without protocol, relative to the user's home
    # directory
    # url = "file:///path/to/dir/or/file"    # with file protocol
    # url = "http://acme.com/file.csv"       # with HTTP protocol
    # url = "-"                              # to read csv data from stdin (for load) or
    # url = "-"                              # write csv data to stdout (for unload)
    # 
    # For other URLs: the URL will be read or written directly; settings like *fileNamePattern*,
    # *recursive*, and *fileNameFormat* will have no effect.
    # 
    # The default value is `-` (read from `stdin` / write to `stdout`).
    # Type: string
    # Default value: "-"
    #connector.csv.url = "-"

    # The character to use as field delimiter.
    # Type: string
    # Default value: ","
    #connector.csv.delimiter = ","

    # Enable or disable whether the files to read or write begin with a header line. If enabled for
    # loading, the first non-empty line in every file will assign field names for each record
    # column, in lieu of `schema.mapping`, `fieldA = col1, fieldB = col2, fieldC = col3`. If
    # disabled for loading, records will not contain fields names, only field indexes, `0 = col1, 1
    # = col2, 2 = col3`. For unloading, if this setting is enabled, each file will begin with a
    # header line, and if disabled, each file will not contain a header line.
    # 
    # Note: This option will apply to all files loaded or unloaded.
    # Type: boolean
    # Default value: true
    #connector.csv.header = true

    # The number of records to skip from each input file before the parser can begin to execute.
    # Note that if the file contains a header line, that line is not counted as a valid record. This
    # setting is ignored when writing.
    # Type: number
    # Default value: 0
    #connector.csv.skipRecords = 0

    # The maximum number of records to read from or write to each file. When reading, all records
    # past this number will be discarded. When writing, a file will contain at most this number of
    # records; if more records remain to be written, a new file will be created using the
    # *fileNameFormat* setting. Note that when writing to anything other than a directory, this
    # setting is ignored. This setting takes into account the *header* setting: if a file begins
    # with a header line, that line is not counted as a record. This feature is disabled by default
    # (indicated by its `-1` value).
    # Type: number
    # Default value: -1
    #connector.csv.maxRecords = -1

    # The character used for quoting fields when the field delimiter is part of the field value.
    # Only one character can be specified. Note that this setting applies to all files to be read or
    # written.
    # Type: string
    # Default value: "\""
    #connector.csv.quote = "\""

    # The character that represents a line comment when found in the beginning of a line of text.
    # Only one character can be specified. Note that this setting applies to all files to be read or
    # written. This feature is disabled by default (indicated by its `null` character value).
    # Type: string
    # Default value: "\u0000"
    #connector.csv.comment = "\u0000"

    # The file encoding to use for all read or written files.
    # Type: string
    # Default value: "UTF-8"
    #connector.csv.encoding = "UTF-8"

    # The character used for escaping quotes inside an already quoted value. Only one character can
    # be specified. Note that this setting applies to all files to be read or written.
    # Type: string
    # Default value: "\\"
    #connector.csv.escape = "\\"

    # The file name format to use when writing. This setting is ignored when reading and for
    # non-file URLs. The file name must comply with the formatting rules of `String.format()`, and
    # must contain a `%d` format specifier that will be used to increment file name counters.
    # Type: string
    # Default value: "output-%0,6d.csv"
    #connector.csv.fileNameFormat = "output-%0,6d.csv"

    # The glob pattern to use when searching for files to read. The syntax to use is the glob
    # syntax, as described in `java.nio.file.FileSystem.getPathMatcher()`. This setting is ignored
    # when writing and for non-file URLs. Only applicable when the *url* setting points to a
    # directory on a known filesystem, ignored otherwise.
    # Type: string
    # Default value: "**/*.csv"
    #connector.csv.fileNamePattern = "**/*.csv"

    # The maximum number of characters that a field can contain. This setting is used to size
    # internal buffers and to avoid out-of-memory problems. If set to -1, internal buffers will be
    # resized dynamically. While convenient, this can lead to memory problems. It could also hurt
    # throughput, if some large fields require constant resizing; if this is the case, set this
    # value to a fixed positive number that is big enough to contain all field values.
    # Type: number
    # Default value: 4096
    #connector.csv.maxCharsPerColumn = 4096

    # The maximum number of files that can be written simultaneously. This setting is ignored when
    # reading and when the output URL is anything other than a directory on a filesystem. The
    # special syntax `NC` can be used to specify a number of threads that is a multiple of the
    # number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    # Type: string
    # Default value: "0.25C"
    #connector.csv.maxConcurrentFiles = "0.25C"

    # Enable or disable scanning for files in the root's subdirectories. Only applicable when *url*
    # is set to a directory on a known filesystem. Used for loading only.
    # Type: boolean
    # Default value: false
    #connector.csv.recursive = false

    ################################################################################################
    # JSON Connector configuration.
    ################################################################################################

    # The URL or path of the resource(s) to read from or write to.
    # 
    # Which URL protocols are available depend on which URL stream handlers have been installed, but
    # at least the **file** protocol is guaranteed to be supported for reads and writes, and the
    # **http** and **https** protocols are guaranteed to be supported for reads.
    # 
    # The file protocol can be used with all supported file systems, local or not.
    # - When reading: the URL can point to a single file, or to an existing directory; in case of a
    # directory, the *fileNamePattern* setting can be used to filter files to read, and the
    # *recursive* setting can be used to control whether or not the connector should look for files
    # in subdirectories as well.
    # - When writing: the URL will be treated as a directory; if it doesn't exist, the loader will
    # attempt to create it; json files will be created inside this directory, and their names can be
    # controlled with the *fileNameFormat* setting.
    # 
    # Note that if the value specified here does not have a protocol, then it is assumed to be a
    # file protocol. Relative URLs will be resolved against the current working directory. Also, for
    # convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the
    # current user's home directory.
    # 
    # In addition the value `-` indicates `stdin` when loading and `stdout` when unloading. This is
    # in line with Unix tools such as tar, which uses `-` to represent stdin/stdout when
    # reading/writing an archive.
    # 
    # Examples:
    # 
    # url = "/path/to/dir/or/file"           # without protocol
    # url = "./path/to/dir/or/file"          # without protocol, relative to working directory
    # url = "~/path/to/dir/or/file"          # without protocol, relative to the user's home
    # directory
    # url = "file:///path/to/dir/or/file"    # with file protocol
    # url = "http://acme.com/file.json"      # with HTTP protocol
    # url = "-"                              # to read json data from stdin (for load) or
    # url = "-"                              # write json data to stdout (for unload)
    # 
    # For other URLs: the URL will be read or written directly; settings like *fileNamePattern*,
    # *recursive*, and *fileNameFormat* will have no effect.
    # 
    # The default value is `-` (read from `stdin` / write to `stdout`).
    # Type: string
    # Default value: "-"
    #connector.json.url = "-"

    # The number of JSON records to skip from each input file before the parser can begin to
    # execute. This setting is ignored when writing.
    # Type: number
    # Default value: 0
    #connector.json.skipRecords = 0

    # The maximum number of records to read from or write to each file. When reading, all records
    # past this number will be discarded. When writing, a file will contain at most this number of
    # records; if more records remain to be written, a new file will be created using the
    # *fileNameFormat* setting. Note that when writing to anything other than a directory, this
    # setting is ignored. This feature is disabled by default (indicated by its `-1` value).
    # Type: number
    # Default value: -1
    #connector.json.maxRecords = -1

    # The mode for loading and unloading JSON documents. Valid values are:
    # 
    # * MULTI_DOCUMENT: Each resource may contain an arbitrary number of successive JSON documents
    # to be mapped to records. For example the format of each JSON document is a single document:
    # `{doc1}`. The root directory for the JSON documents can be specified with `url` and the
    # documents can be read recursively by setting `connector.json.recursive` to true.
    # * SINGLE_DOCUMENT: Each resource contains a root array whose elements are JSON documents to be
    # mapped to records. For example, the format of the JSON document is an array with embedded JSON
    # documents: `[ {doc1}, {doc2}, {doc3} ]`.
    # Type: string
    # Default value: "MULTI_DOCUMENT"
    #connector.json.mode = "MULTI_DOCUMENT"

    # A map of JSON deserialization features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.databind.DeserializationFeature`. Used for loading only.
    # 
    # Note: the default is to set `USE_BIG_DECIMAL_FOR_FLOATS` to `true`; this is the only way to
    # guarantee that floating point numbers will not have their precision truncated when parsed, at
    # the cost of a slightly slower parsing.
    # Type: map<string,boolean>
    # Default value: {"USE_BIG_DECIMAL_FOR_FLOATS":true}
    #connector.json.deserializationFeatures = {"USE_BIG_DECIMAL_FOR_FLOATS":true}

    # The file encoding to use for all read or written files.
    # Type: string
    # Default value: "UTF-8"
    #connector.json.encoding = "UTF-8"

    # The file name format to use when writing. This setting is ignored when reading and for
    # non-file URLs. The file name must comply with the formatting rules of `String.format()`, and
    # must contain a `%d` format specifier that will be used to increment file name counters.
    # Type: string
    # Default value: "output-%0,6d.json"
    #connector.json.fileNameFormat = "output-%0,6d.json"

    # The glob pattern to use when searching for files to read. The syntax to use is the glob
    # syntax, as described in `java.nio.file.FileSystem.getPathMatcher()`. This setting is ignored
    # when writing and for non-file URLs. Only applicable when the *url* setting points to a
    # directory on a known filesystem, ignored otherwise.
    # Type: string
    # Default value: "**/*.json"
    #connector.json.fileNamePattern = "**/*.json"

    # A map of JSON generator features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.core.JsonGenerator.Feature`. For example, a value of `{
    # ESCAPE_NON_ASCII : true, QUOTE_FIELD_NAMES : true }` will configure the generator to escape
    # all characters beyond 7-bit ASCII and quote field names when writing JSON output. Used for
    # unloading only.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.generatorFeatures = {}

    # A map of JSON mapper features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.databind.MapperFeature`. Used both for loading and unloading.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.mapperFeatures = {}

    # The maximum number of files that can be written simultaneously. This setting is ignored when
    # reading and when the output URL is anything other than a directory on a filesystem. The
    # special syntax `NC` can be used to specify a number of threads that is a multiple of the
    # number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    # Type: string
    # Default value: "0.25C"
    #connector.json.maxConcurrentFiles = "0.25C"

    # A map of JSON parser features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.core.JsonParser.Feature`. For example, a value of `{ ALLOW_COMMENTS :
    # true, ALLOW_SINGLE_QUOTES : true }` will configure the parser to allow the use of comments and
    # single-quoted strings in JSON data. Used for loading only.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.parserFeatures = {}

    # Enable or disable pretty printing. When enabled, JSON records are written with indents. Used
    # for unloading only.
    # 
    # Note: Can result in much bigger records.
    # Type: boolean
    # Default value: false
    #connector.json.prettyPrint = false

    # Enable or disable scanning for files in the root's subdirectories. Only applicable when *url*
    # is set to a directory on a known filesystem. Used for loading only.
    # Type: boolean
    # Default value: false
    #connector.json.recursive = false

    # A map of JSON serialization features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.databind.SerializationFeature`. Used for unloading only.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.serializationFeatures = {}

    ################################################################################################
    # Schema-specific settings.
    ################################################################################################

    # Keyspace used for loading or unloading data. Required option if `schema.query` is not
    # specified; otherwise, optional.
    # Type: string
    # Default value: ""
    #schema.keyspace = ""

    # Table used for loading or unloading data. Required option if `schema.query` is not specified;
    # otherwise, optional.
    # Type: string
    # Default value: ""
    #schema.table = ""

    # The field-to-column mapping to use, that applies to both loading and unloading. If not
    # specified, the loader will apply a strict one-to-one mapping between the source fields and the
    # database table. If that is not what you want, then you must supply an explicit mapping.
    # Mappings should be specified as a map of the following form:
    # 
    # - Indexed data sources: `0 = col1, 1 = col2, 2 = col3`, where `0`, `1`, `2`, are the
    # zero-based indices of fields in the source data; and `col1`, `col2`, `col3` are bound variable
    # names in the insert statement.
    # - A shortcut to map the first `n` fields is to simply specify the destination columns: `col1,
    # col2, col3`.
    # - Mapped data sources: `fieldA = col1, fieldB = col2, fieldC = col3`, where `fieldA`,
    # `fieldB`, `fieldC`, are field names in the source data; and `col1`, `col2`, `col3` are bound
    # variable names in the insert statement.
    # 
    # To specify that a field should be used as the timestamp (a.k.a. write-time) or ttl (a.k.a.
    # time-to-live) of the inserted row, use the specially named fake columns `__ttl` and
    # `__timestamp`: `fieldA = __timestamp, fieldB = __ttl`. Note that Timestamp fields are parsed
    # as regular CQL timestamp columns and must comply with either `codec.timestamp`, or
    # alternatively, with `codec.unit` + `codec.epoch`. TTL fields are parsed as integers
    # representing durations in seconds, and must comply with `codec.number`.
    # 
    # To specify that a column should be populated with the result of a function call, specify the
    # function call as the input field (e.g. `now() = c4`). Note, this is only relevant for load
    # operations. In addition, for mapped data sources, it is also possible to specify that the
    # mapping be partly auto-generated and partly explicitly specified. For example, if a source row
    # has fields `c1`, `c2`, `c3`, and `c5`, and the table has columns `c1`, `c2`, `c3`, `c4`, one
    # can map all like-named columns and specify that `c5` in the source maps to `c4` in the table
    # as follows: `* = *, c5 = c4`.
    # 
    # One can specify that all like-named fields be mapped, except for `c2`: `* = -c2`. To skip `c2`
    # and `c3`: `* = [-c2, -c3]`.
    # 
    # The exact type of mapping to use depends on the connector being used. Some connectors can only
    # produce indexed records; others can only produce mapped ones, while others are capable of
    # producing both indexed and mapped records at the same time. Refer to the connector's
    # documentation to know which kinds of mapping it supports.
    # Type: string
    # Default value: ""
    #schema.mapping = ""

    # Whether or not to accept records that contain extra fields that are not declared in the
    # mapping. Only applicable for loads, ignored otherwise.
    # 
    # For example, if a record contains 3 fields A, B, C, but the mapping only declares fields A and
    # B, then if this option is `true`, C will be silently ignored and the record will be considered
    # valid; if it is `false`, the record will be rejected.
    # Type: boolean
    # Default value: true
    #schema.allowExtraFields = true

    # Whether or not to accept records that are missing fields declared in the mapping. Only
    # applicable for loads, ignored otherwise.
    # 
    # For example, if the mapping declares 3 fields A, B, C, but a record contains only A and B,
    # then if this option is `true`, C will be silently assigned `null` and the record will be
    # considered valid; if it is `false`, the record will be rejected.
    # 
    # Note: if the missing field is mapped to a primary key column, the record will be rejected
    # regardless of this option's value, since such a record would be rejected by the database
    # anyway.
    # Type: boolean
    # Default value: false
    #schema.allowMissingFields = false

    # Comma-separated list of strings that should be mapped to `null`. For loading, when a record
    # field value exactly matches one of the specified strings, the value is replaced with `null`
    # before writing to DSE. For unloading, only the first string specified will be used to change a
    # row cell containing `null` to the specified string when written out. By default, empty strings
    # are converted to `null` while loading, and `null` is converted to an empty string while
    # unloading. This setting is applied before `schema.nullToUnset`, hence any `null` produced by a
    # null-string can still be left unset if required.
    # Type: string
    # Default value: ""
    #schema.nullStrings = ""

    # Specify whether to map `null` input values to "unset" in the database, i.e., don't modify a
    # potentially pre-existing value of this field for this row. Valid for load scenarios, otherwise
    # ignore. Note that setting to false creates tombstones to represent `null`.
    # 
    # Note that this setting is applied after the *schema.nullStrings* setting, and may intercept
    # `null`s produced by that setting.
    # Type: boolean
    # Default value: true
    #schema.nullToUnset = true

    # The query to use. If not specified, then *schema.keyspace* and *schema.table* must be
    # specified, and dsbulk will infer the appropriate statement based on the table's metadata,
    # using all available columns. If `schema.keyspace` is provided, the query need not include the
    # keyspace to qualify the table reference.
    # 
    # For loading, the statement can be any `INSERT` or `UPDATE` statement, but must use named bound
    # variables exclusively; positional bound variables will not work. Bound variable names usually
    # match those of the columns in the destination table, but this is not a strict requirement; it
    # is, however, required that their names match those specified in the mapping.
    # 
    # For unloading, the statement can be any regular `SELECT` statement; it can optionally contain
    # a token range restriction clause of the form: `token(...) > :start and token(...) <= :end`. If
    # such a clause is present, the engine will generate as many statements as there are token
    # ranges in the cluster, thus allowing parallelization of reads while at the same time targeting
    # coordinators that are also replicas. The column names in the SELECT clause will be used to
    # match column names specified in the mapping.
    # 
    # Note: The dsbulk query is parsed to discover which bound variables are present, to map the
    # variable correctly to fields.
    # 
    # See *schema.mapping* setting for more information.
    # Type: string
    # Default value: ""
    #schema.query = ""

    # The timestamp of inserted/updated cells during load; otherwise, the current time of the system
    # running the tool is used. Not applicable to unloading. The following formats are supported:
    # 
    # * A numeric timestamp that is parsed using the options `codec.unit` and `codec.epoch`.
    # * A valid date-time format specified in the options `codec.timestamp` and `codec.timeZone`.
    # 
    # Query timestamps for DSE have microsecond resolution; any sub-microsecond information
    # specified is lost. For more information, see the [CQL
    # Reference](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__timestamp-value).
    # Type: string
    # Default value: ""
    #schema.queryTimestamp = ""

    # The Time-To-Live (TTL) of inserted/updated cells during load (seconds); a value of -1 means
    # there is no TTL. Not applicable to unloading. For more information, see the [CQL
    # Reference](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__ime-value),
    # [Setting the time-to-live (TTL) for
    # value](http://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useTTL.html) and [Expiring data
    # with time-to-live](http://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html).
    # Type: number
    # Default value: -1
    #schema.queryTtl = -1

    ################################################################################################
    # Batch-specific settings.
    # 
    # These settings control how the workflow engine groups together statements before writing them.
    # 
    # Only applicable for loading.
    # 
    # See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
    ################################################################################################

    # The buffer size to use for flushing batching statements. Do not set higher than `maxBatchSize`
    # unless the loaded data is unsorted, when a higher value could improve performance. When set to
    # a negative value the buffer size is implicitly set to `maxBatchSize`.
    # Type: number
    # Default value: -1
    #batch.bufferSize = -1

    # Enable or disable statement batching.
    # Type: boolean
    # Default value: true
    #batch.enabled = true

    # The maximum batch size that depends on the size of the data inserted and the batch mode in
    # use. Larger data requires a smaller value. For batch mode, `PARTITION_KEY` requires larger
    # batch sizes, whereas `REPLICA_SET` requires smaller batch sizes, such as below 10.
    # Type: number
    # Default value: 32
    #batch.maxBatchSize = 32

    # The grouping mode. Valid values are:
    # - PARTITION_KEY: Groups together statements that share the same partition key. This is the
    # default mode, and the preferred one.
    # - REPLICA_SET: Groups together statements that share the same replica set. This mode might
    # yield better results for small clusters and lower replication factors, but tends to perform
    # equally well or worse than `PARTITION_KEY` for larger clusters or high replication factors.
    # Type: string
    # Default value: "PARTITION_KEY"
    #batch.mode = "PARTITION_KEY"

    ################################################################################################
    # Conversion-specific settings. These settings apply for both load and unload workflows.
    # 
    # When writing, these settings determine how record fields emitted by connectors are parsed.
    # 
    # When unloading, these settings determine how row cells emitted by DSE are formatted.
    ################################################################################################

    # Set how true and false representations of numbers are interpreted. The representation is of
    # the form `true_value,false_value`. The mapping is reciprocal, so that numbers are mapping to
    # Boolean and vice versa. All numbers unspecified in this setting are rejected.
    # Type: list<number>
    # Default value: [1,0]
    #codec.booleanNumbers = [1,0]

    # Specify how true and false representations can be used by dsbulk. Each representation is of
    # the form `true-value:false-value`, case-insensitive. For loading, all representations are
    # honored. For unloading, the first representation will be used and all others ignored.
    # Type: list<string>
    # Default value: ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]
    #codec.booleanWords = ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]

    # The temporal pattern to use for `String` to CQL date conversions. Valid choices:
    # 
    # - A date-time pattern such as `uuuu-MM-dd`.
    # - A pre-defined formatter such as `ISO_LOCAL_DATE`. Any public static field in
    # `java.time.format.DateTimeFormatter` can be used.
    # 
    # For more information on patterns and pre-defined formatters, see
    # [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # Type: string
    # Default value: "ISO_LOCAL_DATE"
    #codec.date = "ISO_LOCAL_DATE"

    # The instant, or "epoch", to use when converting numeric input to CQL temporal types, and when
    # converting local times to full timestamps.
    # 
    # The value must be expressed in
    # [`ISO_ZONED_DATE_TIME`](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_ZONED_DATE_TIME)
    # format.
    # 
    # Numeric inputs are converted using this instant and the unit specified under `unit`. For
    # example, if the unit is `DAYS` and the epoch is `2000-01-01T00:00:00Z`, then the number 31 is
    # converted to `2000-01-01` + 31 days, that is, `2000-02-01T00:00:00Z`.
    # 
    # When local times need to be converted to full timestamps, they are converted using the local
    # date extracted from this value. For example, if the epoch is `2000-01-01T00:00:00Z`, then the
    # local time `12:34:56` is converted to `2000-01-01T12:34:56Z`.
    # 
    # The default values for `unit` and `epoch` result in numeric input being interpreted as
    # milliseconds since Unix Epoch (1970-01-01).
    # Type: string
    # Default value: "1970-01-01T00:00:00Z"
    #codec.epoch = "1970-01-01T00:00:00Z"

    # Whether or not to use the `number` pattern to format numeric output.
    # 
    # Only applicable when unloading, and only if the connector in use requires stringification
    # (i.e., if it does not handle raw numeric data, e.g. the CSV connector); ignored otherwise.
    # 
    # When `true`, the numeric pattern defined under `number` will be used to format all numbers.
    # This allows for nicely-formatted output, but may result in rounding (see `roundingStrategy`),
    # or alteration of the original decimal's scale. When `false`, numbers will be simply
    # stringified using their `toString()` method. This is the safest choice as it never results in
    # rounding nor in scale alteration.
    # Type: boolean
    # Default value: false
    #codec.formatNumbers = false

    # The locale to use for locale-sensitive conversions.
    # Type: string
    # Default value: "en_US"
    #codec.locale = "en_US"

    # The `DecimalFormat` pattern to use for conversions between `String` and CQL numeric types.
    # 
    # See
    # [java.text.DecimalFormat](https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html)
    # for details about the pattern syntax to use.
    # 
    # The default pattern is `#,###.##`. This format recognizes almost every input, with an
    # optional, localized thousands separator, and a localized decimal separator. It also recognizes
    # an optional exponent. When used with locale `en_US`, the following inputs are all valid when
    # parsing: `1234`, `1,234`, `1234.5678`, `1,234.5678`, `1,234.5678E2`.
    # 
    # Beware that, when unloading / formatting, rounding may be necessary and could incur in
    # precision loss. You can specify the rounding strategy to apply, see `roundingStrategy` below.
    # 
    # In addition to the pattern specified here, DSBulk will always recognize Java's numeric
    # notation as valid input, both decimal and hexadecimal. Thus, regardless of the pattern being
    # used, the following examples are always correctly parsed:
    # 
    # - Decimal notation: `1.234e+56`
    # - Hexadecimal notation: `0x1.fffP+1023`
    # Type: string
    # Default value: "#,###.##"
    #codec.number = "#,###.##"

    # The overflow strategy to use when converting from `String` to CQL numeric types.
    # 
    # "Overflow" should be understood here in 3 possible ways:
    # 
    # - The value is out of the target CQL type's range. For example, trying to convert 128 to a CQL
    # `tinyint` results in overflow, as the maximum value for such type is 127.
    # - The value is decimal, but the target CQL type is integral. For example, trying to convert
    # 123.45 to a CQL `int` results in overflow.
    # - The value's precision is too large for the target CQL type. For example, trying to insert
    # 0.1234567890123456789 into a CQL `double` results in overflow as there are too many
    # significant digits to fit in a 64-bit double.
    # 
    # Valid choices:
    # 
    # - `REJECT`: overflows are considered errors and the data is rejected. This is the default
    # value.
    # - `TRUNCATE`: the data is truncated to fit in the target CQL type. The truncation algorithm is
    # similar to the narrowing primitive conversion defined in section 5.1.3 of The Java Language
    # Specification, with the following exceptions:
    # - If the value is too big or too small, it is rounded up or down to the maximum or minimum
    # value allowed, rather than truncated at bit level. For example, 128 would be rounded down to
    # 127 to fit in a byte, whereas Java would have truncated the exceeding bits and converted to
    # -127 instead.
    # - If the value is decimal, but the target CQL type is integral, it is first rounded to an
    # integral using the defined rounding strategy, then narrowed to fit into the target type.
    # 
    # Beware that `TRUNCATE` may result in precision loss and should be used with caution.
    # 
    # Note: this setting only applies for loading, when parsing numeric inputs; it does not apply
    # for unloading, since formatting never results in any of the overflow situations outlined above
    # (but it may involve rounding – see `roundingStrategy`).
    # Type: string
    # Default value: "REJECT"
    #codec.overflowStrategy = "REJECT"

    # The rounding strategy to use for conversions from CQL numeric types to `String`.
    # 
    # Only applicable when unloading, if `formatNumbers` is true and if the connector in use
    # requires stringification (i.e., if it does not handle raw numeric data, e.g. the CSV
    # connector); ignored otherwise.
    # 
    # Valid choices: any `java.math.RoundingMode` enum constant name, that is: `CEILING`, `FLOOR`,
    # `UP`, `DOWN`, `HALF_UP`, `HALF_EVEN`, `HALF_DOWN`, and `UNNECESSARY`.
    # 
    # The precision used when rounding is inferred from the numeric pattern declared under `number`.
    # For example, `#,###.##` allows up to 2 fraction digits, so the rounding precision will be 2;
    # 123.456 would be rounded to 123.46 with this pattern and strategy `UP`.
    # 
    # The default is `UNNECESSARY` because this is the only strategy that allows to export numeric
    # data without precision loss. Note however that this strategy will result in infinite precision
    # and thus will print decimal numbers with as many fraction digits as required, regardless of
    # the number of fraction digits declared in the numeric pattern in use.
    # Type: string
    # Default value: "UNNECESSARY"
    #codec.roundingStrategy = "UNNECESSARY"

    # The temporal pattern to use for `String` to CQL time conversions. Valid choices:
    # 
    # - A date-time pattern such as `HH:mm:ss`.
    # - A pre-defined formatter such as `ISO_LOCAL_TIME`. Any public static field in
    # `java.time.format.DateTimeFormatter` can be used.
    # 
    # For more information on patterns and pre-defined formatters, see
    # [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # Type: string
    # Default value: "ISO_LOCAL_TIME"
    #codec.time = "ISO_LOCAL_TIME"

    # The time zone to use for temporal conversions, when the input being parsed or formatted does
    # not convey any explicit time zone information.
    # Type: string
    # Default value: "UTC"
    #codec.timeZone = "UTC"

    # The temporal pattern to use for `String` to CQL timestamp conversions. Valid choices:
    # 
    # - A date-time pattern such as `uuuu-MM-dd HH:mm:ss`.
    # - A pre-defined formatter such as `ISO_ZONED_DATE_TIME` or `ISO_INSTANT`. Any public static
    # field in `java.time.format.DateTimeFormatter` can be used.
    # - The special value `CQL_DATE_TIME`, which is a special parser that accepts most CQL date,
    # time and timestamp literals (see below).
    # 
    # For more information on patterns and pre-defined formatters, see
    # [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns).
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # 
    # The default value is the special `CQL_DATE_TIME` value. This format is roughly equivalent to
    # the pre-defined `ISO_ZONED_DATE_TIME`, but is more permissive regarding missing fields and
    # time zone formats.
    # 
    # When parsing, this format recognizes most CQL temporal literals, e.g.:
    # 
    # - Local dates:
    # - `2012-01-01`
    # - Local times:
    # - `12:34`
    # - `12:34:56`
    # - `12:34:56.123`
    # - `12:34:56.123456`
    # - `12:34:56.123456789`
    # - Local date-times:
    # - `2012-01-01T12:34`
    # - `2012-01-01T12:34:56`
    # - `2012-01-01T12:34:56.123`
    # - `2012-01-01T12:34:56.123456`
    # - `2012-01-01T12:34:56.123456789`
    # - Zoned date-times:
    # - `2012-01-01T12:34+01:00`
    # - `2012-01-01T12:34:56+01:00`
    # - `2012-01-01T12:34:56.123+01:00`
    # - `2012-01-01T12:34:56.123456+01:00`
    # - `2012-01-01T12:34:56.123456789+01:00`
    # - `2012-01-01T12:34:56.123456789+01:00[Europe/Paris]`
    # 
    # When the input is a local date, the timestamp is resolved using the time zone specified under
    # `timeZone`, at midnight. When the input is a local time, the timestamp is resolved using the
    # time zone specified under `timeZone`, and the date is inferred from the instant specified
    # under `epoch` (by default, January 1st 1970).
    # 
    # When formatting, this format is equivalent to `ISO_ZONED_DATE_TIME`, and produces formatted
    # output of the following form: '2011-12-03T10:15:30.567+01:00[Europe/Paris]'.
    # Type: string
    # Default value: "CQL_DATE_TIME"
    #codec.timestamp = "CQL_DATE_TIME"

    # The time unit to use when converting numeric input to CQL temporal types.
    # 
    # All `TimeUnit` enum constants are valid choices.
    # 
    # Numeric inputs are converted using this unit and the instant specified under `epoch`. For
    # example, if the unit is `DAYS` and the epoch is `2000-01-01T00:00:00Z`, then the number 31 is
    # converted to `2000-01-01` + 31 days, that is, `2000-02-01T00:00:00Z`.
    # 
    # The default values for `unit` and `epoch` result in numeric input being interpreted as
    # milliseconds since Unix Epoch (1970-01-01).
    # Type: string
    # Default value: "MILLISECONDS"
    #codec.unit = "MILLISECONDS"

    # Strategy to use when generating time-based (version 1) UUIDs from timestamps. Clock sequence
    # and node ID parts of generated UUIDs are determined on a best-effort basis and are not fully
    # compliant with RFC 4122. Valid values are:
    # 
    # - RANDOM: Generates UUIDs using a random number in lieu of the local clock sequence and node
    # ID. This strategy will ensure that the generated UUIDs are unique, even if the original
    # timestamps are not guaranteed to be unique.
    # - FIXED: Preferred strategy if original timestamps are guaranteed unique, since it is faster.
    # Generates UUIDs using a fixed local clock sequence and node ID.
    # - MIN: Generates the smallest possible type 1 UUID for a given timestamp. Warning: this
    # strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    # - MAX: Generates the biggest possible type 1 UUID for a given timestamp. Warning: this
    # strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    # Type: string
    # Default value: "RANDOM"
    #codec.uuidStrategy = "RANDOM"

    ################################################################################################
    # Driver-specific configuration.
    ################################################################################################

    # The contact points to use for the initial connection to the cluster. This must be a
    # comma-separated list of hosts, each specified by a host-name or ip address. If the host is a
    # DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do
    # not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some
    # platforms).
    # Type: string
    # Default value: "127.0.0.1"
    #driver.hosts = "127.0.0.1"

    # The native transport port to connect to. This must match DSE's
    # [native_transport_port](https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html#configCassandra_yaml_r__native_transport_port)
    # configuration option.
    # 
    # Note that all nodes in a cluster must accept connections on the same port number. Mixed-port
    # cluster are not supported.
    # Type: number
    # Default value: 9042
    #driver.port = 9042

    # The simple or fully-qualified class name of the address translator to use. This is only needed
    # if the nodes are not directly reachable from the machine on which dsbulk is running (for
    # example, the dsbulk machine is in a different network region and needs to use a public IP, or
    # it connects through a proxy).
    # Type: string
    # Default value: "IdentityTranslator"
    #driver.addressTranslator = "IdentityTranslator"

    # The simple or fully-qualified class name of the timestamp generator to use. Built-in options
    # are:
    # 
    # - AtomicMonotonicTimestampGenerator: timestamps are guaranteed to be unique across all client
    # threads.
    # - ThreadLocalTimestampGenerator: timestamps are guaranteed to be unique within each thread
    # only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    # Type: string
    # Default value: "AtomicMonotonicTimestampGenerator"
    #driver.timestampGenerator = "AtomicMonotonicTimestampGenerator"

    ################################################################################################
    # Authentication settings.
    ################################################################################################

    # The username to use. Providers that accept this setting:
    # 
    # - `PlainTextAuthProvider`
    # - `DsePlainTextAuthProvider`
    # Type: string
    # Default value: "cassandra"
    #driver.auth.username = "cassandra"

    # The password to use. Providers that accept this setting:
    # 
    # - `PlainTextAuthProvider`
    # - `DsePlainTextAuthProvider`
    # Type: string
    # Default value: "cassandra"
    #driver.auth.password = "cassandra"

    # The name of the AuthProvider to use. Valid choices are:
    # 
    # - None: no authentication.
    # - PlainTextAuthProvider: Uses `com.datastax.driver.core.PlainTextAuthProvider` for
    # authentication. Supports SASL authentication using the `PLAIN` mechanism (plain text
    # authentication).
    # - DsePlainTextAuthProvider: Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for
    # authentication. Supports SASL authentication to DSE clusters using the `PLAIN` mechanism
    # (plain text authentication), and also supports optional proxy authentication; should be
    # preferred to `PlainTextAuthProvider` when connecting to secured DSE clusters.
    # - DseGSSAPIAuthProvider: Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for
    # authentication. Supports SASL authentication to DSE clusters using the `GSSAPI` mechanism
    # (Kerberos authentication), and also supports optional proxy authentication.
    # - Note: When using this provider you may have to set the `java.security.krb5.conf` system
    # property to point to your `krb5.conf` file (e.g. set the `DSBULK_JAVA_OPTS` environment
    # variable to `-Djava.security.krb5.conf=/home/user/krb5.conf`). See the [Oracle Java Kerberos
    # documentation](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html)
    # for more details.
    # Type: string
    # Default value: "None"
    #driver.auth.provider = "None"

    # An authorization ID allows the currently authenticated user to act as a different user (proxy
    # authentication). Providers that accept this setting:
    # 
    # - `DsePlainTextAuthProvider`
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.authorizationId = ""

    # The path of the Kerberos keytab file to use for authentication. If left unspecified,
    # authentication uses a ticket cache. Providers that accept this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.keyTab = ""

    # The Kerberos principal to use. For example, `user@datastax.com`. If left unspecified, the
    # principal is chosen from the first key in the ticket cache or keytab. Providers that accept
    # this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.principal = ""

    # The SASL service name to use. This value should match the username of the Kerberos service
    # principal used by the DSE server. This information is specified in the `dse.yaml` file by the
    # *service_principal* option under the *kerberos_options* section, and may vary from one DSE
    # installation to another - especially if you installed DSE with an automated package installer.
    # Providers that accept this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: "dse"
    #driver.auth.saslService = "dse"

    ################################################################################################
    # Settings for various driver policies.
    ################################################################################################

    # The name of the load balancing policy. Supported policies include: `dse`, `dcAwareRoundRobin`,
    # `roundRobin`, `whiteList`, `tokenAware`. Available options for the policies are listed below
    # as appropriate. For more information, refer to the driver documentation for the policy. If not
    # specified, defaults to the driver's default load balancing policy, which is currently the
    # `DseLoadBalancingPolicy` wrapping a `TokenAwarePolicy`, wrapping a `DcAwareRoundRobinPolicy`.
    # 
    # Note: It is critical for a token-aware policy to be used in the chain in order to benefit from
    # batching by partition key.
    # Type: string
    # Default value: ""
    #driver.policy.lbp.name = ""

    # Enable or disable whether to allow remote datacenters to count for local consistency level in
    # round robin awareness.
    # Type: boolean
    # Default value: false
    #driver.policy.lbp.dcAwareRoundRobin.allowRemoteDCsForLocalConsistencyLevel = false

    # The datacenter name (commonly dc1, dc2, etc.) local to the machine on which dsbulk is running,
    # so that requests are sent to nodes in the local datacenter whenever possible.
    # Type: string
    # Default value: ""
    #driver.policy.lbp.dcAwareRoundRobin.localDc = ""

    # The number of hosts per remote datacenter that the round robin policy should consider.
    # Type: number
    # Default value: 0
    #driver.policy.lbp.dcAwareRoundRobin.usedHostsPerRemoteDc = 0

    # The child policy that the specified `dse` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.dse.childPolicy = "roundRobin"

    # The child policy that the specified `tokenAware` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.tokenAware.childPolicy = "roundRobin"

    # Specify whether to shuffle the list of replicas that can process a request. For loading,
    # shuffling can improve performance by distributing writes across nodes.
    # Type: boolean
    # Default value: true
    #driver.policy.lbp.tokenAware.shuffleReplicas = true

    # The child policy that the specified `whiteList` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.whiteList.childPolicy = "roundRobin"

    # List of hosts to white list. This must be a comma-separated list of hosts, each specified by a
    # host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all
    # the corresponding addresses will be used. Do not use `localhost` as a host-name (since it
    # resolves to both IPv4 and IPv6 addresses on some platforms).
    # Type: string
    # Default value: ""
    #driver.policy.lbp.whiteList.hosts = ""

    # Maximum number of retries for a timed-out request.
    # Type: number
    # Default value: 10
    #driver.policy.maxRetries = 10

    ################################################################################################
    # Pooling-specific settings.
    # 
    # The driver maintains a connection pool to each node, according to the distance assigned to it
    # by the load balancing policy. If the distance is `IGNORED`, no connections are maintained.
    ################################################################################################

    # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver
    # sends a dummy message on it to make sure it's still alive. If not, the connection is trashed
    # and replaced.
    # Type: string
    # Default value: "30 seconds"
    #driver.pooling.heartbeat = "30 seconds"

    # The number of connections in the pool for nodes at "local" distance.
    # Type: number
    # Default value: 8
    #driver.pooling.local.connections = 8

    # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
    # Type: number
    # Default value: 32768
    #driver.pooling.local.requests = 32768

    # The number of connections in the pool for remote nodes.
    # Type: number
    # Default value: 1
    #driver.pooling.remote.connections = 1

    # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
    # Type: number
    # Default value: 1024
    #driver.pooling.remote.requests = 1024

    ################################################################################################
    # Native Protocol-specific settings.
    ################################################################################################

    # Specify the compression algorithm to use. Valid values are: `NONE`, `LZ4`, `SNAPPY`.
    # Type: string
    # Default value: "NONE"
    #driver.protocol.compression = "NONE"

    ################################################################################################
    # Query-related settings.
    ################################################################################################

    # The consistency level to use for both loading and unloading. Valid values are: `ANY`,
    # `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`, `EACH_QUORUM`, `ALL`.
    # 
    # Note that stronger consistency levels usually result in reduced throughput; besides, any level
    # higher than `ONE` will automatically disable continuous paging, which can dramatically reduce
    # read throughput.
    # Type: string
    # Default value: "LOCAL_ONE"
    #driver.query.consistency = "LOCAL_ONE"

    # The page size, or how many rows will be retrieved simultaneously in a single network round
    # trip. This setting will limit the number of results loaded into memory simultaneously during
    # unloading. Not applicable for loading.
    # Type: number
    # Default value: 5000
    #driver.query.fetchSize = 5000

    # The default idempotence of statements generated by the loader.
    # Type: boolean
    # Default value: true
    #driver.query.idempotence = true

    # The serial consistency level to use for writes. Only applicable if the data is inserted using
    # lightweight transactions, ignored otherwise. Valid values are: `SERIAL` and `LOCAL_SERIAL`.
    # Type: string
    # Default value: "LOCAL_SERIAL"
    #driver.query.serialConsistency = "LOCAL_SERIAL"

    ################################################################################################
    # Socket-related settings.
    ################################################################################################

    # The time the driver waits for a request to complete. This is a global limit on the duration of
    # a `session.execute()` call, including any internal retries the driver might do.
    # Type: string
    # Default value: "60 seconds"
    #driver.socket.readTimeout = "60 seconds"

    ################################################################################################
    # Encryption-specific settings.
    # 
    # For more information about how to configure this section, see the Java Secure Socket Extension
    # (JSSE) Reference Guide:
    # http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can
    # also check the DataStax Java driver documentation on SSL:
    # http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ################################################################################################

    # The cipher suites to enable. For example:
    # 
    # `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
    # 
    # This property is optional. If it is not present, the driver won't explicitly enable cipher
    # suites, which according to the JDK documentation results in "a minimum quality of service".
    # Type: list
    # Default value: []
    #driver.ssl.cipherSuites = []

    # The algorithm to use for the SSL keystore. Valid values are: `SunX509`, `NewSunX509`.
    # Type: string
    # Default value: "SunX509"
    #driver.ssl.keystore.algorithm = "SunX509"

    # The keystore password.
    # Type: string
    # Default value: ""
    #driver.ssl.keystore.password = ""

    # The path of the keystore file. This setting is optional. If left unspecified, no client
    # authentication will be used.
    # Type: string
    # Default value: ""
    #driver.ssl.keystore.path = ""

    # The path of the certificate chain file. This setting is optional. If left unspecified, no
    # client authentication will be used.
    # Type: string
    # Default value: ""
    #driver.ssl.openssl.keyCertChain = ""

    # The path of the private key file.
    # Type: string
    # Default value: ""
    #driver.ssl.openssl.privateKey = ""

    # The SSL provider to use. Valid values are:
    # 
    # - **None**: no SSL.
    # - **JDK**: uses the JDK SSLContext
    # - **OpenSSL**: uses Netty's native support for OpenSSL. It provides better performance and
    # generates less garbage. This is the recommended provider when using SSL.
    # Type: string
    # Default value: "None"
    #driver.ssl.provider = "None"

    # The algorithm to use for the SSL truststore. Valid values are: `PKIX`, `SunX509`.
    # Type: string
    # Default value: "SunX509"
    #driver.ssl.truststore.algorithm = "SunX509"

    # The truststore password.
    # Type: string
    # Default value: ""
    #driver.ssl.truststore.password = ""

    # The path of the truststore file. This setting is optional. If left unspecified, server
    # certificates will not be validated.
    # Type: string
    # Default value: ""
    #driver.ssl.truststore.path = ""

    ################################################################################################
    # Workflow Engine-specific settings.
    ################################################################################################

    # Enable or disable dry-run mode, a test mode that runs the command but does not load data. Not
    # applicable for unloading.
    # Type: boolean
    # Default value: false
    #engine.dryRun = false

    # A unique identifier to attribute to each execution. When unspecified or empty, the engine will
    # automatically generate identifiers of the following form: *workflow*_*timestamp*, where :
    # 
    # - *workflow* stands for the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - *timestamp* is the current timestamp formatted as `uuuuMMdd-HHmmss-SSSSSS` (see
    # [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns))
    # in UTC, with microsecond precision if available, and millisecond precision otherwise.
    # 
    # When this identifier is user-supplied, it is important to guarantee its uniqueness; failing to
    # do so may result in execution failures. It is also possible to provide templates here. Any
    # format compliant with the formatting rules of
    # [`String.format()`](https://docs.oracle.com/javase/8/docs/api/java/util/Formatter.html#syntax)
    # is accepted, and can contain the following parameters:
    # 
    # - `%1$s` : the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - `%2$t` : the current time (with microsecond precision if available, and millisecond
    # precision otherwise);
    # - `%3$s` : the JVM process PID (this parameter might not be available on some operating
    # systems; if its value cannot be determined, a random integer will be inserted instead).
    # Type: string
    # Default value: ""
    #engine.executionId = ""

    ################################################################################################
    # Executor-specific settings.
    ################################################################################################

    # The maximum number of concurrent operations per second. This acts as a safeguard to prevent
    # more requests than the cluster can handle. Batch statements are counted by the number of
    # statements included. Reduce this setting when the latencies get too high and a remote cluster
    # cannot keep up with throughput, as `dsbulk` requests will eventually time out. Setting this
    # option to any negative value will disable it.
    # Type: number
    # Default value: -1
    #executor.maxPerSecond = -1

    # Enable or disable continuous paging.
    # 
    # Note that if the target cluster does not support continuous paging, or if the consistency
    # level is not `ONE` or `LOCAL_ONE` (see `driver.query.consistency`), traditional paging will be
    # used regardless of this setting.
    # Type: boolean
    # Default value: true
    #executor.continuousPaging.enabled = true

    # The maximum number of pages to retrieve. Setting this value to zero retrieves all pages
    # available.
    # Type: number
    # Default value: 0
    #executor.continuousPaging.maxPages = 0

    # The maximum number of pages per second. Setting this value to zero indicates no limit.
    # Type: number
    # Default value: 0
    #executor.continuousPaging.maxPagesPerSecond = 0

    # The size of the page. The unit to use is determined by the `pageUnit` setting.
    # Type: number
    # Default value: 5000
    #executor.continuousPaging.pageSize = 5000

    # The unit to use for the `pageSize` setting. Possible values are: `ROWS`, `BYTES`.
    # Type: string
    # Default value: "ROWS"
    #executor.continuousPaging.pageUnit = "ROWS"

    # The maximum number of "in-flight" requests, or maximum number of concurrent requests waiting
    # for a response from the server. This acts as a safeguard to prevent more requests than the
    # cluster can handle. Batch statements count as one request. Reduce this value when the
    # throughput for reads and writes cannot match the throughput of mappers; this is usually a sign
    # that the workflow engine is not well calibrated and will eventually run out of memory. Setting
    # this option to any negative value will disable it.
    # Type: number
    # Default value: 1024
    #executor.maxInFlight = 1024

    ################################################################################################
    # Log and error management settings.
    ################################################################################################

    # The maximum number of errors to tolerate before aborting the entire operation. Set to either a
    # number or a string of the form `N%` where `N` is a decimal number between 0 and 100. Setting
    # this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 100
    #log.maxErrors = 100

    # The writable directory where all log files will be stored; if the directory specified does not
    # exist, it will be created. URLs are not acceptable (not even `file:/` URLs). Log files for a
    # specific run, or execution, will be located in a sub-directory under the specified directory.
    # Each execution generates a sub-directory identified by an "execution ID". See
    # `engine.executionId` for more information about execution IDs. Relative paths will be resolved
    # against the current working directory. Also, for convenience, if the path begins with a tilde
    # (`~`), that symbol will be expanded to the current user's home directory.
    # Type: string
    # Default value: "./logs"
    #log.directory = "./logs"

    # The desired log level. Valid values are:
    # 
    # - ABRIDGED: Print only basic information in summarized form.
    # - NORMAL: Print basic information in summarized form, and the statement's query string, if
    # available. For batch statements, this verbosity level also prints information about the
    # batch's inner statements.
    # - EXTENDED: Print full information, including the statement's query string, if available, and
    # the statement's bound values, if available. For batch statements, this verbosity level also
    # prints all information available about the batch's inner statements.
    # Type: string
    # Default value: "EXTENDED"
    #log.stmt.level = "EXTENDED"

    # The maximum length for a bound value. Bound values longer than this value will be truncated.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 50
    #log.stmt.maxBoundValueLength = 50

    # The maximum number of bound values to print. If the statement has more bound values than this
    # limit, the exceeding values will not be printed.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 50
    #log.stmt.maxBoundValues = 50

    # The maximum number of inner statements to print for a batch statement. Only applicable for
    # batch statements, ignored otherwise. If the batch statement has more children than this value,
    # the exceeding child statements will not be printed.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 10
    #log.stmt.maxInnerStatements = 10

    # The maximum length for a query string. Query strings longer than this value will be truncated.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 500
    #log.stmt.maxQueryStringLength = 500

    ################################################################################################
    # Monitoring-specific settings.
    ################################################################################################

    # The report interval for the console reporter. The console reporter will print useful metrics
    # about the ongoing operation at this rate. Durations lesser than one second will be rounded up
    # to 1 second.
    # Type: string
    # Default value: "5 seconds"
    #monitoring.reportRate = "5 seconds"

    # Enable or disable CSV reporting. If enabled, CSV files containing metrics will be generated in
    # the designated log directory.
    # Type: boolean
    # Default value: false
    #monitoring.csv = false

    # The time unit used when printing latency durations. Valid values: all `TimeUnit` enum
    # constants.
    # Type: string
    # Default value: "MILLISECONDS"
    #monitoring.durationUnit = "MILLISECONDS"

    # The expected total number of reads. Optional, but if set, the console reporter will also print
    # the overall achievement percentage. Setting this value to `-1` disables this feature.
    # Type: number
    # Default value: -1
    #monitoring.expectedReads = -1

    # The expected total number of writes. Optional, but if set, the console reporter will also
    # print the overall achievement percentage. Setting this value to `-1` disables this feature.
    # Type: number
    # Default value: -1
    #monitoring.expectedWrites = -1

    # Enable or disable JMX reporting. Note that to enable remote JMX reporting, several properties
    # must also be set in the JVM during launch. This is accomplished via the `DSBULK_JAVA_OPTS`
    # environment variable.
    # Type: boolean
    # Default value: true
    #monitoring.jmx = true

    # The time unit used when printing throughput rates. Valid values: all `TimeUnit` enum
    # constants.
    # Type: string
    # Default value: "SECONDS"
    #monitoring.rateUnit = "SECONDS"

}
