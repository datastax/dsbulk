####################################################################################################
# This is a template configuration file for the DataStax Bulk Loader (DSBulk).
#
# This file is written in HOCON format; see
# https://github.com/typesafehub/config/blob/master/HOCON.md
# for more information on its syntax.
#
# Uncomment settings as needed to configure DSBulk. When this file is named application.conf and
# placed in the /conf directory, it will be automatically picked up and used by default. To use
# other file names see the -f command-line option.
####################################################################################################

dsbulk {

    ################################################################################################
    # Connector-specific settings. This section contains settings for the connector to use; it also
    # contains sub-sections, one for each available connector.
    ################################################################################################

    # The name of the connector to use.
    # Type: string
    # Default value: "csv"
    #connector.name = "csv"

    ################################################################################################
    # CSV Connector configuration.
    ################################################################################################

    # The URL or path of the resource(s) to read from or write to.
    # 
    # Which URL protocols are available depend on which URL stream handlers have been installed, but
    # at least the **file** protocol is guaranteed to be supported for reads and writes, and the
    # **http** and **https** protocols are guaranteed to be supported for reads.
    # 
    # The file protocol can be used with all supported file systems, local or not.
    # - When reading: the URL can point to a single file, or to an existing directory; in case of a
    # directory, the *fileNamePattern* setting can be used to filter files to read, and the
    # *recursive* setting can be used to control whether or not the connector should look for files
    # in subdirectories as well.
    # - When writing: the URL will be treated as a directory; if it doesn't exist, the loader will
    # attempt to create it; CSV files will be created inside this directory, and their names can be
    # controlled with the *fileNameFormat* setting.
    # 
    # Note that if the value specified here does not have a protocol, then it is assumed to be a
    # file protocol. Relative URLs will be resolved against the current working directory. Also, for
    # convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the
    # current user's home directory.
    # 
    # In addition the value `-` indicates `stdin` when loading and `stdout` when unloading. This is
    # in line with Unix tools such as tar, which uses `-` to represent stdin/stdout when
    # reading/writing an archive.
    # 
    # Examples:
    # 
    # url = "/path/to/dir/or/file"           # without protocol
    # url = "./path/to/dir/or/file"          # without protocol, relative to working directory
    # url = "~/path/to/dir/or/file"          # without protocol, relative to the user's home
    # directory
    # url = "file:///path/to/dir/or/file"    # with file protocol
    # url = "http://acme.com/file.csv"       # with HTTP protocol
    # url = "-"                              # to read csv data from stdin (for load) or
    # url = "-"                              # write csv data to stdout (for unload)
    # 
    # For other URLs: the URL will be read or written directly; settings like *fileNamePattern*,
    # *recursive*, and *fileNameFormat* will have no effect.
    # 
    # The default value is `-` (read from `stdin` / write to `stdout`).
    # Type: string
    # Default value: "-"
    #connector.csv.url = "-"

    # The character to use as field delimiter.
    # Type: string
    # Default value: ","
    #connector.csv.delimiter = ","

    # Enable or disable whether the files to read or write begin with a header line. If enabled for
    # loading, the first non-empty line in every file will assign field names for each record
    # column, in lieu of `schema.mapping`, `fieldA = col1, fieldB = col2, fieldC = col3`. If
    # disabled for loading, records will not contain fields names, only field indexes, `0 = col1, 1
    # = col2, 2 = col3`. For unloading, if this setting is enabled, each file will begin with a
    # header line, and if disabled, each file will not contain a header line.
    # 
    # Note: This option will apply to all files loaded or unloaded.
    # Type: boolean
    # Default value: true
    #connector.csv.header = true

    # The number of records to skip from each input file before the parser can begin to execute.
    # Note that if the file contains a header line, that line is not counted as a valid record. This
    # setting is ignored when writing.
    # Type: number
    # Default value: 0
    #connector.csv.skipRecords = 0

    # The maximum number of records to read from or write to each file. When reading, all records
    # past this number will be discarded. When writing, a file will contain at most this number of
    # records; if more records remain to be written, a new file will be created using the
    # *fileNameFormat* setting. Note that when writing to anything other than a directory, this
    # setting is ignored. This setting takes into account the *header* setting: if a file begins
    # with a header line, that line is not counted as a record. This feature is disabled by default
    # (indicated by its `-1` value).
    # Type: number
    # Default value: -1
    #connector.csv.maxRecords = -1

    # The character used for quoting fields when the field delimiter is part of the field value.
    # Only one character can be specified. Note that this setting applies to all files to be read or
    # written.
    # Type: string
    # Default value: "\""
    #connector.csv.quote = "\""

    # The character that represents a line comment when found in the beginning of a line of text.
    # Only one character can be specified. Note that this setting applies to all files to be read or
    # written. This feature is disabled by default (indicated by its `null` character value).
    # Type: string
    # Default value: "\u0000"
    #connector.csv.comment = "\u0000"

    # The file encoding to use for all read or written files.
    # Type: string
    # Default value: "UTF-8"
    #connector.csv.encoding = "UTF-8"

    # The character used for escaping quotes inside an already quoted value. Only one character can
    # be specified. Note that this setting applies to all files to be read or written.
    # Type: string
    # Default value: "\\"
    #connector.csv.escape = "\\"

    # The file name format to use when writing. This setting is ignored when reading and for
    # non-file URLs. The file name must comply with the formatting rules of `String.format()`, and
    # must contain a `%d` format specifier that will be used to increment file name counters.
    # Type: string
    # Default value: "output-%0,6d.csv"
    #connector.csv.fileNameFormat = "output-%0,6d.csv"

    # The glob pattern to use when searching for files to read. The syntax to use is the glob
    # syntax, as described in `java.nio.file.FileSystem.getPathMatcher()`. This setting is ignored
    # when writing and for non-file URLs. Only applicable when the *url* setting points to a
    # directory on a known filesystem, ignored otherwise.
    # Type: string
    # Default value: "**/*.csv"
    #connector.csv.fileNamePattern = "**/*.csv"

    # The maximum number of characters that a field can contain. This setting is used to size
    # internal buffers and to avoid out-of-memory problems. If set to -1, internal buffers will be
    # resized dynamically. While convenient, this can lead to memory problems. It could also hurt
    # throughput, if some large fields require constant resizing; if this is the case, set this
    # value to a fixed positive number that is big enough to contain all field values.
    # Type: number
    # Default value: 4096
    #connector.csv.maxCharsPerColumn = 4096

    # The maximum number of files that can be written simultaneously. This setting is ignored when
    # reading and when the output URL is anything other than a directory on a filesystem. The
    # special syntax `NC` can be used to specify a number of threads that is a multiple of the
    # number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    # Type: string
    # Default value: "0.25C"
    #connector.csv.maxConcurrentFiles = "0.25C"

    # Enable or disable scanning for files in the root's subdirectories. Only applicable when *url*
    # is set to a directory on a known filesystem. Used for loading only.
    # Type: boolean
    # Default value: false
    #connector.csv.recursive = false

    ################################################################################################
    # JSON Connector configuration.
    ################################################################################################

    # The URL or path of the resource(s) to read from or write to.
    # 
    # Which URL protocols are available depend on which URL stream handlers have been installed, but
    # at least the **file** protocol is guaranteed to be supported for reads and writes, and the
    # **http** and **https** protocols are guaranteed to be supported for reads.
    # 
    # The file protocol can be used with all supported file systems, local or not.
    # - When reading: the URL can point to a single file, or to an existing directory; in case of a
    # directory, the *fileNamePattern* setting can be used to filter files to read, and the
    # *recursive* setting can be used to control whether or not the connector should look for files
    # in subdirectories as well.
    # - When writing: the URL will be treated as a directory; if it doesn't exist, the loader will
    # attempt to create it; json files will be created inside this directory, and their names can be
    # controlled with the *fileNameFormat* setting.
    # 
    # Note that if the value specified here does not have a protocol, then it is assumed to be a
    # file protocol. Relative URLs will be resolved against the current working directory. Also, for
    # convenience, if the path begins with a tilde (`~`), that symbol will be expanded to the
    # current user's home directory.
    # 
    # In addition the value `-` indicates `stdin` when loading and `stdout` when unloading. This is
    # in line with Unix tools such as tar, which uses `-` to represent stdin/stdout when
    # reading/writing an archive.
    # 
    # Examples:
    # 
    # url = "/path/to/dir/or/file"           # without protocol
    # url = "./path/to/dir/or/file"          # without protocol, relative to working directory
    # url = "~/path/to/dir/or/file"          # without protocol, relative to the user's home
    # directory
    # url = "file:///path/to/dir/or/file"    # with file protocol
    # url = "http://acme.com/file.json"      # with HTTP protocol
    # url = "-"                              # to read json data from stdin (for load) or
    # url = "-"                              # write json data to stdout (for unload)
    # 
    # For other URLs: the URL will be read or written directly; settings like *fileNamePattern*,
    # *recursive*, and *fileNameFormat* will have no effect.
    # 
    # The default value is `-` (read from `stdin` / write to `stdout`).
    # Type: string
    # Default value: "-"
    #connector.json.url = "-"

    # The number of JSON records to skip from each input file before the parser can begin to
    # execute. This setting is ignored when writing.
    # Type: number
    # Default value: 0
    #connector.json.skipRecords = 0

    # The maximum number of records to read from or write to each file. When reading, all records
    # past this number will be discarded. When writing, a file will contain at most this number of
    # records; if more records remain to be written, a new file will be created using the
    # *fileNameFormat* setting. Note that when writing to anything other than a directory, this
    # setting is ignored. This feature is disabled by default (indicated by its `-1` value).
    # Type: number
    # Default value: -1
    #connector.json.maxRecords = -1

    # The mode for loading and unloading JSON documents. Valid values are:
    # 
    # * MULTI_DOCUMENT: Each resource may contain an arbitrary number of successive JSON documents
    # to be mapped to records. For example the format of each JSON document is a single document:
    # `{doc1}`. The root directory for the JSON documents can be specified with `url` and the
    # documents can be read recursively by setting `connector.json.recursive` to true.
    # * SINGLE_DOCUMENT: Each resource contains a root array whose elements are JSON documents to be
    # mapped to records. For example, the format of the JSON document is an array with embedded JSON
    # documents: `[ {doc1}, {doc2}, {doc3} ]`.
    # Type: string
    # Default value: "MULTI_DOCUMENT"
    #connector.json.mode = "MULTI_DOCUMENT"

    # A map of JSON deserialization features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.databind.DeserializationFeature`. The default value is the only way to
    # guarantee that floating point numbers will not have their precision truncated when parsed, but
    # can result in slightly slower parsing. Used for loading only.
    # 
    # Note that some Jackson features might not be supported, in particular features that operate on
    # the resulting Json tree by filtering elements or altering their contents, since such features
    # conflict with dsbulk's own filtering and formatting capabilities. Instead of trying to modify
    # the resulting tree using Jackson features, you should try to achieve the same result using the
    # settings available under the `codec` and `schema` sections.
    # Type: map<string,boolean>
    # Default value: {"USE_BIG_DECIMAL_FOR_FLOATS":true}
    #connector.json.deserializationFeatures = {"USE_BIG_DECIMAL_FOR_FLOATS":true}

    # The file encoding to use for all read or written files.
    # Type: string
    # Default value: "UTF-8"
    #connector.json.encoding = "UTF-8"

    # The file name format to use when writing. This setting is ignored when reading and for
    # non-file URLs. The file name must comply with the formatting rules of `String.format()`, and
    # must contain a `%d` format specifier that will be used to increment file name counters.
    # Type: string
    # Default value: "output-%0,6d.json"
    #connector.json.fileNameFormat = "output-%0,6d.json"

    # The glob pattern to use when searching for files to read. The syntax to use is the glob
    # syntax, as described in `java.nio.file.FileSystem.getPathMatcher()`. This setting is ignored
    # when writing and for non-file URLs. Only applicable when the *url* setting points to a
    # directory on a known filesystem, ignored otherwise.
    # Type: string
    # Default value: "**/*.json"
    #connector.json.fileNamePattern = "**/*.json"

    # JSON generator features to enable. Valid values are all the enum constants defined in
    # `com.fasterxml.jackson.core.JsonGenerator.Feature`. For example, a value of `{
    # ESCAPE_NON_ASCII : true, QUOTE_FIELD_NAMES : true }` will configure the generator to escape
    # all characters beyond 7-bit ASCII and quote field names when writing JSON output. Used for
    # unloading only.
    # 
    # Note that some Jackson features might not be supported, in particular features that operate on
    # the resulting Json tree by filtering elements or altering their contents, since such features
    # conflict with dsbulk's own filtering and formatting capabilities. Instead of trying to modify
    # the resulting tree using Jackson features, you should try to achieve the same result using the
    # settings available under the `codec` and `schema` sections.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.generatorFeatures = {}

    # The maximum number of files that can be written simultaneously. This setting is ignored when
    # reading and when the output URL is anything other than a directory on a filesystem. The
    # special syntax `NC` can be used to specify a number of threads that is a multiple of the
    # number of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
    # Type: string
    # Default value: "0.25C"
    #connector.json.maxConcurrentFiles = "0.25C"

    # JSON parser features to enable. Valid values are all the enum constants defined in
    # `com.fasterxml.jackson.core.JsonParser.Feature`. For example, a value of `{ ALLOW_COMMENTS :
    # true, ALLOW_SINGLE_QUOTES : true }` will configure the parser to allow the use of comments and
    # single-quoted strings in JSON data. Used for loading only.
    # 
    # Note that some Jackson features might not be supported, in particular features that operate on
    # the resulting Json tree by filtering elements or altering their contents, since such features
    # conflict with dsbulk's own filtering and formatting capabilities. Instead of trying to modify
    # the resulting tree using Jackson features, you should try to achieve the same result using the
    # settings available under the `codec` and `schema` sections.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.parserFeatures = {}

    # Enable or disable pretty printing. When enabled, JSON records are written with indents. Used
    # for unloading only.
    # 
    # Note: Can result in much bigger records.
    # Type: boolean
    # Default value: false
    #connector.json.prettyPrint = false

    # Enable or disable scanning for files in the root's subdirectories. Only applicable when *url*
    # is set to a directory on a known filesystem. Used for loading only.
    # Type: boolean
    # Default value: false
    #connector.json.recursive = false

    # A map of JSON serialization features to set. Map keys should be enum constants defined in
    # `com.fasterxml.jackson.databind.SerializationFeature`. Used for unloading only.
    # 
    # Note that some Jackson features might not be supported, in particular features that operate on
    # the resulting Json tree by filtering elements or altering their contents, since such features
    # conflict with dsbulk's own filtering and formatting capabilities. Instead of trying to modify
    # the resulting tree using Jackson features, you should try to achieve the same result using the
    # settings available under the `codec` and `schema` sections.
    # Type: map<string,boolean>
    # Default value: {}
    #connector.json.serializationFeatures = {}

    # The strategy to use for filtering out entries when formatting output. Valid values are enum
    # constants defined in `com.fasterxml.jackson.annotation.JsonInclude.Include` (but beware that
    # the `CUSTOM` strategy cannot be honored). Used for unloading only.
    # Type: string
    # Default value: "ALWAYS"
    #connector.json.serializationStrategy = "ALWAYS"

    ################################################################################################
    # Schema-specific settings.
    ################################################################################################

    # Keyspace used for loading or unloading data. Required option if `schema.query` is not
    # specified; otherwise, optional.
    # 
    # Keyspace names should not be quoted and will be treated in a case-sensitive manner, i.e.,
    # `MyKeyspace` will match
    # a keyspace named "MyKeyspace" but will not match a keyspace named "mykeyspace".
    # Type: string
    # Default value: ""
    #schema.keyspace = ""

    # Table used for loading or unloading data. Required option if `schema.query` is not specified;
    # otherwise, optional.
    # 
    # Table names should not be quoted and will be treated in a case-sensitive manner, i.e.,
    # `MyTable` will match
    # a table named "MyTable" but will not match a table named "mytable".
    # Type: string
    # Default value: ""
    #schema.table = ""

    # The field-to-column mapping to use, that applies to both loading and unloading. If not
    # specified, the loader will apply a strict one-to-one mapping between the source fields and the
    # database table. If that is not what you want, then you must supply an explicit mapping.
    # Mappings should be specified as a map of the following form:
    # 
    # - Indexed data sources: `0 = col1, 1 = col2, 2 = col3`, where `0`, `1`, `2`, are the
    # zero-based indices of fields in the source data; and `col1`, `col2`, `col3` are bound variable
    # names in the insert statement.
    # - A shortcut to map the first `n` fields is to simply specify the destination columns: `col1,
    # col2, col3`.
    # - Mapped data sources: `fieldA = col1, fieldB = col2, fieldC = col3`, where `fieldA`,
    # `fieldB`, `fieldC`, are field names in the source data; and `col1`, `col2`, `col3` are bound
    # variable names in the insert statement.
    # 
    # To specify that a field should be used as the timestamp (a.k.a. write-time) or ttl (a.k.a.
    # time-to-live) of the inserted row, use the specially named fake columns `__ttl` and
    # `__timestamp`: `fieldA = __timestamp, fieldB = __ttl`. Note that Timestamp fields are parsed
    # as regular CQL timestamp columns and must comply with either `codec.timestamp`, or
    # alternatively, with `codec.unit` + `codec.epoch`. TTL fields are parsed as integers
    # representing durations in seconds, and must comply with `codec.number`.
    # 
    # To specify that a column should be populated with the result of a function call, specify the
    # function call as the input field (e.g. `now() = c4`). Note, this is only relevant for load
    # operations. In addition, for mapped data sources, it is also possible to specify that the
    # mapping be partly auto-generated and partly explicitly specified. For example, if a source row
    # has fields `c1`, `c2`, `c3`, and `c5`, and the table has columns `c1`, `c2`, `c3`, `c4`, one
    # can map all like-named columns and specify that `c5` in the source maps to `c4` in the table
    # as follows: `* = *, c5 = c4`.
    # 
    # One can specify that all like-named fields be mapped, except for `c2`: `* = -c2`. To skip `c2`
    # and `c3`: `* = [-c2, -c3]`.
    # 
    # The exact type of mapping to use depends on the connector being used. Some connectors can only
    # produce indexed records; others can only produce mapped ones, while others are capable of
    # producing both indexed and mapped records at the same time. Refer to the connector's
    # documentation to know which kinds of mapping it supports.
    # Type: string
    # Default value: ""
    #schema.mapping = ""

    # Specify whether or not to accept records that contain extra fields that are not declared in
    # the mapping. For example, if a record contains three fields A, B, and C, but the mapping only
    # declares fields A and B, then if this option is true, C will be silently ignored and the
    # record will be considered valid, and if false, the record will be rejected. Only applicable
    # for loading, ignored otherwise.
    # Type: boolean
    # Default value: true
    #schema.allowExtraFields = true

    # Specify whether or not to accept records that are missing fields declared in the mapping. For
    # example, if the mapping declares three fields A, B, and C, but a record contains only fields A
    # and B, then if this option is true, C will be silently assigned null and the record will be
    # considered valid, and if false, the record will be rejected. If the missing field is mapped to
    # a primary key column, the record will always be rejected, since the database will reject the
    # record. Only applicable for loading, ignored otherwise.
    # Type: boolean
    # Default value: false
    #schema.allowMissingFields = false

    # Specify whether to map `null` input values to "unset" in the database, i.e., don't modify a
    # potentially pre-existing value of this field for this row. Valid for load scenarios, otherwise
    # ignore. Note that setting to false creates tombstones to represent `null`.
    # 
    # Note that this setting is applied after the *codec.nullStrings* setting, and may intercept
    # `null`s produced by that setting.
    # Type: boolean
    # Default value: true
    #schema.nullToUnset = true

    # The query to use. If not specified, then *schema.keyspace* and *schema.table* must be
    # specified, and dsbulk will infer the appropriate statement based on the table's metadata,
    # using all available columns. If `schema.keyspace` is provided, the query need not include the
    # keyspace to qualify the table reference.
    # 
    # For loading, the statement can be any `INSERT` or `UPDATE` statement.
    # 
    # For unloading, the statement can be any regular `SELECT` statement; it can optionally contain
    # a token range restriction clause of the form: `token(...) > :start and token(...) <= :end`. If
    # such a clause is present, the engine will generate as many statements as there are token
    # ranges in the cluster, thus allowing parallelization of reads while at the same time targeting
    # coordinators that are also replicas.
    # 
    # Statements can use both positional and named bound variables. Positional variables will be
    # named after their corresponding column in the destination table. Named bound variables usually
    # have names matching those of the columns in the destination table, but this is not a strict
    # requirement; it is, however, required that their names match those specified in the mapping.
    # 
    # Note: The dsbulk query is parsed to discover which bound variables are present, to map the
    # variable correctly to fields.
    # 
    # See *schema.mapping* setting for more information.
    # Type: string
    # Default value: ""
    #schema.query = ""

    # The timestamp of inserted/updated cells during load; otherwise, the current time of the system
    # running the tool is used. Not applicable to unloading. The value must be expressed in
    # [`ISO_ZONED_DATE_TIME`](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_ZONED_DATE_TIME)
    # format.
    # 
    # Query timestamps for DSE have microsecond resolution; any sub-microsecond information
    # specified is lost. For more information, see the [CQL
    # Reference](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__timestamp-value).
    # Type: string
    # Default value: ""
    #schema.queryTimestamp = ""

    # The Time-To-Live (TTL) of inserted/updated cells during load (seconds); a value of -1 means
    # there is no TTL. Not applicable to unloading. For more information, see the [CQL
    # Reference](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__ime-value),
    # [Setting the time-to-live (TTL) for
    # value](http://docs.datastax.com/en/dse/6.0/cql/cql/cql_using/useTTL.html), and [Expiring data
    # with time-to-live](http://docs.datastax.com/en/dse/6.0/cql/cql/cql_using/useExpire.html).
    # Type: number
    # Default value: -1
    #schema.queryTtl = -1

    ################################################################################################
    # Batch-specific settings.
    # 
    # These settings control how the workflow engine groups together statements before writing them.
    # 
    # Only applicable for loading.
    # 
    # See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
    ################################################################################################

    # The buffer size to use for flushing batching statements. Do not set higher than `maxBatchSize`
    # unless the loaded data is unsorted, when a higher value could improve performance. When set to
    # a negative value the buffer size is implicitly set to `maxBatchSize`.
    # Type: number
    # Default value: -1
    #batch.bufferSize = -1

    # Enable or disable statement batching.
    # Type: boolean
    # Default value: true
    #batch.enabled = true

    # The maximum batch size that depends on the size of the data inserted and the batch mode in
    # use. Larger data requires a smaller value. For batch mode, `PARTITION_KEY` requires larger
    # batch sizes, whereas `REPLICA_SET` requires smaller batch sizes, such as below 10.
    # Type: number
    # Default value: 32
    #batch.maxBatchSize = 32

    # The grouping mode. Valid values are:
    # - PARTITION_KEY: Groups together statements that share the same partition key. This is the
    # default mode, and the preferred one.
    # - REPLICA_SET: Groups together statements that share the same replica set. This mode might
    # yield better results for small clusters and lower replication factors, but tends to perform
    # equally well or worse than `PARTITION_KEY` for larger clusters or high replication factors.
    # Type: string
    # Default value: "PARTITION_KEY"
    #batch.mode = "PARTITION_KEY"

    ################################################################################################
    # Conversion-specific settings. These settings apply for both load and unload workflows.
    # 
    # When writing, these settings determine how record fields emitted by connectors are parsed.
    # 
    # When unloading, these settings determine how row cells emitted by DSE are formatted.
    ################################################################################################

    # Set how true and false representations of numbers are interpreted. The representation is of
    # the form `true_value,false_value`. The mapping is reciprocal, so that numbers are mapping to
    # Boolean and vice versa. All numbers unspecified in this setting are rejected.
    # Type: list<number>
    # Default value: [1,0]
    #codec.booleanNumbers = [1,0]

    # Specify how true and false representations can be used by dsbulk. Each representation is of
    # the form `true_value:false_value`, case-insensitive. For loading, all representations are
    # honored: when a record field value exactly matches one of the specified strings, the value is
    # replaced with `true` of `false` before writing to DSE. For unloading, this setting is only
    # applicable for string-based connectors, such as the CSV connector: the first representation
    # will be used to format booleans before they are written out, and all others are ignored.
    # Type: list<string>
    # Default value: ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]
    #codec.booleanStrings = ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]

    # The temporal pattern to use for `String` to CQL `date` conversion. Valid choices:
    # 
    # - A date-time pattern such as `yyyy-MM-dd`.
    # - A pre-defined formatter such as `ISO_LOCAL_DATE`. Any public static field in
    # `java.time.format.DateTimeFormatter` can be used.
    # 
    # For more information on patterns and pre-defined formatters, see [Patterns for Formatting and
    # Parsing](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns)
    # in Oracle Java documentation.
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # Type: string
    # Default value: "ISO_LOCAL_DATE"
    #codec.date = "ISO_LOCAL_DATE"

    # This setting applies only to CQL `timestamp` columns, and `USING TIMESTAMP` clauses in
    # queries. If the input is a string containing only digits that cannot be parsed using the
    # `codec.timestamp` format, the specified epoch determines the relative point in time used with
    # the parsed value. The value must be expressed in
    # [`ISO_ZONED_DATE_TIME`](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#ISO_ZONED_DATE_TIME)
    # format.
    # Type: string
    # Default value: "1970-01-01T00:00:00Z"
    #codec.epoch = "1970-01-01T00:00:00Z"

    # Whether or not to use the `codec.number` pattern to format numeric output. When set to `true`,
    # the numeric pattern defined by `codec.number` will be applied. This allows for
    # nicely-formatted output, but may result in rounding (see `codec.roundingStrategy`), or
    # alteration of the original decimal's scale. When set to `false`, numbers will be stringified
    # using the `toString()` method, and will never result in rounding or scale alteration. Only
    # applicable when unloading, and only if the connector in use requires stringification, because
    # the connector, such as the CSV connector, does not handle raw numeric data; ignored otherwise.
    # Type: boolean
    # Default value: false
    #codec.formatNumbers = false

    # The locale to use for locale-sensitive conversions.
    # Type: string
    # Default value: "en_US"
    #codec.locale = "en_US"

    # Comma-separated list of case-sensitive strings that should be mapped to `null`. For loading,
    # when a record field value exactly matches one of the specified strings, the value is replaced
    # with `null` before writing to DSE. For unloading, this setting is only applicable for
    # string-based connectors, such as the CSV connector: the first string specified will be used to
    # change a row cell containing `null` to the specified string when written out.
    # 
    # For example, setting this to `["NULL"]` will cause a field containing the word `NULL` to be
    # mapped to `null` while loading, and a column containing `null` to be converted to the word
    # `NULL` while unloading.
    # 
    # The default value is `[]` (no strings are mapped to `null`). In the default mode, DSBulk
    # behaves as follows:
    # * When loading, if the target CQL type is textual (i.e. text, varchar or ascii), the original
    # field value is left untouched; for other types, if the value is an empty string, it is
    # converted to `null`.
    # * When unloading, all `null` values are converted to an empty string.
    # 
    # Note that, regardless of this setting, DSBulk will always convert empty strings to `null` if
    # the target CQL type is not textual (i.e. not text, varchar or ascii).
    # 
    # This setting is applied before `schema.nullToUnset`, hence any `null` produced by a
    # null-string can still be left unset if required.
    # Type: list
    # Default value: []
    #codec.nullStrings = []

    # The `DecimalFormat` pattern to use for conversions between `String` and CQL numeric types.
    # 
    # See
    # [java.text.DecimalFormat](https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html)
    # for details about the pattern syntax to use.
    # 
    # Most inputs are recognized: optional localized thousands separator, localized decimal
    # separator, or optional exponent. Using locale `en_US`, `1234`, `1,234`, `1234.5678`,
    # `1,234.5678` and `1,234.5678E2` are all valid. For unloading and formatting, rounding may
    # occur and cause precision loss. See `codec.formatNumbers` and `codec.roundingStrategy`.
    # Type: string
    # Default value: "#,###.##"
    #codec.number = "#,###.##"

    # This setting can mean one of three possibilities:
    # 
    # - The value is outside the range of the target CQL type. For example, trying to convert 128 to
    # a CQL `tinyint` (max value of 127) results in overflow.
    # - The value is decimal, but the target CQL type is integral. For example, trying to convert
    # 123.45 to a CQL `int` results in overflow.
    # - The value's precision is too large for the target CQL type. For example, trying to insert
    # 0.1234567890123456789 into a CQL `double` results in overflow, because there are too many
    # significant digits to fit in a 64-bit double.
    # 
    # Valid choices:
    # 
    # - `REJECT`: overflows are considered errors and the data is rejected. This is the default
    # value.
    # - `TRUNCATE`: the data is truncated to fit in the target CQL type. The truncation algorithm is
    # similar to the narrowing primitive conversion defined in The Java Language Specification,
    # Section 5.1.3, with the following exceptions:
    # - If the value is too big or too small, it is rounded up or down to the maximum or minimum
    # value allowed, rather than truncated at bit level. For example, 128 would be rounded down to
    # 127 to fit in a byte, whereas Java would have truncated the exceeding bits and converted to
    # -127 instead.
    # - If the value is decimal, but the target CQL type is integral, it is first rounded to an
    # integral using the defined rounding strategy, then narrowed to fit into the target type. This
    # can result in precision loss and should be used with caution.
    # 
    # Only applicable for loading, when parsing numeric inputs; it does not apply for unloading,
    # since formatting never results in overflow.
    # Type: string
    # Default value: "REJECT"
    #codec.overflowStrategy = "REJECT"

    # The rounding strategy to use for conversions from CQL numeric types to `String`.
    # 
    # Valid choices: any `java.math.RoundingMode` enum constant name, including: `CEILING`, `FLOOR`,
    # `UP`, `DOWN`, `HALF_UP`, `HALF_EVEN`, `HALF_DOWN`, and `UNNECESSARY`. The precision used when
    # rounding is inferred from the numeric pattern declared under `codec.number`. For example, the
    # default `codec.number` (`#,###.##`) has a rounding precision of 2, and the number 123.456
    # would be rounded to 123.46 if `roundingStrategy` was set to `UP`. The default value will
    # result in infinite precision, and ignore the `codec.number` setting.
    # 
    # Only applicable when unloading, if `codec.formatNumbers` is true and if the connector in use
    # requires stringification, because the connector, such as the CSV connector, does not handle
    # raw numeric data; ignored otherwise.
    # Type: string
    # Default value: "UNNECESSARY"
    #codec.roundingStrategy = "UNNECESSARY"

    # The temporal pattern to use for `String` to CQL `time` conversion. Valid choices:
    # 
    # - A date-time pattern, such as `HH:mm:ss`.
    # - A pre-defined formatter, such as `ISO_LOCAL_TIME`. Any public static field in
    # `java.time.format.DateTimeFormatter` can be used.
    # 
    # For more information on patterns and pre-defined formatters, see [Patterns for formatting and
    # Parsing](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns)
    # in Oracle Java documentation.
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # Type: string
    # Default value: "ISO_LOCAL_TIME"
    #codec.time = "ISO_LOCAL_TIME"

    # The time zone to use for temporal conversions. When loading, the time zone will be used to
    # obtain a timestamp from inputs that do not convey any explicit time zone information. When
    # unloading, the time zone will be used to format all timestamps.
    # Type: string
    # Default value: "UTC"
    #codec.timeZone = "UTC"

    # The temporal pattern to use for `String` to CQL `timestamp` conversion. Valid choices:
    # 
    # - A date-time pattern such as `yyyy-MM-dd HH:mm:ss`.
    # - A pre-defined formatter such as `ISO_ZONED_DATE_TIME` or `ISO_INSTANT`. Any public static
    # field in `java.time.format.DateTimeFormatter` can be used.
    # - The special formatter `CQL_DATE_TIME`, which is a special parser that accepts all valid CQL
    # literal formats for the `timestamp` type.
    # 
    # For more information on patterns and pre-defined formatters, see [Patterns for Formatting and
    # Parsing](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns)
    # in Oracle Java documentation.
    # 
    # For more information about CQL date, time and timestamp literals, see [Date, time, and
    # timestamp
    # format](https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/refDateTimeFormats.html?hl=timestamp).
    # 
    # The default value is the special `CQL_DATE_TIME` value. When parsing, this format recognizes
    # all CQL temporal literals; if the input is a local date or date/time, the timestamp is
    # resolved using the time zone specified under `timeZone`. When formatting, this format uses the
    # `ISO_OFFSET_DATE_TIME` pattern, which is compliant with both CQL and ISO-8601.
    # Type: string
    # Default value: "CQL_DATE_TIME"
    #codec.timestamp = "CQL_DATE_TIME"

    # This setting applies only to CQL `timestamp` columns, and `USING TIMESTAMP` clauses in
    # queries. If the input is a string containing only digits that cannot be parsed using the
    # `codec.timestamp` format, the specified time unit is applied to the parsed value. All
    # `TimeUnit` enum constants are valid choices.
    # Type: string
    # Default value: "MILLISECONDS"
    #codec.unit = "MILLISECONDS"

    # Strategy to use when generating time-based (version 1) UUIDs from timestamps. Clock sequence
    # and node ID parts of generated UUIDs are determined on a best-effort basis and are not fully
    # compliant with RFC 4122. Valid values are:
    # 
    # - RANDOM: Generates UUIDs using a random number in lieu of the local clock sequence and node
    # ID. This strategy will ensure that the generated UUIDs are unique, even if the original
    # timestamps are not guaranteed to be unique.
    # - FIXED: Preferred strategy if original timestamps are guaranteed unique, since it is faster.
    # Generates UUIDs using a fixed local clock sequence and node ID.
    # - MIN: Generates the smallest possible type 1 UUID for a given timestamp. Warning: this
    # strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    # - MAX: Generates the biggest possible type 1 UUID for a given timestamp. Warning: this
    # strategy doesn't guarantee uniquely generated UUIDs and should be used with caution.
    # Type: string
    # Default value: "RANDOM"
    #codec.uuidStrategy = "RANDOM"

    ################################################################################################
    # Driver-specific configuration.
    ################################################################################################

    # The contact points to use for the initial connection to the cluster. This must be a
    # comma-separated list of hosts, each specified by a host-name or ip address. If the host is a
    # DNS name that resolves to multiple A-records, all the corresponding addresses will be used. Do
    # not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on some
    # platforms). The port for all hosts must be specified with `driver.port`.
    # Type: list<string>
    # Default value: ["127.0.0.1"]
    #driver.hosts = ["127.0.0.1"]

    # The native transport port to connect to. This must match DSE's
    # [native_transport_port](https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html#configCassandra_yaml_r__native_transport_port)
    # configuration option.
    # 
    # Note that all nodes in a cluster must accept connections on the same port number. Mixed-port
    # clusters are not supported.
    # Type: number
    # Default value: 9042
    #driver.port = 9042

    # The simple or fully-qualified class name of the address translator to use. This is only needed
    # if the nodes are not directly reachable from the machine on which dsbulk is running (for
    # example, the dsbulk machine is in a different network region and needs to use a public IP, or
    # it connects through a proxy).
    # Type: string
    # Default value: "IdentityTranslator"
    #driver.addressTranslator = "IdentityTranslator"

    # The simple or fully-qualified class name of the timestamp generator to use. Built-in options
    # are:
    # 
    # - AtomicMonotonicTimestampGenerator: timestamps are guaranteed to be unique across all client
    # threads.
    # - ThreadLocalTimestampGenerator: timestamps are guaranteed to be unique within each thread
    # only.
    # - ServerSideTimestampGenerator: do not generate timestamps, let the server assign them.
    # Type: string
    # Default value: "AtomicMonotonicTimestampGenerator"
    #driver.timestampGenerator = "AtomicMonotonicTimestampGenerator"

    ################################################################################################
    # Authentication settings.
    ################################################################################################

    # The username to use. Providers that accept this setting:
    # 
    # - `PlainTextAuthProvider`
    # - `DsePlainTextAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.username = ""

    # The password to use. Providers that accept this setting:
    # 
    # - `PlainTextAuthProvider`
    # - `DsePlainTextAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.password = ""

    # The name of the AuthProvider to use. Valid choices are:
    # 
    # - None: no authentication.
    # - PlainTextAuthProvider: Uses `com.datastax.driver.core.PlainTextAuthProvider` for
    # authentication. Supports SASL authentication using the `PLAIN` mechanism (plain text
    # authentication).
    # - DsePlainTextAuthProvider: Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for
    # authentication. Supports SASL authentication to DSE clusters using the `PLAIN` mechanism
    # (plain text authentication), and also supports optional proxy authentication; should be
    # preferred to `PlainTextAuthProvider` when connecting to secured DSE clusters.
    # - DseGSSAPIAuthProvider: Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for
    # authentication. Supports SASL authentication to DSE clusters using the `GSSAPI` mechanism
    # (Kerberos authentication), and also supports optional proxy authentication.
    # - Note: When using this provider you may have to set the `java.security.krb5.conf` system
    # property to point to your `krb5.conf` file (e.g. set the `DSBULK_JAVA_OPTS` environment
    # variable to `-Djava.security.krb5.conf=/home/user/krb5.conf`). See the [Oracle Java Kerberos
    # documentation](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/KerberosReq.html)
    # for more details.
    # Type: string
    # Default value: "None"
    #driver.auth.provider = "None"

    # An authorization ID allows the currently authenticated user to act as a different user (proxy
    # authentication). Providers that accept this setting:
    # 
    # - `DsePlainTextAuthProvider`
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.authorizationId = ""

    # The path of the Kerberos keytab file to use for authentication. If left unspecified,
    # authentication uses a ticket cache. Providers that accept this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.keyTab = ""

    # The Kerberos principal to use. For example, `user@datastax.com`. If left unspecified, the
    # principal is chosen from the first key in the ticket cache or keytab. Providers that accept
    # this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: ""
    #driver.auth.principal = ""

    # The SASL service name to use. This value should match the username of the Kerberos service
    # principal used by the DSE server. This information is specified in the `dse.yaml` file by the
    # *service_principal* option under the *kerberos_options* section, and may vary from one DSE
    # installation to another - especially if you installed DSE with an automated package installer.
    # Providers that accept this setting:
    # 
    # - `DseGSSAPIAuthProvider`
    # Type: string
    # Default value: "dse"
    #driver.auth.saslService = "dse"

    ################################################################################################
    # Settings for various driver policies.
    ################################################################################################

    # The name of the load balancing policy. Supported policies include: `dse`, `dcAwareRoundRobin`,
    # `roundRobin`, `whiteList`, `tokenAware`. Available options for the policies are listed below
    # as appropriate. For more information, refer to the driver documentation for the policy. If not
    # specified, defaults to the driver's default load balancing policy, which is currently the
    # `DseLoadBalancingPolicy` wrapping a `TokenAwarePolicy`, wrapping a `DcAwareRoundRobinPolicy`.
    # 
    # Note: It is critical for a token-aware policy to be used in the chain in order to benefit from
    # batching by partition key.
    # Type: string
    # Default value: ""
    #driver.policy.lbp.name = ""

    # Enable or disable whether to allow remote datacenters to count for local consistency level in
    # round robin awareness.
    # Type: boolean
    # Default value: false
    #driver.policy.lbp.dcAwareRoundRobin.allowRemoteDCsForLocalConsistencyLevel = false

    # The datacenter name (commonly dc1, dc2, etc.) local to the machine on which dsbulk is running,
    # so that requests are sent to nodes in the local datacenter whenever possible.
    # Type: string
    # Default value: ""
    #driver.policy.lbp.dcAwareRoundRobin.localDc = ""

    # The number of hosts per remote datacenter that the round robin policy should consider.
    # Type: number
    # Default value: 0
    #driver.policy.lbp.dcAwareRoundRobin.usedHostsPerRemoteDc = 0

    # The child policy that the specified `dse` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.dse.childPolicy = "roundRobin"

    # The child policy that the specified `tokenAware` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.tokenAware.childPolicy = "roundRobin"

    # Specify how to order replicas.
    # 
    # Valid values are all `TokenAwarePolicy.ReplicaOrdering` enum constants:
    # 
    # - RANDOM: Return replicas in a different, random order for each query plan. This is the
    # default strategy;
    # for loading, it should be preferred has it can improve performance by distributing writes
    # across replicas.
    # - TOPOLOGICAL: Order replicas by token ring topology, i.e. always return the "primary" replica
    # first.
    # - NEUTRAL: Return the replicas in the exact same order in which they appear in the child
    # policy's query plan.
    # Type: string
    # Default value: "RANDOM"
    #driver.policy.lbp.tokenAware.replicaOrdering = "RANDOM"

    # The child policy that the specified `whiteList` policy wraps.
    # Type: string
    # Default value: "roundRobin"
    #driver.policy.lbp.whiteList.childPolicy = "roundRobin"

    # List of hosts to white list. This must be a comma-separated list of hosts, each specified by a
    # host-name or ip address. If the host is a DNS name that resolves to multiple A-records, all
    # the corresponding addresses will be used. Do not use `localhost` as a host-name (since it
    # resolves to both IPv4 and IPv6 addresses on some platforms).
    # Type: list
    # Default value: []
    #driver.policy.lbp.whiteList.hosts = []

    # Maximum number of retries for a timed-out request.
    # Type: number
    # Default value: 10
    #driver.policy.maxRetries = 10

    ################################################################################################
    # Pooling-specific settings.
    # 
    # The driver maintains a connection pool to each node, according to the distance assigned to it
    # by the load balancing policy. If the distance is `IGNORED`, no connections are maintained.
    ################################################################################################

    # The heartbeat interval. If a connection stays idle for that duration (no reads), the driver
    # sends a dummy message on it to make sure it's still alive. If not, the connection is trashed
    # and replaced.
    # Type: string
    # Default value: "30 seconds"
    #driver.pooling.heartbeat = "30 seconds"

    # The number of connections in the pool for nodes at "local" distance.
    # Type: number
    # Default value: 8
    #driver.pooling.local.connections = 8

    # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
    # Type: number
    # Default value: 32768
    #driver.pooling.local.requests = 32768

    # The number of connections in the pool for remote nodes.
    # Type: number
    # Default value: 1
    #driver.pooling.remote.connections = 1

    # The maximum number of requests (1 to 32768) that can be executed concurrently on a connection.
    # Type: number
    # Default value: 1024
    #driver.pooling.remote.requests = 1024

    ################################################################################################
    # Native Protocol-specific settings.
    ################################################################################################

    # Specify the compression algorithm to use. Valid values are: `NONE`, `LZ4`, `SNAPPY`.
    # Type: string
    # Default value: "NONE"
    #driver.protocol.compression = "NONE"

    ################################################################################################
    # Query-related settings.
    ################################################################################################

    # The consistency level to use for both loading and unloading. Note that stronger consistency
    # levels usually result in reduced throughput. In addition, any level higher than `ONE` will
    # automatically disable continuous paging, which can dramatically reduce read throughput.
    # 
    # Valid values are: `ANY`, `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`,
    # `EACH_QUORUM`, `ALL`.
    # Type: string
    # Default value: "LOCAL_ONE"
    #driver.query.consistency = "LOCAL_ONE"

    # The page size, or how many rows will be retrieved simultaneously in a single network round
    # trip. This setting will limit the number of results loaded into memory simultaneously during
    # unloading. Not applicable for loading.
    # Type: number
    # Default value: 5000
    #driver.query.fetchSize = 5000

    # The default idempotence of statements generated by the loader.
    # Type: boolean
    # Default value: true
    #driver.query.idempotence = true

    # The serial consistency level to use for writes. Only applicable if the data is inserted using
    # lightweight transactions, ignored otherwise. Valid values are: `SERIAL` and `LOCAL_SERIAL`.
    # Type: string
    # Default value: "LOCAL_SERIAL"
    #driver.query.serialConsistency = "LOCAL_SERIAL"

    ################################################################################################
    # Socket-related settings.
    ################################################################################################

    # The time the driver waits for a request to complete. This is a global limit on the duration of
    # a `session.execute()` call, including any internal retries the driver might do.
    # Type: string
    # Default value: "60 seconds"
    #driver.socket.readTimeout = "60 seconds"

    ################################################################################################
    # Encryption-specific settings.
    # 
    # For more information about how to configure this section, see the Java Secure Socket Extension
    # (JSSE) Reference Guide:
    # http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can
    # also check the DataStax Java driver documentation on SSL:
    # http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
    ################################################################################################

    # The cipher suites to enable. For example:
    # 
    # `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
    # 
    # This property is optional. If it is not present, the driver won't explicitly enable cipher
    # suites, which according to the JDK documentation results in "a minimum quality of service".
    # Type: list
    # Default value: []
    #driver.ssl.cipherSuites = []

    # The algorithm to use for the SSL keystore. Valid values are: `SunX509`, `NewSunX509`.
    # Type: string
    # Default value: "SunX509"
    #driver.ssl.keystore.algorithm = "SunX509"

    # The keystore password.
    # Type: string
    # Default value: ""
    #driver.ssl.keystore.password = ""

    # The path of the keystore file. This setting is optional. If left unspecified, no client
    # authentication will be used.
    # Type: string
    # Default value: ""
    #driver.ssl.keystore.path = ""

    # The path of the certificate chain file. This setting is optional. If left unspecified, no
    # client authentication will be used.
    # Type: string
    # Default value: ""
    #driver.ssl.openssl.keyCertChain = ""

    # The path of the private key file.
    # Type: string
    # Default value: ""
    #driver.ssl.openssl.privateKey = ""

    # The SSL provider to use. Valid values are:
    # 
    # - **None**: no SSL.
    # - **JDK**: uses the JDK SSLContext
    # - **OpenSSL**: uses Netty's native support for OpenSSL. It provides better performance and
    # generates less garbage. This is the recommended provider when using SSL.
    # Type: string
    # Default value: "None"
    #driver.ssl.provider = "None"

    # The algorithm to use for the SSL truststore. Valid values are: `PKIX`, `SunX509`.
    # Type: string
    # Default value: "SunX509"
    #driver.ssl.truststore.algorithm = "SunX509"

    # The truststore password.
    # Type: string
    # Default value: ""
    #driver.ssl.truststore.password = ""

    # The path of the truststore file. This setting is optional. If left unspecified, server
    # certificates will not be validated.
    # Type: string
    # Default value: ""
    #driver.ssl.truststore.path = ""

    ################################################################################################
    # Workflow Engine-specific settings.
    ################################################################################################

    # Enable or disable dry-run mode, a test mode that runs the command but does not load data. Not
    # applicable for unloading.
    # Type: boolean
    # Default value: false
    #engine.dryRun = false

    # A unique identifier to attribute to each execution. When unspecified or empty, the engine will
    # automatically generate identifiers of the following form: *workflow*_*timestamp*, where :
    # 
    # - *workflow* stands for the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - *timestamp* is the current timestamp formatted as `uuuuMMdd-HHmmss-SSSSSS` (see
    # [https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html#patterns))
    # in UTC, with microsecond precision if available, and millisecond precision otherwise.
    # 
    # When this identifier is user-supplied, it is important to guarantee its uniqueness; failing to
    # do so may result in execution failures. It is also possible to provide templates here. Any
    # format compliant with the formatting rules of
    # [`String.format()`](https://docs.oracle.com/javase/8/docs/api/java/util/Formatter.html#syntax)
    # is accepted, and can contain the following parameters:
    # 
    # - `%1$s` : the workflow type (`LOAD`, `UNLOAD`, etc.);
    # - `%2$t` : the current time (with microsecond precision if available, and millisecond
    # precision otherwise);
    # - `%3$s` : the JVM process PID (this parameter might not be available on some operating
    # systems; if its value cannot be determined, a random integer will be inserted instead).
    # Type: string
    # Default value: ""
    #engine.executionId = ""

    ################################################################################################
    # Executor-specific settings.
    ################################################################################################

    # The maximum number of concurrent operations per second. This acts as a safeguard to prevent
    # more requests than the cluster can handle. Batch statements are counted by the number of
    # statements included. Reduce this setting when the latencies get too high and a remote cluster
    # cannot keep up with throughput, as `dsbulk` requests will eventually time out. Setting this
    # option to any negative value will disable it.
    # Type: number
    # Default value: -1
    #executor.maxPerSecond = -1

    # Enable or disable continuous paging. If the target cluster does not support continuous paging
    # or if `driver.query.consistency` is not `ONE` or `LOCAL_ONE`, traditional paging will be used
    # regardless of this setting.
    # Type: boolean
    # Default value: true
    #executor.continuousPaging.enabled = true

    # The maximum number of pages to retrieve. Setting this value to zero retrieves all pages
    # available.
    # Type: number
    # Default value: 0
    #executor.continuousPaging.maxPages = 0

    # The maximum number of pages per second. Setting this value to zero indicates no limit.
    # Type: number
    # Default value: 0
    #executor.continuousPaging.maxPagesPerSecond = 0

    # The size of the page. The unit to use is determined by the `pageUnit` setting.
    # Type: number
    # Default value: 5000
    #executor.continuousPaging.pageSize = 5000

    # The unit to use for the `pageSize` setting. Possible values are: `ROWS`, `BYTES`.
    # Type: string
    # Default value: "ROWS"
    #executor.continuousPaging.pageUnit = "ROWS"

    # The maximum number of "in-flight" requests, or maximum number of concurrent requests waiting
    # for a response from the server. This acts as a safeguard to prevent more requests than the
    # cluster can handle. Batch statements count as one request. Reduce this value when the
    # throughput for reads and writes cannot match the throughput of mappers; this is usually a sign
    # that the workflow engine is not well calibrated and will eventually run out of memory. Setting
    # this option to any negative value will disable it.
    # Type: number
    # Default value: 1024
    #executor.maxInFlight = 1024

    ################################################################################################
    # Log and error management settings.
    ################################################################################################

    # The maximum number of errors to tolerate before aborting the entire operation. Set to either a
    # number or a string of the form `N%` where `N` is a decimal number between 0 and 100. Setting
    # this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 100
    #log.maxErrors = 100

    # The writable directory where all log files will be stored; if the directory specified does not
    # exist, it will be created. URLs are not acceptable (not even `file:/` URLs). Log files for a
    # specific run, or execution, will be located in a sub-directory under the specified directory.
    # Each execution generates a sub-directory identified by an "execution ID". See
    # `engine.executionId` for more information about execution IDs. Relative paths will be resolved
    # against the current working directory. Also, for convenience, if the path begins with a tilde
    # (`~`), that symbol will be expanded to the current user's home directory.
    # Type: string
    # Default value: "./logs"
    #log.directory = "./logs"

    # Whether or not to use ANSI colors and other escape sequences in log messages printed to
    # standard output and standard error.
    # 
    # By default, DSBulk will use colored output when the terminal is:
    # 
    # - compatible with ANSI escape sequences; all common terminals on *nix and BSD systems,
    # including MacOS, are ANSI-compatible, and some popular terminals for Windows (Mintty, MinGW)
    # - a standard Windows DOS command prompt (ANSI sequences are translated on the fly).
    # 
    # There should be no reason to disable ANSI escape sequences, but if, for some reason, colored
    # messages are not desired or not printed correctly, this option allows disabling ANSI support
    # altogether.
    # Type: boolean
    # Default value: true
    #log.ansiEnabled = true

    # The desired log level. Valid values are:
    # 
    # - ABRIDGED: Print only basic information in summarized form.
    # - NORMAL: Print basic information in summarized form, and the statement's query string, if
    # available. For batch statements, this verbosity level also prints information about the
    # batch's inner statements.
    # - EXTENDED: Print full information, including the statement's query string, if available, and
    # the statement's bound values, if available. For batch statements, this verbosity level also
    # prints all information available about the batch's inner statements.
    # Type: string
    # Default value: "EXTENDED"
    #log.stmt.level = "EXTENDED"

    # The maximum length for a bound value. Bound values longer than this value will be truncated.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 50
    #log.stmt.maxBoundValueLength = 50

    # The maximum number of bound values to print. If the statement has more bound values than this
    # limit, the exceeding values will not be printed.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 50
    #log.stmt.maxBoundValues = 50

    # The maximum number of inner statements to print for a batch statement. Only applicable for
    # batch statements, ignored otherwise. If the batch statement has more children than this value,
    # the exceeding child statements will not be printed.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 10
    #log.stmt.maxInnerStatements = 10

    # The maximum length for a query string. Query strings longer than this value will be truncated.
    # 
    # Setting this value to `-1` disables this feature (not recommended).
    # Type: number
    # Default value: 500
    #log.stmt.maxQueryStringLength = 500

    ################################################################################################
    # Monitoring-specific settings.
    ################################################################################################

    # The report interval for the console reporter. The console reporter will print useful metrics
    # about the ongoing operation at this rate. Durations lesser than one second will be rounded up
    # to 1 second.
    # Type: string
    # Default value: "5 seconds"
    #monitoring.reportRate = "5 seconds"

    # Enable or disable CSV reporting. If enabled, CSV files containing metrics will be generated in
    # the designated log directory.
    # Type: boolean
    # Default value: false
    #monitoring.csv = false

    # The time unit used when printing latency durations. Valid values: all `TimeUnit` enum
    # constants.
    # Type: string
    # Default value: "MILLISECONDS"
    #monitoring.durationUnit = "MILLISECONDS"

    # The expected total number of reads. Optional, but if set, the console reporter will also print
    # the overall achievement percentage. Setting this value to `-1` disables this feature.
    # Type: number
    # Default value: -1
    #monitoring.expectedReads = -1

    # The expected total number of writes. Optional, but if set, the console reporter will also
    # print the overall achievement percentage. Setting this value to `-1` disables this feature.
    # Type: number
    # Default value: -1
    #monitoring.expectedWrites = -1

    # Enable or disable JMX reporting. Note that to enable remote JMX reporting, several properties
    # must also be set in the JVM during launch. This is accomplished via the `DSBULK_JAVA_OPTS`
    # environment variable.
    # Type: boolean
    # Default value: true
    #monitoring.jmx = true

    # The time unit used when printing throughput rates. Valid values: all `TimeUnit` enum
    # constants.
    # Type: string
    # Default value: "SECONDS"
    #monitoring.rateUnit = "SECONDS"

}
