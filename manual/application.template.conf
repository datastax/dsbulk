####################################################################################################
# This is a template configuration file for the DataStax Bulk Loader (DSBulk).
#
# This file is written in HOCON format; see
# https://github.com/typesafehub/config/blob/master/HOCON.md
# for more information on its syntax.
#
# Uncomment settings as needed to configure DSBulk. When this file is named application.conf and
# placed in the /conf directory, it will be automatically picked up and used by default. To use other
# file names see the -f command-line option.
####################################################################################################

####################################################################################################
# Connector-specific settings. This section contains settings for the connector to use; it also
# contains sub-sections, one for each available connector.
####################################################################################################

# The name of the connector to use.
# 
# It is used in two places:
# 
# 1. The path to the group of settings for the connector are located under `connector.<name>`.
# 2. The connector class name must start with `name`, case-insensitively. It is permitted for `name`
# to be the fully-qualified class name of the connector. That simply implies that the settings root
# will be at that fully-qualified location.
# 
# Example: `csv` for class `CSVConnector`, with settings located under `connector.csv`.
# Type: string
# Default value: "csv"
#dsbulk.connector.name = "csv"


####################################################################################################
# CSV Connector configuration.
####################################################################################################

# The URL or path of the resource(s) to read from or write to.
# 
# Which URL protocols are available depend on which URL stream handlers have been installed, but at
# least the following are guaranteed to be supported:
# 
# - **stdin**:  the stdin protocol can be used to read from standard input; the only valid URL for
# this protocol is: `stdin:/`.
# 
# This protocol cannot be used for writing.
# 
# - **stdout**: the stdout protocol can be used to write to standard output; the only valid URL for
# this protocol is: `stdout:/`.
# 
# This protocol cannot be used for reading.
# 
# - **file**: the file protocol can be used with all supported file systems, local or not.
# - **When reading**: the URL can point to a single file, or to an existing directory; in case of a
# directory, the *fileNamePattern* setting can be used to filter files to read, and the *recursive*
# setting can be used to control whether or not the connector should look for files in subdirectories
# as well.
# - **When writing**: the URL will be treated as a directory; if it doesn't exist, the loader will
# attempt to create it; CSV files will be created inside this directory, and their names can be
# controlled with the *fileNameFormat* setting.
# 
# Note that if the value specified here does not have a protocol, then it is assumed to be a file
# protocol.
# 
# Examples:
# 
# url = /path/to/dir/or/file           # without protocol
# url = "file:///path/to/dir/or/file"  # with protocol
# url = "stdin:/"                      # to read csv data from stdin
# url = "stdout:/"                     # to write csv data to stdout
# 
# For other URLs: the URL will be read or written directly; settings like *fileNamePattern*,
# *recursive*, and *fileNameFormat* will have no effect.
# 
# This setting has no default value and must be supplied by the user.
# Type: string
# Default value: ""
#dsbulk.connector.csv.url = ""

# The character to use as field delimiter.
# 
# Only one character can be specified. Note that this setting applies to all files to be read or
# written.
# Type: string
# Default value: ","
#dsbulk.connector.csv.delimiter = ","

# Whether the files to read or write begin with a header line or not.
# 
# When reading:
# 
# - if set to true, the first non-empty line in every file is discarded, even if the *skipLines*
# setting is set to zero (see below). However, that line will be used to assign field names to each
# record, thus allowing mappings by field name such as `{myFieldName1 = myColumnName1, myFieldName2 =
# myColumnName2}`.
# - if set to false, records will not contain field names, only (zero-based) field indexes; in this
# case, the mapping should be index-based, such as in `{0 = myColumnName1, 1 = myColumnName2}`.
# 
# When writing:
# 
# - if set to true: each file will begin with a header line;
# - if set to false, files will not contain header lines.
# 
# Note that this setting applies to all files to be read or written.
# Type: boolean
# Default value: true
#dsbulk.connector.csv.header = true

# Defines a number of lines to skip from each input file before the parser can begin to execute.
# 
# Ignored when writing.
# Type: number
# Default value: 0
#dsbulk.connector.csv.skipLines = 0

# Defines the maximum number of lines to read from or write to each file.
# 
# When reading, all lines past this number will be discarded.
# 
# When writing, a file will contain at most this number of lines; if more records remain to be
# written, a new file will be created using the *fileNameFormat* setting.
# 
# Note that when writing to anything other than a directory, this setting is ignored.
# 
# This setting takes into account the *header* setting: if a file begins with a header line, that line
# is counted.
# 
# This feature is disabled by default (indicated by its `-1` value).
# Type: number
# Default value: -1
#dsbulk.connector.csv.maxLines = -1

# The character that represents a line comment when found in the beginning of a line of text.
# 
# Only one character can be specified. Note that this setting applies to all files to be read or
# written.
# 
# This feature is disabled by default (indicated by its `null` character value).
# Type: string
# Default value: "\u0000"
#dsbulk.connector.csv.comment = "\u0000"

# The file encoding to use.
# 
# Note that this setting applies to all files to be read or written.
# Type: string
# Default value: "UTF-8"
#dsbulk.connector.csv.encoding = "UTF-8"

# The character used for escaping quotes inside an already quoted value.
# 
# Only one character can be specified. Note that this setting applies to all files to be read or
# written.
# Type: string
# Default value: "\\"
#dsbulk.connector.csv.escape = "\\"

# The file name format to use when writing.
# 
# Ignored when reading. Ignored for non-file URLs.
# 
# The file name must comply with the formatting rules of `String.format()`, and must contain a `%d`
# format specifier that will be used to increment file name counters.
# Type: string
# Default value: "output-%0,6d.csv"
#dsbulk.connector.csv.fileNameFormat = "output-%0,6d.csv"

# The glob pattern to use when searching for files to read. The syntax to use is the glob syntax, as
# described in `java.nio.file.FileSystem.getPathMatcher()`.
# 
# Ignored when writing. Ignored for non-file URLs.
# 
# Only applicable when the *url* setting points to a directory on a known filesystem, ignored
# otherwise.
# Type: string
# Default value: "**/*.csv"
#dsbulk.connector.csv.fileNamePattern = "**/*.csv"

# The maximum number of characters that a field can contain.
# 
# This setting is used to size internal buffers and to avoid out-of-memory problems.
# 
# If set to -1, internal buffers will be resized dynamically. While convenient, this might lead to
# memory problems. It could also hurt throughput, if some large fields require constant resizings; if
# this is the case, it is preferable to set this value to a fixed positive number that is big enough
# to contain all field values.
# Type: number
# Default value: 4096
#dsbulk.connector.csv.maxCharsPerColumn = 4096

# The maximum number of files that can be written simultaneously.
# 
# Ignored when reading.
# 
# The special syntax `NC` can be used to specify a number of threads that is a multiple of the number
# of available cores, e.g. if the number of cores is 8, then 0.5C = 0.5 * 8 = 4 threads.
# Type: string
# Default value: "0.25C"
#dsbulk.connector.csv.maxConcurrentFiles = "0.25C"

# The character used for quoting fields when the field delimiter is part of the field value.
# 
# Only one character can be specified. Note that this setting applies to all files to be read or
# written.
# Type: string
# Default value: "\""
#dsbulk.connector.csv.quote = "\""

# Whether to scan for files in subdirectories of the root directory.
# 
# Only applicable when the *url* setting points to a directory on a known filesystem, ignored
# otherwise. Ignored when writing.
# Type: boolean
# Default value: false
#dsbulk.connector.csv.recursive = false


####################################################################################################
# Schema-specific settings.
####################################################################################################

# The keyspace to connect to. Optional.
# 
# If not specified, then the *schema.query* setting must be specified.
# Type: string
# Default value: ""
#dsbulk.schema.keyspace = ""

# The destination table. Optional.
# 
# If not specified, then the *schema.query* setting must be specified.
# Type: string
# Default value: ""
#dsbulk.schema.table = ""

# The field-to-column mapping to use.
# 
# Applies to both load and unload workflows.
# 
# If not specified, the loader will apply a strict one-to-one mapping between the source fields and
# the database table. If that is not what you want, then you must supply an explicit mapping.
# 
# Mappings should be specified as a map of the following form:
# 
# - Indexed data sources: `0 = col1, 1 = col2, 2 = col3`, where `0`, `1`, `2`, etc. are the zero-based
# indices of fields in the source data; and `col1`, `col2`, `col3` are bound variable names in the
# insert statement.
# - A shortcut to map the first `n` fields is to simply specify the destination columns: `col1, col2,
# col3`.
# - Mapped data sources: `fieldA = col1, fieldB = col2, fieldC = col3`, where `fieldA`, `fieldB`,
# `fieldC`, etc. are field names in the source data; and `col1`, `col2`, `col3` are bound variable
# names in the insert statement.
# 
# To specify that a field should be used for the query timestamp or ttl, use the specially named fake
# columns `__ttl` and `__timestamp`: `fieldA = __ttl`. Note that unlike `schema.query_timestamp`, this
# mapping only supports the numeric format of timestamp.
# 
# In addition, for mapped data sources, it is also possible to specify that the mapping be partly
# auto-generated and partly explicitly specified. For example, if a source row has fields `c1`, `c2`,
# `c3`, and `c5`, and the table has columns `c1`, `c2`, `c3`, `c4`, one can map all like-named columns
# and specify that `c5` in the source maps to `c4` in the table as follows: `* = *, c5 = c4`
# 
# One can specify that all like-named fields be mapped, except for `c2`: `* = -c2`
# 
# To skip `c2` and `c3`: `* = [-c2, -c3]`
# 
# The exact type of mapping to use depends on the connector being used. Some connectors can only
# produce indexed records; others can only produce mapped ones, while others are capable of producing
# both indexed and mapped records at the same time. Refer to the connector's documentation to know
# which kinds of mapping it supports.
# Type: string
# Default value: ""
#dsbulk.schema.mapping = ""

# Case-sensitive strings (in the form of a comma-delimited list) that should be mapped to `null`.
# 
# In load workflows, when a record field value matches one of these words, then that value is replaced
# with a `null` and forwarded to DSE as such.
# 
# This setting only applies for fields of type String.
# 
# Note that this setting is applied before the *schema.nullToUnset* setting, hence any `null`s
# produced by a null-string can still be left unset if required.
# 
# In unload workflows, only the first string specified here will be used: when a row cell contains a
# `null` value, then it will be replaced with that word and forwarded as such to the connector.
# 
# By default, empty strings are converted to `null`s in load workflows, and conversely `null`s are
# converted to empty strings in unload workflows.
# Type: string
# Default value: ""
#dsbulk.schema.nullStrings = ""

# Whether or not to map `null` input values to "unset" in the database, meaning don't modify a
# potentially pre-existing value of this field for this row.
# 
# This is only valid for load scenarios; it is ignored otherwise.
# 
# Note that this setting is applied after the *schema.nullStrings* setting, and may intercept `null`s
# produced by that setting.
# 
# Note that setting this to false leads to tombstones being created in the database to represent
# `null`.
# Type: boolean
# Default value: true
#dsbulk.schema.nullToUnset = true

# The query to use. Optional.
# 
# If not specified, then *schema.keyspace* and *schema.table* must be specified, and dsbulk will infer
# the appropriate statement based on the table's metadata, using all available columns.
# 
# In load worflows, the statement can be any `INSERT` or `UPDATE` statement, but **must** use named
# bound variables exclusively; positional bound variables will not work.
# 
# Bound variable names usually match those of the columns in the destination table, but this is not a
# strict requirement; it is, however, required that their names match those specified in the mapping.
# 
# See *schema.mapping* setting for more information.
# 
# In unload worflows, the statement can be any regular `SELECT` statement; it can optionally contain a
# token range restriction clause of the form: `token(...) > :start and token(...) <= :end.`
# 
# If such a clause is present, the engine will generate as many statements as there are token ranges
# in the cluster, thus allowing parallelization of reads while at the same time targeting coordinators
# that are also replicas.
# 
# The column names in the SELECT clause will be used to match column names specified in the mapping.
# See "mapping" setting for more information.
# Type: string
# Default value: ""
#dsbulk.schema.query = ""

# Timestamp of inserted/updated cells during load.
# 
# Only applicable for load; ignored for unload.
# 
# The following formats are supported:
# 
# * An integer indicating number of microseconds since epoch.
# * A valid ISO-8601 date-time format, e.g. 2017-01-02T14:56:78+01:00 (if the format does not include
# a zone information, it is assumed UTC).
# 
# Note that the second format has seconds-precision, not microseconds.
# 
# If not specified, inserts/updates use current time of the system running the tool.
# 
# For more information, see the [CQL
# Reference](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__timestamp-value).
# Type: string
# Default value: ""
#dsbulk.schema.queryTimestamp = ""

# Time-to-live of inserted/updated cells during load (seconds).
# 
# Only applicable for load; ignored for unload.
# 
# A value of -1 means there is no ttl.
# 
# For more information, see the [CQL
# Reference](https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlInsert.html#cqlInsert__ime-value).
# Type: number
# Default value: -1
#dsbulk.schema.queryTtl = -1

# Record metadata.
# 
# Applies within both load and unload workflows to records being respectively read from or written to
# the connector.
# 
# This information is optional, and rarely needed.
# 
# If not specified:
# 
# - If the connector is capable of reporting the record metadata accurately (for example, some
# database connectors might be able to inspect the target table's metadata), then this section is only
# required if you want to override some field types as reported by the connector.
# - If the connector is not capable of reporting the record metadata accurately (for example, file
# connectors usually cannot report such information), then all fields are assumed to be of type
# `String`. If this is not correct, then you need to provide the correct type information here.
# 
# Field metadata should be specified as a HOCON map
# (https://github.com/typesafehub/config/blob/master/HOCON.md) of the following form:
# 
# - Indexed data sources: `0 = java.lang.String, 1 = java.lang.Double`, where `0`, `1`, etc. are the
# zero-based indices of fields in the source data; and the values are the expected types for each
# field.
# - Mapped data sources: `fieldA = java.lang.String, fieldB = java.lang.Double`, where `fieldA`,
# `fieldB`, etc. are field names in the source data; and the values are the expected types for each
# field.
# Type: string
# Default value: ""
#dsbulk.schema.recordMetadata = ""


####################################################################################################
# Batch-specific settings.
# 
# These settings control how the workflow engine groups together statements before writing them.
# 
# Only applicable for load workflows, ignored otherwise.
# 
# See `com.datastax.dsbulk.executor.api.batch.StatementBatcher` for more information.
####################################################################################################

# The buffer size to use for batching statements.
# 
# The buffer will be flushed when this size is reached.
# 
# It is usually not necessary to set this value higher than `maxBatchSize`, unless the dataset to load
# is unsorted, in which case a higher value might improve the average batch size.
# Type: number
# Default value: 32
#dsbulk.batch.bufferSize = 32

# Whether to enable batching of statements or not.
# Type: boolean
# Default value: true
#dsbulk.batch.enabled = true

# The maximum batch size.
# 
# The ideal batch size depends on how large is the data to be inserted: the larger the data, the
# smaller this value should be.
# 
# The ideal batch size also depends on the batch mode in use. When using **PARTITION_KEY**, it is
# usually better to use larger batch sizes. When using **REPLICA_SET** however, batches sizes should
# remain small (below 10).
# Type: number
# Default value: 32
#dsbulk.batch.maxBatchSize = 32

# The grouping mode. Valid values are:
# - **PARTITION_KEY**: Groups together statements that share the same partition key. This is the
# default mode, and the preferred one.
# - **REPLICA_SET**: Groups together statements that share the same replica set. This mode might yield
# better results for small clusters and lower replication factors, but tends to perform equally well
# or worse than `PARTITION_KEY` for larger clusters or high replication factors.
# Type: string
# Default value: "PARTITION_KEY"
#dsbulk.batch.mode = "PARTITION_KEY"


####################################################################################################
# Conversion-specific settings. These settings apply for both load and unload workflows.
# 
# When writing, these settings determine how record fields emitted by connectors are parsed.
# 
# When unloading, these settings determine how row cells emitted by DSE are formatted.
####################################################################################################

# All representations of true and false supported by dsbulk. Each representation is of the form
# `true-value:false-value`, case-insensitive.
# 
# In load workflows, all representations are taken into account.
# 
# In unload workflows, the first true-false pair will be used to format booleans; all other pairs will
# be ignored.
# Type: list<string>
# Default value: ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]
#dsbulk.codec.booleanWords = ["1:0","Y:N","T:F","YES:NO","TRUE:FALSE"]

# The temporal pattern to use for `String` to CQL date conversions. This can be either:
# 
# - A date-time pattern
# - A pre-defined formatter such as `ISO_LOCAL_DATE`
# 
# For more information on patterns and pre-defined formatters, see
# https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
# Type: string
# Default value: "ISO_LOCAL_DATE"
#dsbulk.codec.date = "ISO_LOCAL_DATE"

# The locale to use for locale-sensitive conversions.
# Type: string
# Default value: "en_US"
#dsbulk.codec.locale = "en_US"

# The `DecimalFormat` pattern to use for `String` to `Number` conversions. See
# `java.text.DecimalFormat` for details about the pattern syntax to use.
# Type: string
# Default value: "#,###.##"
#dsbulk.codec.number = "#,###.##"

# The temporal pattern to use for `String` to CQL time conversions. This can be either:
# 
# - A date-time pattern
# - A pre-defined formatter such as `ISO_LOCAL_TIME`
# 
# For more information on patterns and pre-defined formatters, see
# https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
# Type: string
# Default value: "ISO_LOCAL_TIME"
#dsbulk.codec.time = "ISO_LOCAL_TIME"

# The time zone to use for temporal conversions that do not convey any explicit time zone information.
# Type: string
# Default value: "UTC"
#dsbulk.codec.timeZone = "UTC"

# The temporal pattern to use for `String` to CQL timestamp conversions. This can be either:
# 
# - A date-time pattern
# - A pre-defined formatter such as `ISO_DATE_TIME`
# - The special value `CQL_DATE_TIME`, which is a special parser that accepts all valid CQL literal
# formats for the `timestamp` type
# 
# For more information on patterns and pre-defined formatters, see
# https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html.
# Type: string
# Default value: "CQL_DATE_TIME"
#dsbulk.codec.timestamp = "CQL_DATE_TIME"


####################################################################################################
# Driver-specific configuration.
####################################################################################################

# The contact points to use for the initial connection to the cluster.
# 
# This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the
# host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be
# used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on
# some platforms).
# 
# Note that each host entry may optionally be followed by `:port` to specify the port to connect to.
# When not specified, this value falls back to the *port* setting.
# Type: string
# Default value: "127.0.0.1"
#dsbulk.driver.hosts = "127.0.0.1"

# The port to connect to at initial contact points.
# 
# Note that all nodes in a cluster must accept connections on the same port number.
# Type: number
# Default value: 9042
#dsbulk.driver.port = 9042

# The simple or fully-qualified class name of the address translator to use.
# 
# This is only needed if the nodes are not directly reachable from the driver (for example, the driver
# is in a different network region and needs to use a public IP, or it connects through a proxy).
# Type: string
# Default value: "IdentityTranslator"
#dsbulk.driver.addressTranslator = "IdentityTranslator"

# The simple or fully-qualified class name of the timestamp generator to use. Built-in options are:
# 
# - **AtomicTimestampGenerator**: timestamps are guaranteed to be unique across all client threads.
# - **ThreadLocalTimestampGenerator**: timestamps are guaranteed to be unique within each thread only.
# - **ServerSideTimestampGenerator**: do not generate timestamps, let the server assign them.
# Type: string
# Default value: "AtomicMonotonicTimestampGenerator"
#dsbulk.driver.timestampGenerator = "AtomicMonotonicTimestampGenerator"


####################################################################################################
# Authentication settings.
####################################################################################################

# The password to use. Required.
# 
# Providers that accept this setting:
# - `PlainTextAuthProvider`
# - `DsePlainTextAuthProvider`
# Type: string
# Default value: "cassandra"
#dsbulk.driver.auth.password = "cassandra"

# The username to use. Required.
# 
# Providers that accept this setting:
# - `PlainTextAuthProvider`
# - `DsePlainTextAuthProvider`
# Type: string
# Default value: "cassandra"
#dsbulk.driver.auth.username = "cassandra"

# The name of the AuthProvider to use.
# - **None**: no authentication.
# - **PlainTextAuthProvider**:
# Uses `com.datastax.driver.core.PlainTextAuthProvider` for authentication. Supports SASL
# authentication using the `PLAIN` mechanism (plain text authentication).
# - **DsePlainTextAuthProvider**:
# Uses `com.datastax.driver.dse.auth.DsePlainTextAuthProvider` for authentication. Supports SASL
# authentication to DSE clusters using the `PLAIN` mechanism (plain text authentication), and also
# supports optional proxy authentication; should be preferred to `PlainTextAuthProvider` when
# connecting to secured DSE clusters.
# - **DseGSSAPIAuthProvider**:
# Uses `com.datastax.driver.dse.auth.DseGSSAPIAuthProvider` for authentication. Supports SASL
# authentication to DSE clusters using the `GSSAPI` mechanism (Kerberos authentication), and also
# supports optional proxy authentication.
# Type: string
# Default value: "None"
#dsbulk.driver.auth.provider = "None"

# The authorization ID to use. Optional.
# 
# An authorization ID allows the currently authenticated user to act as a different user (a.k.a. proxy
# authentication).
# 
# Providers that accept this setting:
# - `DsePlainTextAuthProvider`
# - `DseGSSAPIAuthProvider`
# Type: string
# Default value: ""
#dsbulk.driver.auth.authorizationId = ""

# The path of the Kerberos keytab file to use for authentication. Optional.
# 
# If left unspecified, it is assumed that authentication will be done with a ticket cache instead.
# 
# Providers that accept this setting:
# - `DseGSSAPIAuthProvider`
# Type: string
# Default value: ""
#dsbulk.driver.auth.keyTab = ""

# The Kerberos principal to use. Required.
# 
# Providers that accept this setting:
# - `DseGSSAPIAuthProvider`
# Type: string
# Default value: "user@DATASTAX.COM"
#dsbulk.driver.auth.principal = "user@DATASTAX.COM"

# The SASL protocol name to use. Required.
# 
# This value should match the username of the Kerberos service principal used by the DSE server. This
# information is specified in the `dse.yaml` file by the *service_principal* option under the
# *kerberos_options* section, and may vary from one DSE installation to another â€“ especially if you
# installed DSE with an automated package installer.
# 
# Providers that accept this setting:
# - `DseGSSAPIAuthProvider`
# Type: string
# Default value: "dse"
#dsbulk.driver.auth.saslProtocol = "dse"


####################################################################################################
# Settings for various driver policies.
####################################################################################################

# The name of the load balancing policy.
# 
# Supported policies include: `dse`, `dcAwareRoundRobin`, `roundRobin`, `whiteList`, `tokenAware`.
# Available options for the policies are listed below as appropriate. For more information, refer to
# the driver documentation for the policy.
# 
# If not specified, defaults to the driver's default load balancing policy, which is currently the
# `DseLoadBalancingPolicy` wrapping a `TokenAwarePolicy`, wrapping a `DcAwareRoundRobinPolicy`.
# 
# NOTE: It is critical for a token-aware policy to be used in the chain in order to benefit from
# batching by partition key.
# Type: string
# Default value: ""
#dsbulk.driver.policy.lbp.name = ""

# Type: boolean
# Default value: false
#dsbulk.driver.policy.lbp.dcAwareRoundRobin.allowRemoteDCsForLocalConsistencyLevel = false

# Type: string
# Default value: ""
#dsbulk.driver.policy.lbp.dcAwareRoundRobin.localDc = ""

# Type: number
# Default value: 0
#dsbulk.driver.policy.lbp.dcAwareRoundRobin.usedHostsPerRemoteDc = 0

# The child policy being wrapped.
# 
# It is required to be one of the policies mentioned above.
# Type: string
# Default value: "roundRobin"
#dsbulk.driver.policy.lbp.dse.childPolicy = "roundRobin"

# The child policy being wrapped.
# 
# It is required to be one of the policies mentioned above.
# Type: string
# Default value: "roundRobin"
#dsbulk.driver.policy.lbp.tokenAware.childPolicy = "roundRobin"

# Type: boolean
# Default value: true
#dsbulk.driver.policy.lbp.tokenAware.shuffleReplicas = true

# The child policy being wrapped.
# 
# It is required to be one of the policies mentioned above.
# Type: string
# Default value: "roundRobin"
#dsbulk.driver.policy.lbp.whiteList.childPolicy = "roundRobin"

# List of hosts to white list.
# 
# This must be a comma-separated list of hosts, each specified by a host-name or ip address. If the
# host is a DNS name that resolves to multiple A-records, all the corresponding addresses will be
# used. Do not use `localhost` as a host-name (since it resolves to both IPv4 and IPv6 addresses on
# some platforms).
# Type: string
# Default value: ""
#dsbulk.driver.policy.lbp.whiteList.hosts = ""

# Maximum number of retries for a timed-out request.
# Type: number
# Default value: 10
#dsbulk.driver.policy.maxRetries = 10


####################################################################################################
# Pooling-specific settings.
# 
# The driver maintains a connection pool to each node, according to the distance assigned to it by the
# load balancing policy. If the distance is `IGNORED`, no connections are maintained.
####################################################################################################

# The heartbeat interval. If a connection stays idle for that duration (no reads), the driver sends a
# dummy message on it to make sure it's still alive. If not, the connection is trashed and replaced.
# Type: string
# Default value: "30 seconds"
#dsbulk.driver.pooling.heartbeat = "30 seconds"

# The number of connections in the pool for nodes at "local" distance.
# Type: number
# Default value: 4
#dsbulk.driver.pooling.local.connections = 4

# The maximum number of requests that can be executed concurrently on a connection.
# 
# This must be between 1 and 32768.
# Type: number
# Default value: 32768
#dsbulk.driver.pooling.local.requests = 32768

# The number of connections in the pool for nodes at "remote" distance.
# Type: number
# Default value: 1
#dsbulk.driver.pooling.remote.connections = 1

# The maximum number of requests that can be executed concurrently on a connection.
# 
# This must be between 1 and 32768.
# Type: number
# Default value: 1024
#dsbulk.driver.pooling.remote.requests = 1024


####################################################################################################
# Native Protocol-specific settings.
####################################################################################################

# The compression algorithm to use.
# Valid values are: `NONE`, `LZ4`, `SNAPPY`.
# Type: string
# Default value: "NONE"
#dsbulk.driver.protocol.compression = "NONE"


####################################################################################################
# Query-related settings.
####################################################################################################

# The consistency level to use for both loads and unloads.
# 
# Valid values are: `ANY`, `LOCAL_ONE`, `ONE`, `TWO`, `THREE`, `LOCAL_QUORUM`, `QUORUM`,
# `EACH_QUORUM`, `ALL`.
# Type: string
# Default value: "LOCAL_ONE"
#dsbulk.driver.query.consistency = "LOCAL_ONE"

# The page size. This controls how many rows will be retrieved simultaneously in a single network
# round trip (the goal being to avoid loading too many results in memory at the same time).
# 
# Only applicable in unload scenarios, ignored otherwise.
# Type: number
# Default value: 5000
#dsbulk.driver.query.fetchSize = 5000

# The default idempotence of statements generated by the loader.
# Type: boolean
# Default value: true
#dsbulk.driver.query.idempotence = true

# The serial consistency level to use for writes.
# 
# Valid values are: `SERIAL` and `LOCAL_SERIAL`.
# 
# Only applicable if the data is inserted using lightweight transactions, ignored otherwise.
# Type: string
# Default value: "LOCAL_SERIAL"
#dsbulk.driver.query.serialConsistency = "LOCAL_SERIAL"


####################################################################################################
# Socket-related settings.
####################################################################################################

# How long the driver waits for a request to complete. This is a global limit on the duration of a
# `session.execute()` call, including any internal retries the driver might do.
# Type: string
# Default value: "60 seconds"
#dsbulk.driver.socket.readTimeout = "60 seconds"


####################################################################################################
# Encryption-specific settings.
# 
# For more information about how to configure this section, see the Java Secure Socket Extension
# (JSSE) Reference Guide:
# http://docs.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html. You can also
# check the DataStax Java driver documentation on SSL:
# http://docs.datastax.com/en/developer/java-driver-dse/latest/manual/ssl/
####################################################################################################

# The cipher suites to enable.
# 
# Example: `cipherSuites = ["TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_256_CBC_SHA"]`
# 
# This property is optional. If it is not present, the driver won't explicitly enable cipher suites,
# which according to the JDK documentation results in "a minimum quality of service".
# Type: list
# Default value: []
#dsbulk.driver.ssl.cipherSuites = []

# The algorithm to use.
# 
# Valid values are: `SunX509`, `NewSunX509`.
# Type: string
# Default value: "SunX509"
#dsbulk.driver.ssl.keystore.algorithm = "SunX509"

# The keystore password.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.keystore.password = ""

# The path of the keystore file.
# 
# This setting is optional. If left unspecified, no client authentication will be used.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.keystore.path = ""

# The path of the certificate chain file.
# 
# This setting is optional. If left unspecified, no client authentication will be used.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.openssl.keyCertChain = ""

# The path of the private key file.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.openssl.privateKey = ""

# The SSL provider to use.
# 
# Valid values are:
# 
# - **None**: no SSL.
# - **JDK**: uses JDK's SSLContext
# - **OpenSSL**: uses Netty's native support for OpenSSL
# 
# Using OpenSSL provides better performance and generates less garbage. A disadvantage of using the
# OpenSSL provider is that, unlike the JDK provider, it requires a platform-specific dependency, named
# `netty-tcnative`, which must be added manually to the loader's classpath (typically by dropping its
# jar in the lib subdirectory of the DSBulk archive).
# 
# Follow these instructions to find out how to add this dependency:
# http://netty.io/wiki/forked-tomcat-native.html
# Type: string
# Default value: "None"
#dsbulk.driver.ssl.provider = "None"

# The algorithm to use.
# 
# Valid values are: `PKIX`, `SunX509`.
# Type: string
# Default value: "SunX509"
#dsbulk.driver.ssl.truststore.algorithm = "SunX509"

# The truststore password.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.truststore.password = ""

# The path of the truststore file.
# 
# This setting is optional. If left unspecified, server certificates will not be validated.
# Type: string
# Default value: ""
#dsbulk.driver.ssl.truststore.path = ""


####################################################################################################
# Workflow Engine-specific settings.
####################################################################################################

# Whether or not to run in dry-run mode.
# 
# Only applicable for loads, the tool proceeds as normal except it does not actually load any data.
# Type: boolean
# Default value: false
#dsbulk.engine.dryRun = false


####################################################################################################
# Executor-specific settings.
####################################################################################################

# The maximum number of concurrent operations per second. This acts as a safeguard against workflows
# that could overwhelm the cluster with more requests than it can handle. Batch statements count for
# as many operations as their number of inner statements.
# 
# You should reduce this value when your latencies get too high. This is usually a sign that the
# remote cluster cannot keep up with the throughput, and requests will eventually time out.
# 
# Setting this option to any negative value will disable it.
# Type: number
# Default value: -1
#dsbulk.executor.maxPerSecond = -1

# Whether or not continuous paging is enabled.
# 
# If the target cluster does not support continuous paging, traditional paging will be used regardless
# of this setting.
# Type: boolean
# Default value: true
#dsbulk.executor.continuousPaging.enabled = true

# The maximum number of pages to retrieve.
# 
# Setting this value to zero retrieves all pages available.
# Type: number
# Default value: 0
#dsbulk.executor.continuousPaging.maxPages = 0

# The maximum number of pages per second.
# 
# Setting this value to zero indicates no limit.
# Type: number
# Default value: 0
#dsbulk.executor.continuousPaging.maxPagesPerSecond = 0

# The size of the page. The unit to use is determined by the *executor.continuousPaging.pageUnit*
# setting.
# 
# Type: number
# Default value: 5000
#dsbulk.executor.continuousPaging.pageSize = 5000

# The unit to use for the *executor.continuousPaging.pageSize* setting.
# 
# Possible values are: `ROWS`, `BYTES`.
# Type: string
# Default value: "ROWS"
#dsbulk.executor.continuousPaging.pageUnit = "ROWS"

# The maximum number of "in-flight" requests. In other words, sets the maximum number of concurrent
# uncompleted futures waiting for a response from the server. This acts as a safeguard against
# workflows that generate more requests than they can handle. Batch statements count for 1 request.
# 
# You should reduce this value when the throughput for reads and writes cannot match the throughput of
# mappers; this is usually a sign that the workflow engine is not well calibrated and will eventually
# run out of memory.
# 
# Setting this option to any negative value will disable it.
# Type: number
# Default value: 1024
#dsbulk.executor.maxInFlight = 1024


####################################################################################################
# Log and error management settings.
####################################################################################################

# The maximum number of errors to tolerate before aborting the entire operation.
# 
# Setting this value to `-1` disables this feature (not recommended).
# Type: number
# Default value: 100
#dsbulk.log.maxErrors = 100

# The directory where all log files will be stored.
# 
# Note that this must be a path pointing to a writable directory.
# 
# Log files for a specific run will be located in a sub-directory inside the directory specified here.
# Each run generates a sub-directory identified by an "operation ID', which is basically a timestamp
# in the format: `yyyy_MM_dd_HH_mm_ss_nnnnnnnnn`.
# 
# Setting this value to `.` denotes the current working directory.
# Type: string
# Default value: "./logs"
#dsbulk.log.directory = "./logs"

# The desired log level.
# 
# Possible values are:
# - **ABRIDGED**: only prints basic information in summarized form.
# - **NORMAL**: prints basic information in summarized form, and the statement's query string, if
# available. For batch statements, this verbosity level also prints information about the batch's
# inner statements.
# - **EXTENDED**: prints full information, including the statement's query string, if available, and
# the statement's bound values, if available. For batch statements, this verbosity level also prints
# all information available about the batch's inner statements.
# Type: string
# Default value: "EXTENDED"
#dsbulk.log.stmt.level = "EXTENDED"

# The maximum length for a bound value. Bound values longer than this value will be truncated.
# 
# Setting this value to `-1` disables this feature (not recommended).
# Type: number
# Default value: 50
#dsbulk.log.stmt.maxBoundValueLength = 50

# The maximum number of bound values to print. If the statement has more bound values than this limit,
# the exceeding values will not be printed.
# 
# Setting this value to `-1` disables this feature (not recommended).
# Type: number
# Default value: 50
#dsbulk.log.stmt.maxBoundValues = 50

# The maximum number of inner statements to print for a batch statement.
# 
# Only applicable for batch statements, ignored otherwise.
# 
# If the batch statement has more children than this value, the exceeding child statements will not be
# printed.
# 
# Setting this value to `-1` disables this feature (not recommended).
# Type: number
# Default value: 10
#dsbulk.log.stmt.maxInnerStatements = 10

# The maximum length for a query string. Query strings longer than this value will be truncated.
# 
# Setting this value to `-1` disables this feature (not recommended).
# Type: number
# Default value: 500
#dsbulk.log.stmt.maxQueryStringLength = 500


####################################################################################################
# Monitoring-specific settings.
####################################################################################################

# The report interval for the console reporter.
# 
# The console reporter will print useful metrics about the ongoing operation at this rate.
# 
# Durations lesser than one second will be rounded up to 1 second.
# Type: string
# Default value: "5 seconds"
#dsbulk.monitoring.reportRate = "5 seconds"

# The time unit to use when printing latency durations.
# 
# Valid values: all `TimeUnit` enum constants.
# Type: string
# Default value: "MILLISECONDS"
#dsbulk.monitoring.durationUnit = "MILLISECONDS"

# The expected total number of reads.
# 
# This information is optional; if present, the console reporter will also print the the overall
# achievement percentage.
# 
# Setting this value to `-1` disables this feature.
# Type: number
# Default value: -1
#dsbulk.monitoring.expectedReads = -1

# The expected total number of writes.
# 
# This information is optional; if present, the console reporter will also print the the overall
# achievement percentage.
# 
# Setting this value to `-1` disables this feature.
# Type: number
# Default value: -1
#dsbulk.monitoring.expectedWrites = -1

# Whether or not to enable JMX reporting.
# 
# Note that to enable *remote* JMX reporting, several properties must also be set in the JVM during
# launch. This is accomplished via the `DSBULK_JAVA_OPTS` environment variable.
# Type: boolean
# Default value: true
#dsbulk.monitoring.jmx = true

# The time unit to use when printing throughput rates.
# 
# Valid values: all `TimeUnit` enum constants.
# Type: string
# Default value: "SECONDS"
#dsbulk.monitoring.rateUnit = "SECONDS"


